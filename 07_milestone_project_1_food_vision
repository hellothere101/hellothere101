Loading...
07_milestone_project_1_food_vision.ipynb
07_milestone_project_1_food_vision.ipynb_
Milestone project 1: food Vision big
check GPU
google colab offers free gpu's,however not all of them are compatible with mixed precision training

Google colab offer

K80(not compatible)
p100(not compatible)
Tesla T4 (compatible)
knowing this, in order to use precision training we need acesses to tesla t4 (from google) or using our own hardware it need aGpu score of 7.0+

[ ]
!nvidia-smi
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.

Get the helper functions
In the past modules we have created a bunch of helper functions to do small task required for our notebook.

rather then rewriting all these we will import the script and load it in from there.

you can find the helper function here:- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

[ ]
#download helper function 
!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py
--2023-01-11 15:45:52--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10246 (10K) [text/plain]
Saving to: ‘helper_functions.py’

helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      

2023-01-11 15:45:52 (86.9 MB/s) - ‘helper_functions.py’ saved [10246/10246]

[ ]
#import helper_function for the notebook
from helper_functions import create_tensorboard_callback,plot_loss_curves,compare_historys,unzip_data,load_and_prep_image
use Tensorflow datasets to download data
if you want to get an overview of the a Tensorflow datasets(TFDS), read the guide:- https://www.tensorflow.org/datasets

[ ]
#Get the tensorflow datasets
import tensorflow_datasets as tfds

[ ]
#list all the avaliable data sets avaliable in the datasets
datasets_list=tfds.list_builders() #get all the datasets in the tfds
print("food101" in datasets_list) #is our target datasets in the list of TFDS datasets 
True
[ ]
# Load in the data (takes about 5-6 minutes in Google Colab)
(train_data, test_data), ds_info = tfds.load(name="food101", # target dataset to get from TFDS
                                             split=["train", "validation"], # what splits of data should we get? note: not all datasets have train, valid, test
                                             shuffle_files=False, # shuffle files on download?
                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)
                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info

explore the food101 from data from tensorflow datasets
to become one with our data, we want to find:

class_name
shape of our input data(image tensors)
datatype of our input data
what does the label look like(e.g are they one-hot encoded or are they label encoded)
do labels match up with class name
[ ]
#features of food 101
ds_info.features
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=uint8),
    'label': ClassLabel(shape=(), dtype=int64, num_classes=101),
})
[ ]
#get the classnames
class_names=ds_info.features["label"].names
class_names
['apple_pie',
 'baby_back_ribs',
 'baklava',
 'beef_carpaccio',
 'beef_tartare',
 'beet_salad',
 'beignets',
 'bibimbap',
 'bread_pudding',
 'breakfast_burrito',
 'bruschetta',
 'caesar_salad',
 'cannoli',
 'caprese_salad',
 'carrot_cake',
 'ceviche',
 'cheesecake',
 'cheese_plate',
 'chicken_curry',
 'chicken_quesadilla',
 'chicken_wings',
 'chocolate_cake',
 'chocolate_mousse',
 'churros',
 'clam_chowder',
 'club_sandwich',
 'crab_cakes',
 'creme_brulee',
 'croque_madame',
 'cup_cakes',
 'deviled_eggs',
 'donuts',
 'dumplings',
 'edamame',
 'eggs_benedict',
 'escargots',
 'falafel',
 'filet_mignon',
 'fish_and_chips',
 'foie_gras',
 'french_fries',
 'french_onion_soup',
 'french_toast',
 'fried_calamari',
 'fried_rice',
 'frozen_yogurt',
 'garlic_bread',
 'gnocchi',
 'greek_salad',
 'grilled_cheese_sandwich',
 'grilled_salmon',
 'guacamole',
 'gyoza',
 'hamburger',
 'hot_and_sour_soup',
 'hot_dog',
 'huevos_rancheros',
 'hummus',
 'ice_cream',
 'lasagna',
 'lobster_bisque',
 'lobster_roll_sandwich',
 'macaroni_and_cheese',
 'macarons',
 'miso_soup',
 'mussels',
 'nachos',
 'omelette',
 'onion_rings',
 'oysters',
 'pad_thai',
 'paella',
 'pancakes',
 'panna_cotta',
 'peking_duck',
 'pho',
 'pizza',
 'pork_chop',
 'poutine',
 'prime_rib',
 'pulled_pork_sandwich',
 'ramen',
 'ravioli',
 'red_velvet_cake',
 'risotto',
 'samosa',
 'sashimi',
 'scallops',
 'seaweed_salad',
 'shrimp_and_grits',
 'spaghetti_bolognese',
 'spaghetti_carbonara',
 'spring_rolls',
 'steak',
 'strawberry_shortcake',
 'sushi',
 'tacos',
 'takoyaki',
 'tiramisu',
 'tuna_tartare',
 'waffles']
[ ]
#take one sample of the train data
train_one_sample=train_data.take(1) #sample are in format (image_tensor,label)
[ ]
#what does one sample of our training data looks like
train_one_sample
<TakeDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>
[ ]
#output info about our training sample
for image, label in train_one_sample:
  print(f"""
      image Shape: {image.shape}
      image datatype: {image.dtype}
      target class from food101(tensor form): {label}
      class name(str form): {class_names[label.numpy()]}
  """)

      image Shape: (512, 512, 3)
      image datatype: <dtype: 'uint8'>
      target class from food101(tensor form): 65
      class name(str form): mussels
  
[ ]
#what does our FTDS's food101 looks like?
image
<tf.Tensor: shape=(512, 512, 3), dtype=uint8, numpy=
array([[[ 19,   6,   0],
        [ 29,  16,   8],
        [ 34,  21,  13],
        ...,
        [ 36,  19,   3],
        [ 35,  19,   3],
        [ 36,  20,   4]],

       [[ 20,   7,   0],
        [ 28,  15,   7],
        [ 34,  21,  13],
        ...,
        [ 37,  20,   4],
        [ 37,  20,   4],
        [ 37,  21,   5]],

       [[ 20,   7,   0],
        [ 25,  12,   4],
        [ 31,  18,  10],
        ...,
        [ 40,  21,   4],
        [ 40,  21,   4],
        [ 39,  22,   4]],

       ...,

       [[208, 206, 194],
        [209, 207, 195],
        [211, 209, 197],
        ...,
        [192, 196, 179],
        [190, 194, 177],
        [203, 207, 190]],

       [[208, 206, 194],
        [209, 207, 195],
        [210, 208, 196],
        ...,
        [203, 207, 190],
        [211, 214, 197],
        [202, 205, 186]],

       [[208, 206, 194],
        [209, 207, 195],
        [210, 208, 196],
        ...,
        [200, 204, 187],
        [201, 204, 185],
        [198, 201, 182]]], dtype=uint8)>
[ ]
#which can be assume as the min and max value of our image?
import tensorflow as tf
tf.reduce_min(image),tf.reduce_max(image)
(<tf.Tensor: shape=(), dtype=uint8, numpy=0>,
 <tf.Tensor: shape=(), dtype=uint8, numpy=255>)
plot an image from tensorflow datasets
[ ]
#import tensor as matplotlib
import matplotlib.pyplot as plt
plt.imshow(image)
plt.title(class_names[label.numpy()]) #add title to verify our image
plt.axis(False)

create preprocessing function for our data
neural networks works the best when our data is in a certain way(e.g - batches,normalized,etc).

However not all data (including the tensorflow datasets) comes like this.

in order to get it ready in the neural network , you will often have to write preprocessing functions and map it on our data.

what we know about our data:

uint_8 datatype
comprized of different size tensors(different sized images)
Not scaled (the pixel are between 0 and 255)
what we know models like:

data in float_32 dtype (or for mixed precision float16 and float32)
for batches, tensorflow likes all of the tensorflow within a batch of the same size
scaled (values between 0 and 1): also called normalized tensor performs better
with these point learned we got a few things we can tackle with a preprocessing data function.

since we are using efficientNetB0 our data will already be scaled inbuild

This mean we need to:

Reshape our images to all the same size
convert the dtype of our image from unit_8 to float_32
[ ]
# Make a function for preprocessing images
def preprocess_img(image, label, img_shape=224):
  """
  Converts image datatype from 'uint8' -> 'float32' and reshapes
  image to [img_shape, img_shape, colour_channels]
  """
  image = tf.image.resize(image, [img_shape, img_shape]) # reshape target image
  # image = image/255. # scale image values (not required with EfficientNetBX models from tf.keras.applications)
  return tf.cast(image, tf.float32), label # return (float32_image, label) tuple
[ ]
#preprocess the single sample image to preprocess the outputs 
preprocessed_img=preprocess_img(image,label)[0]
print(f"image before preprocessing :\n {image[:2]}..., \nshape: {image.shape},\nDatatype: {image.dtype}")
print(f"image after preprocessing :\n {preprocessed_img[:2]}..., \nshape: {preprocessed_img.shape},\nDatatype: {preprocessed_img.dtype}")
image before preprocessing :
 [[[19  6  0]
  [29 16  8]
  [34 21 13]
  ...
  [36 19  3]
  [35 19  3]
  [36 20  4]]

 [[20  7  0]
  [28 15  7]
  [34 21 13]
  ...
  [37 20  4]
  [37 20  4]
  [37 21  5]]]..., 
shape: (512, 512, 3),
Datatype: <dtype: 'uint8'>
image after preprocessing :
 [[[25.244898   12.244898    4.7295923 ]
  [32.40816    19.408163   11.408163  ]
  [32.719387   19.719387   10.719387  ]
  ...
  [39.005093   20.994862    3.4999783 ]
  [36.31123    19.31123     2.1173685 ]
  [36.413277   20.000034    4.000035  ]]

 [[19.897957    6.897958    0.18367267]
  [28.943878   15.943878    7.9438777 ]
  [22.112244    9.112244    1.1122437 ]
  ...
  [48.89794    27.015284    6.1428356 ]
  [44.642845   23.852028    3.0714417 ]
  [42.7602     21.954084    3.0714283 ]]]..., 
shape: (224, 224, 3),
Datatype: <dtype: 'float32'>
batch and prepare datasets
[ ]
# Map preprocessing function to training (and parallelize)
train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)
# Shuffle train_data and turn it into batches and prefetch it (load it faster)
train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)

# Map preprocessing function to test data
test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)
[ ]
train_data,test_data
(<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,
 <PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>)
create some modelling callbacks
we are going to create a couple of callbacks to help us with our our model trains:

Tensorboard callback to log train results do we can visuzalize them later
Modelcheckpoint callbacks to save our model progress after each extraction
[ ]
###The create_tensorboard_callback function is given below we can recalled it using our import function using the helper function
#(code is provided for helpingto understand the function by the viewer)

#import datetime

#def create_tensorboard_callback(dir_name, experiment_name):
#  """
#  Creates a TensorBoard callback instand to store log files.

#  Stores log files with the filepath:
#    "dir_name/experiment_name/current_datetime/"

#  Args:
#    dir_name: target directory to store TensorBoard log files
#    experiment_name: name of experiment directory (e.g. efficientnet_model_1)
#  """
#  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
#  tensorboard_callback = tf.keras.callbacks.TensorBoard(
#      log_dir=log_dir
#  )
#  print(f"Saving TensorBoard log files to: {log_dir}")
#  return tensorboard_callback
[ ]
#create a modelcheckpoint callback to save a model progress during training
checkpoint_path="model_checkpoint/cp.cpkt"
model_checkpoint=tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                    monitor="val_accuracy",
                                                    save_best_only=True,
                                                    save_weights_only=True,
                                                    verbose=0) # don't print whether the model is being saved or not
setup mixed processing training
first and foremost for a deeper understanding of mixed processing training, checkout the tensorflow guide for the mixed processing:- https://www.tensorflow.org/guide/mixed_precision

mixed processing utilize the combination of float32 and float16 datatype to speed up model performance.

[ ]
#turn on mixed processing training
#from tensorflow.keras import mixed_precision
#mixed_precision.set_global_policy("mixed_float16") #set global policy to mixed precision
[ ]
#mixed_precision.global_policy()
#mixedm preocessing not working in the new tensorflow update
build the features extraction model
[ ]
from tensorflow.keras import layers

# Create base model
input_shape = (224, 224, 3)
base_model = tf.keras.applications.EfficientNetB0(include_top=False)
base_model.trainable = False # freeze base model layers

# Create Functional model 
inputs = layers.Input(shape=input_shape, name="input_layer")
# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below
# x = layers.Rescaling(1./255)(x)
x = base_model(inputs, training=False) # set base_model to inference mode only
x = layers.GlobalAveragePooling2D(name="pooling_layer")(x)
x = layers.Dense(len(class_names))(x) # want one output neuron per class 
# Separate activation of output layer so we can output float32 activations
outputs = layers.Activation("softmax", dtype=tf.float32, name="softmax_float32")(x) 
model = tf.keras.Model(inputs, outputs)

# Compile the model
model.compile(loss="sparse_categorical_crossentropy", # Use sparse_categorical_crossentropy when labels are *not* one-hot
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])
Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5
16705208/16705208 [==============================] - 0s 0us/step
[ ]
model.summary()
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_layer (InputLayer)    [(None, 224, 224, 3)]     0         
                                                                 
 efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  
                                                                 
 pooling_layer (GlobalAverag  (None, 1280)             0         
 ePooling2D)                                                     
                                                                 
 dense (Dense)               (None, 101)               129381    
                                                                 
 softmax_float32 (Activation  (None, 101)              0         
 )                                                               
                                                                 
=================================================================
Total params: 4,178,952
Trainable params: 129,381
Non-trainable params: 4,049,571
_________________________________________________________________
[ ]
label
<tf.Tensor: shape=(), dtype=int64, numpy=65>
checking dtype policies(are we using mixed precision or not)
[ ]
# sadly for the new update mixed-precison has seen some error after 2.9.2 update and 
#lack of a good GPU perfromance is not running so the code given below will only work when mixed precision is in play
# Check the dtype_policy attributes of layers in our model
#for layer in model.layers:
#  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)
if the above code had runned properly we had get the output as

input_layer True float32 <Policy "float32">
efficientnetb0 False float32 <Policy "mixed_float16">
global_average_pooling2d True float32 <Policy "mixed_float16">
dense True float32 <Policy "mixed_float16">
softmax_float32 True float32 <Policy "float32">
Going through the above we see:

layer.name: the human readable name of a particular layer
layer.trainable: is the layer trainable or not? (if False, the weights are frozen)
layer.dtype: the data type a layer stores its variables in
layer.dtype_policy: the data type policy a layer computes on its variables with
[ ]
#again sadly for the new update mixed-precison has seen some error after 2.9.2 update and 
#lack of a good GPU perfromance is not running so the code given below will only work when mixed precision is in play
# Check the dtype_policy attributes of layers in the base model
#for layer in model.layers[1].layers[:20]: # check the layers of the base model (layer at index 1 of `model`)
#  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)
fit the extraction model
If our goal is to fine-tune a feature extraction model, the general rule or rule of doing thinks is :

build a feature extraction model(train a a couple output layers with base model frozen)
fine-tune some of the froze layers
[ ]
#fit the model
history_101_feature_extraction=model.fit(train_data,
                                         epochs=3,
                                         steps_per_epoch=(len(train_data)),
                                         validation_data=test_data,
                                         validation_steps=(0.15*len(test_data)),
                                         callbacks=[create_tensorboard_callback(dir_name="training_logs",
                                                                                experiment_name="efficientNet_feature_extraction"),
                                                    model_checkpoint])
Saving TensorBoard log files to: training_logs/efficientNet_feature_extraction/20230111-023941
Epoch 1/3
2368/2368 [==============================] - 3383s 1s/step - loss: 1.7155 - accuracy: 0.5807 - val_loss: 1.1322 - val_accuracy: 0.6943
Epoch 2/3
2368/2368 [==============================] - 3622s 2s/step - loss: 1.1994 - accuracy: 0.6878 - val_loss: 1.0285 - val_accuracy: 0.7172
Epoch 3/3
2368/2368 [==============================] - 3488s 1s/step - loss: 1.0539 - accuracy: 0.7250 - val_loss: 0.9936 - val_accuracy: 0.7258
[ ]
model.save("/content/drive/MyDrive/tensorflow/efficientNet_feature_extraction")

[ ]
#load our model
import tensorflow as tf
load_feature_extraction=tf.keras.models.load_model("/content/drive/MyDrive/tensorflow/efficientNet_feature_extraction")
[ ]
load_feature_extraction.summary()
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_layer (InputLayer)    [(None, 224, 224, 3)]     0         
                                                                 
 efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  
                                                                 
 pooling_layer (GlobalAverag  (None, 1280)             0         
 ePooling2D)                                                     
                                                                 
 dense (Dense)               (None, 101)               129381    
                                                                 
 softmax_float32 (Activation  (None, 101)              0         
 )                                                               
                                                                 
=================================================================
Total params: 4,178,952
Trainable params: 129,381
Non-trainable params: 4,049,571
_________________________________________________________________
[ ]
#evaluating the feature model
featute_extraction_evaluation=load_feature_extraction.evaluate(test_data)
featute_extraction_evaluation
790/790 [==============================] - 63s 68ms/step - loss: 1.0040 - accuracy: 0.7254
[1.0039913654327393, 0.7253861427307129]
Early stopping callback
etup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs

[ ]
#setup the early stopping callback
early_stopping=tf.keras.callbacks.EarlyStopping(monitor="val_loss", #watch the val loss metrics
                                               patience=3) #if val loss decrease for 3 epochs in a row stop training
#create checkpoint for our modelcheckpoint callback
checkpoint_path="fine_tune_checkpoints/"
model_checkpoint=tf.keras.callbacks.ModelCheckpoint(checkpoint_path)
[ ]
#create learning rate reduction callback
reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss",
                                               factor=0.2,# multiply the learning factor by 0.2
                                               patience=2,
                                               verbose=1,
                                               min_lr=1e-7)
[ ]
#unfreeze our base model layers
for layer in load_feature_extraction.layers:
    layer.trainable = True
[ ]
# Check the layers in the base model and see what  they're using
for layer in load_feature_extraction.layers[1].layers[:20]:
    print(layer.name, layer.trainable, layer.dtype)
input_1 True float32
rescaling True float32
normalization True float32
tf.math.truediv True float32
stem_conv_pad True float32
stem_conv True float32
stem_bn True float32
stem_activation True float32
block1a_dwconv True float32
block1a_bn True float32
block1a_activation True float32
block1a_se_squeeze True float32
block1a_se_reshape True float32
block1a_se_reduce True float32
block1a_se_expand True float32
block1a_se_excite True float32
block1a_project_conv True float32
block1a_project_bn True float32
block2a_expand_conv True float32
block2a_expand_bn True float32
[ ]
# Compile the model
load_feature_extraction.compile(loss="sparse_categorical_crossentropy", 
                        optimizer=tf.keras.optimizers.Adam(0.0001),
                        metrics=["accuracy"])
[ ]
#fit our model
history_tune_model=load_feature_extraction.fit(train_data,
                            epochs=10,
                            validation_data=test_data,
                            validation_steps=int(0.15*len(test_data)),
                            callbacks=[create_tensorboard_callback("training_log",
                                                                  "fine_tune_model"),
                                      model_checkpoint,
                                      reduce_lr ,
                                      early_stopping])
Saving TensorBoard log files to: training_log/fine_tune_model/20230111-143751
Epoch 1/10
2368/2368 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9936WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.
2368/2368 [==============================] - 821s 346ms/step - loss: 0.0345 - accuracy: 0.9936 - val_loss: 1.0130 - val_accuracy: 0.8056 - lr: 2.0000e-05
Epoch 2/10
2368/2368 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9982WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.
2368/2368 [==============================] - 828s 349ms/step - loss: 0.0143 - accuracy: 0.9982 - val_loss: 1.1479 - val_accuracy: 0.8022 - lr: 2.0000e-05
Epoch 3/10
2368/2368 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9992WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.

Epoch 3: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.
2368/2368 [==============================] - 825s 347ms/step - loss: 0.0074 - accuracy: 0.9992 - val_loss: 1.2899 - val_accuracy: 0.8006 - lr: 2.0000e-05
Epoch 4/10
2368/2368 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9999WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.
2368/2368 [==============================] - 821s 346ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 1.3552 - val_accuracy: 0.8019 - lr: 4.0000e-06
[ ]
#save our fine tune model for future use
load_feature_extraction.save("/content/drive/MyDrive/tensorflow/efficientNet_fine_tune")

[ ]
#import the fine tune model
import tensorflow as tf
load_fine_tune=tf.keras.models.load_model("/content/drive/MyDrive/tensorflow/efficientNet_fine_tune")
[ ]
#evaluate the model
fine_tune_evaluate=load_fine_tune.evaluate(test_data)
790/790 [==============================] - 1628s 2s/step - loss: 1.3912 - accuracy: 0.8001
[ ]
# Download some custom images from Google Storage
# Note: you can upload your own custom images to Google Colab using the "upload" button in the Files tab
!wget https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip

unzip_data("custom_food_images.zip") 
--2023-01-11 18:30:38--  https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip
Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.128, 172.253.123.128, 142.250.98.128, ...
Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13192985 (13M) [application/zip]
Saving to: ‘custom_food_images.zip.1’

custom_food_images. 100%[===================>]  12.58M  --.-KB/s    in 0.1s    

2023-01-11 18:30:38 (108 MB/s) - ‘custom_food_images.zip.1’ saved [13192985/13192985]

creating our prediction
[ ]
import os
# Get custom food images filepaths
custom_food_images = ["custom_food_images/" + img_path for img_path in os.listdir("custom_food_images")]
custom_food_images     
['custom_food_images/hamburger.jpeg',
 'custom_food_images/chicken_wings.jpeg',
 'custom_food_images/ramen.jpeg',
 'custom_food_images/sushi.jpeg',
 'custom_food_images/pizza-dad.jpeg',
 'custom_food_images/steak.jpeg']
[ ]
# Make predictions on custom food images
for img in custom_food_images:
  img = load_and_prep_image(img, scale=False) # load in target image and turn it into tensor
  pred_prob = load_fine_tune.predict(tf.expand_dims(img, axis=0)) # make prediction on image with shape [None, 224, 224, 3]
  pred_class = class_names[pred_prob.argmax()] # find the predicted class label
  # Plot the image with appropriate annotations
  plt.figure()
  plt.imshow(img/255.) # imshow() requires float inputs to be normalized
  plt.title(f"pred: {pred_class} \n, prob: {pred_prob.max():.2f} \n,Real label: {pred_class}")
  plt.axis(False)

Colab paid products - Cancel contracts here
