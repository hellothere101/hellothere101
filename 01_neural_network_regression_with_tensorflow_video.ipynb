
01_neural_network_regression_with_tensorflow_video.ipynb
01_neural_network_regression_with_tensorflow_video.ipynbC_
Introduction to regression with neural network using tensorflow.
There are many definition for regression problem but in our case case, we are goingh to simplify it. predicting a numerical variable based on combination of variable,.. even shorter predicting number

[1]
3s
#import tensorflow 
import tensorflow as tf
print(tf.__version__)
2.9.2
[2]
0s
import numpy as np
import matplotlib.pyplot as plt

#create features
X=np.array([-7.0,-4.0,-1.0,2.0,5.0,8.0,11.0,14.0])

#create labels
Y=np.array([3.0,6.0,9.0,12.0,15.0,18.0,21.0,24.0])

#Visualize it
plt.scatter(X,Y)

[3]
0s
Y==X+10
array([ True,  True,  True,  True,  True,  True,  True,  True])
Input shapes and output shapes
[4]
0s
#let us create a demo tensor for house predicting problem
house_info=tf.constant(["bedroom","bathroom","garage"])
house_price=tf.constant([939700])
house_info,house_price
(<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'bedroom', b'bathroom', b'garage'], dtype=object)>,
 <tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700], dtype=int32)>)
[5]
0s
X[0],Y[0]
(-7.0, 3.0)
[6]
0s
X[1],Y[1]
(-4.0, 6.0)
[7]
0s
input_shape=X[0].shape
output_shape=Y[0].shape
input_shape,output_shape
((), ())
[8]
1s
X[0].ndim
0
[9]
0s
#let us turn our numpy array to tensors
X=tf.cast(tf.constant(X),dtype=tf.float32)
Y=tf.cast(tf.constant(Y),dtype=tf.float32)
X,Y
(<tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)>,
 <tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)>)
[10]
0s
input_shape=X[0].shape
output_shape=Y[0].shape
input_shape,output_shape
(TensorShape([]), TensorShape([]))
steps with modelling with Tensorflow
create a model - define the input and output layers, as well as the hidden layer of a deep learning model

compile a model - define the loss function.(in other words, the function which tells our model how wrong it is) and the optimizer (tells the model how to improve the patterns it is learning) and evaluation metrics.(what can we use to intepret out model)

fitting a model - letting our model try to find patterns between X and Y(features and labels)

[11]
0s
#set random seed
tf.random.set_seed(42)

#1. create the model using the sequential API
model=tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])

#2. compile the model
model.compile(loss=tf.keras.losses.mae, #mae- mean absolute error
              optimizer=tf.keras.optimizers.SGD(),#SGD stands for stochastic gardient descend)
              metrics=["mae"])

#3. fit the model
model.fit(tf.expand_dims(X, axis=-1), Y, epochs=5)
Epoch 1/5
1/1 [==============================] - 1s 551ms/step - loss: 11.5048 - mae: 11.5048
Epoch 2/5
1/1 [==============================] - 0s 9ms/step - loss: 11.3723 - mae: 11.3723
Epoch 3/5
1/1 [==============================] - 0s 10ms/step - loss: 11.2398 - mae: 11.2398
Epoch 4/5
1/1 [==============================] - 0s 6ms/step - loss: 11.1073 - mae: 11.1073
Epoch 5/5
1/1 [==============================] - 0s 24ms/step - loss: 10.9748 - mae: 10.9748
<keras.callbacks.History at 0x7f07f665f070>
[12]
0s
#check X and Y
X,Y
(<tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)>,
 <tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)>)
[13]
0s
#predict using our model
model.predict([17.0])
1/1 [==============================] - 0s 94ms/step
array([[12.716021]], dtype=float32)
Improving our model
we are improve our model by atlering the steps we took to create our model.

creating a model - add more sequential layers, increasing the no of hidden units(also called neurons). within ecah of hidden layers, changing the activation function of each layer

compile a model- Here we might change the optimization function or the or the learning rate of the optimization function

Fitting a model- model have more epochs, leave it training for longer. or give more data to our model

[14]
2s
#let's us rebuild our model

#create our model
model=tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])

#compile our model
model.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["mae"])

#fitting our model
model.fit(tf.expand_dims(X,axis=-1),Y,epochs=100)
Epoch 1/100
1/1 [==============================] - 0s 439ms/step - loss: 11.2219 - mae: 11.2219
Epoch 2/100
1/1 [==============================] - 0s 8ms/step - loss: 11.0894 - mae: 11.0894
Epoch 3/100
1/1 [==============================] - 0s 12ms/step - loss: 10.9569 - mae: 10.9569
Epoch 4/100
1/1 [==============================] - 0s 9ms/step - loss: 10.8244 - mae: 10.8244
Epoch 5/100
1/1 [==============================] - 0s 10ms/step - loss: 10.6919 - mae: 10.6919
Epoch 6/100
1/1 [==============================] - 0s 8ms/step - loss: 10.5594 - mae: 10.5594
Epoch 7/100
1/1 [==============================] - 0s 6ms/step - loss: 10.4269 - mae: 10.4269
Epoch 8/100
1/1 [==============================] - 0s 6ms/step - loss: 10.2944 - mae: 10.2944
Epoch 9/100
1/1 [==============================] - 0s 6ms/step - loss: 10.1619 - mae: 10.1619
Epoch 10/100
1/1 [==============================] - 0s 9ms/step - loss: 10.0294 - mae: 10.0294
Epoch 11/100
1/1 [==============================] - 0s 6ms/step - loss: 9.8969 - mae: 9.8969
Epoch 12/100
1/1 [==============================] - 0s 7ms/step - loss: 9.7644 - mae: 9.7644
Epoch 13/100
1/1 [==============================] - 0s 7ms/step - loss: 9.6319 - mae: 9.6319
Epoch 14/100
1/1 [==============================] - 0s 6ms/step - loss: 9.4994 - mae: 9.4994
Epoch 15/100
1/1 [==============================] - 0s 7ms/step - loss: 9.3669 - mae: 9.3669
Epoch 16/100
1/1 [==============================] - 0s 6ms/step - loss: 9.2344 - mae: 9.2344
Epoch 17/100
1/1 [==============================] - 0s 8ms/step - loss: 9.1019 - mae: 9.1019
Epoch 18/100
1/1 [==============================] - 0s 20ms/step - loss: 8.9694 - mae: 8.9694
Epoch 19/100
1/1 [==============================] - 0s 8ms/step - loss: 8.8369 - mae: 8.8369
Epoch 20/100
1/1 [==============================] - 0s 11ms/step - loss: 8.7044 - mae: 8.7044
Epoch 21/100
1/1 [==============================] - 0s 11ms/step - loss: 8.5719 - mae: 8.5719
Epoch 22/100
1/1 [==============================] - 0s 8ms/step - loss: 8.4394 - mae: 8.4394
Epoch 23/100
1/1 [==============================] - 0s 7ms/step - loss: 8.3069 - mae: 8.3069
Epoch 24/100
1/1 [==============================] - 0s 6ms/step - loss: 8.1744 - mae: 8.1744
Epoch 25/100
1/1 [==============================] - 0s 7ms/step - loss: 8.0419 - mae: 8.0419
Epoch 26/100
1/1 [==============================] - 0s 10ms/step - loss: 7.9094 - mae: 7.9094
Epoch 27/100
1/1 [==============================] - 0s 6ms/step - loss: 7.7769 - mae: 7.7769
Epoch 28/100
1/1 [==============================] - 0s 6ms/step - loss: 7.6444 - mae: 7.6444
Epoch 29/100
1/1 [==============================] - 0s 6ms/step - loss: 7.5119 - mae: 7.5119
Epoch 30/100
1/1 [==============================] - 0s 17ms/step - loss: 7.3794 - mae: 7.3794
Epoch 31/100
1/1 [==============================] - 0s 8ms/step - loss: 7.2750 - mae: 7.2750
Epoch 32/100
1/1 [==============================] - 0s 7ms/step - loss: 7.2694 - mae: 7.2694
Epoch 33/100
1/1 [==============================] - 0s 8ms/step - loss: 7.2638 - mae: 7.2638
Epoch 34/100
1/1 [==============================] - 0s 8ms/step - loss: 7.2581 - mae: 7.2581
Epoch 35/100
1/1 [==============================] - 0s 12ms/step - loss: 7.2525 - mae: 7.2525
Epoch 36/100
1/1 [==============================] - 0s 8ms/step - loss: 7.2469 - mae: 7.2469
Epoch 37/100
1/1 [==============================] - 0s 6ms/step - loss: 7.2412 - mae: 7.2412
Epoch 38/100
1/1 [==============================] - 0s 7ms/step - loss: 7.2356 - mae: 7.2356
Epoch 39/100
1/1 [==============================] - 0s 7ms/step - loss: 7.2300 - mae: 7.2300
Epoch 40/100
1/1 [==============================] - 0s 7ms/step - loss: 7.2244 - mae: 7.2244
Epoch 41/100
1/1 [==============================] - 0s 7ms/step - loss: 7.2188 - mae: 7.2188
Epoch 42/100
1/1 [==============================] - 0s 6ms/step - loss: 7.2131 - mae: 7.2131
Epoch 43/100
1/1 [==============================] - 0s 9ms/step - loss: 7.2075 - mae: 7.2075
Epoch 44/100
1/1 [==============================] - 0s 7ms/step - loss: 7.2019 - mae: 7.2019
Epoch 45/100
1/1 [==============================] - 0s 8ms/step - loss: 7.1962 - mae: 7.1962
Epoch 46/100
1/1 [==============================] - 0s 6ms/step - loss: 7.1906 - mae: 7.1906
Epoch 47/100
1/1 [==============================] - 0s 7ms/step - loss: 7.1850 - mae: 7.1850
Epoch 48/100
1/1 [==============================] - 0s 8ms/step - loss: 7.1794 - mae: 7.1794
Epoch 49/100
1/1 [==============================] - 0s 6ms/step - loss: 7.1737 - mae: 7.1737
Epoch 50/100
1/1 [==============================] - 0s 8ms/step - loss: 7.1681 - mae: 7.1681
Epoch 51/100
1/1 [==============================] - 0s 6ms/step - loss: 7.1625 - mae: 7.1625
Epoch 52/100
1/1 [==============================] - 0s 7ms/step - loss: 7.1569 - mae: 7.1569
Epoch 53/100
1/1 [==============================] - 0s 33ms/step - loss: 7.1512 - mae: 7.1512
Epoch 54/100
1/1 [==============================] - 0s 55ms/step - loss: 7.1456 - mae: 7.1456
Epoch 55/100
1/1 [==============================] - 0s 9ms/step - loss: 7.1400 - mae: 7.1400
Epoch 56/100
1/1 [==============================] - 0s 7ms/step - loss: 7.1344 - mae: 7.1344
Epoch 57/100
1/1 [==============================] - 0s 7ms/step - loss: 7.1287 - mae: 7.1287
Epoch 58/100
1/1 [==============================] - 0s 7ms/step - loss: 7.1231 - mae: 7.1231
Epoch 59/100
1/1 [==============================] - 0s 7ms/step - loss: 7.1175 - mae: 7.1175
Epoch 60/100
1/1 [==============================] - 0s 8ms/step - loss: 7.1119 - mae: 7.1119
Epoch 61/100
1/1 [==============================] - 0s 6ms/step - loss: 7.1062 - mae: 7.1062
Epoch 62/100
1/1 [==============================] - 0s 5ms/step - loss: 7.1006 - mae: 7.1006
Epoch 63/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0950 - mae: 7.0950
Epoch 64/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0894 - mae: 7.0894
Epoch 65/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0838 - mae: 7.0838
Epoch 66/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0781 - mae: 7.0781
Epoch 67/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0725 - mae: 7.0725
Epoch 68/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0669 - mae: 7.0669
Epoch 69/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0613 - mae: 7.0613
Epoch 70/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0556 - mae: 7.0556
Epoch 71/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0500 - mae: 7.0500
Epoch 72/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0444 - mae: 7.0444
Epoch 73/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0388 - mae: 7.0388
Epoch 74/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0331 - mae: 7.0331
Epoch 75/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0275 - mae: 7.0275
Epoch 76/100
1/1 [==============================] - 0s 6ms/step - loss: 7.0219 - mae: 7.0219
Epoch 77/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0163 - mae: 7.0163
Epoch 78/100
1/1 [==============================] - 0s 5ms/step - loss: 7.0106 - mae: 7.0106
Epoch 79/100
1/1 [==============================] - 0s 6ms/step - loss: 7.0050 - mae: 7.0050
Epoch 80/100
1/1 [==============================] - 0s 5ms/step - loss: 6.9994 - mae: 6.9994
Epoch 81/100
1/1 [==============================] - 0s 6ms/step - loss: 6.9938 - mae: 6.9938
Epoch 82/100
1/1 [==============================] - 0s 57ms/step - loss: 6.9881 - mae: 6.9881
Epoch 83/100
1/1 [==============================] - 0s 40ms/step - loss: 6.9825 - mae: 6.9825
Epoch 84/100
1/1 [==============================] - 0s 7ms/step - loss: 6.9769 - mae: 6.9769
Epoch 85/100
1/1 [==============================] - 0s 7ms/step - loss: 6.9713 - mae: 6.9713
Epoch 86/100
1/1 [==============================] - 0s 7ms/step - loss: 6.9656 - mae: 6.9656
Epoch 87/100
1/1 [==============================] - 0s 7ms/step - loss: 6.9600 - mae: 6.9600
Epoch 88/100
1/1 [==============================] - 0s 7ms/step - loss: 6.9544 - mae: 6.9544
Epoch 89/100
1/1 [==============================] - 0s 7ms/step - loss: 6.9488 - mae: 6.9488
Epoch 90/100
1/1 [==============================] - 0s 6ms/step - loss: 6.9431 - mae: 6.9431
Epoch 91/100
1/1 [==============================] - 0s 7ms/step - loss: 6.9375 - mae: 6.9375
Epoch 92/100
1/1 [==============================] - 0s 6ms/step - loss: 6.9319 - mae: 6.9319
Epoch 93/100
1/1 [==============================] - 0s 6ms/step - loss: 6.9263 - mae: 6.9263
Epoch 94/100
1/1 [==============================] - 0s 6ms/step - loss: 6.9206 - mae: 6.9206
Epoch 95/100
1/1 [==============================] - 0s 6ms/step - loss: 6.9150 - mae: 6.9150
Epoch 96/100
1/1 [==============================] - 0s 6ms/step - loss: 6.9094 - mae: 6.9094
Epoch 97/100
1/1 [==============================] - 0s 6ms/step - loss: 6.9038 - mae: 6.9038
Epoch 98/100
1/1 [==============================] - 0s 6ms/step - loss: 6.8981 - mae: 6.8981
Epoch 99/100
1/1 [==============================] - 0s 5ms/step - loss: 6.8925 - mae: 6.8925
Epoch 100/100
1/1 [==============================] - 0s 5ms/step - loss: 6.8869 - mae: 6.8869
<keras.callbacks.History at 0x7f07f4ddeb20>
[15]
0s
#let's us see if our model prediction has improved or not
model.predict([17.0])
1/1 [==============================] - 0s 48ms/step
array([[29.739855]], dtype=float32)
[16]
1s
#experiment change

#create a model
model= tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])

#compile a model
model.compile(loss=tf.keras.losses.mae,
              optimizer= tf.keras.optimizers.Adam(lr=0.0001),
              metrics=['mae']
             )

#fitting a model
model.fit(tf.expand_dims(X,axis=-1),Y ,epochs=100)
Epoch 1/100
/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
1/1 [==============================] - 0s 323ms/step - loss: 10.5736 - mae: 10.5736
Epoch 2/100
1/1 [==============================] - 0s 8ms/step - loss: 10.5731 - mae: 10.5731
Epoch 3/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5727 - mae: 10.5727
Epoch 4/100
1/1 [==============================] - 0s 8ms/step - loss: 10.5722 - mae: 10.5722
Epoch 5/100
1/1 [==============================] - 0s 8ms/step - loss: 10.5718 - mae: 10.5718
Epoch 6/100
1/1 [==============================] - 0s 14ms/step - loss: 10.5713 - mae: 10.5713
Epoch 7/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5709 - mae: 10.5709
Epoch 8/100
1/1 [==============================] - 0s 9ms/step - loss: 10.5704 - mae: 10.5704
Epoch 9/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5700 - mae: 10.5700
Epoch 10/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5695 - mae: 10.5695
Epoch 11/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5691 - mae: 10.5691
Epoch 12/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5686 - mae: 10.5686
Epoch 13/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5682 - mae: 10.5682
Epoch 14/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5677 - mae: 10.5677
Epoch 15/100
1/1 [==============================] - 0s 11ms/step - loss: 10.5673 - mae: 10.5673
Epoch 16/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5668 - mae: 10.5668
Epoch 17/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5664 - mae: 10.5664
Epoch 18/100
1/1 [==============================] - 0s 7ms/step - loss: 10.5659 - mae: 10.5659
Epoch 19/100
1/1 [==============================] - 0s 10ms/step - loss: 10.5655 - mae: 10.5655
Epoch 20/100
1/1 [==============================] - 0s 8ms/step - loss: 10.5650 - mae: 10.5650
Epoch 21/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5646 - mae: 10.5646
Epoch 22/100
1/1 [==============================] - 0s 11ms/step - loss: 10.5641 - mae: 10.5641
Epoch 23/100
1/1 [==============================] - 0s 7ms/step - loss: 10.5637 - mae: 10.5637
Epoch 24/100
1/1 [==============================] - 0s 8ms/step - loss: 10.5632 - mae: 10.5632
Epoch 25/100
1/1 [==============================] - 0s 10ms/step - loss: 10.5628 - mae: 10.5628
Epoch 26/100
1/1 [==============================] - 0s 7ms/step - loss: 10.5623 - mae: 10.5623
Epoch 27/100
1/1 [==============================] - 0s 10ms/step - loss: 10.5619 - mae: 10.5619
Epoch 28/100
1/1 [==============================] - 0s 13ms/step - loss: 10.5614 - mae: 10.5614
Epoch 29/100
1/1 [==============================] - 0s 13ms/step - loss: 10.5610 - mae: 10.5610
Epoch 30/100
1/1 [==============================] - 0s 23ms/step - loss: 10.5605 - mae: 10.5605
Epoch 31/100
1/1 [==============================] - 0s 7ms/step - loss: 10.5601 - mae: 10.5601
Epoch 32/100
1/1 [==============================] - 0s 12ms/step - loss: 10.5596 - mae: 10.5596
Epoch 33/100
1/1 [==============================] - 0s 7ms/step - loss: 10.5592 - mae: 10.5592
Epoch 34/100
1/1 [==============================] - 0s 7ms/step - loss: 10.5587 - mae: 10.5587
Epoch 35/100
1/1 [==============================] - 0s 11ms/step - loss: 10.5583 - mae: 10.5583
Epoch 36/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5578 - mae: 10.5578
Epoch 37/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5574 - mae: 10.5574
Epoch 38/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5569 - mae: 10.5569
Epoch 39/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5565 - mae: 10.5565
Epoch 40/100
1/1 [==============================] - 0s 10ms/step - loss: 10.5560 - mae: 10.5560
Epoch 41/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5556 - mae: 10.5556
Epoch 42/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5551 - mae: 10.5551
Epoch 43/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5547 - mae: 10.5547
Epoch 44/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5542 - mae: 10.5542
Epoch 45/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5538 - mae: 10.5538
Epoch 46/100
1/1 [==============================] - 0s 8ms/step - loss: 10.5533 - mae: 10.5533
Epoch 47/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5529 - mae: 10.5529
Epoch 48/100
1/1 [==============================] - 0s 10ms/step - loss: 10.5524 - mae: 10.5524
Epoch 49/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5520 - mae: 10.5520
Epoch 50/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5515 - mae: 10.5515
Epoch 51/100
1/1 [==============================] - 0s 7ms/step - loss: 10.5511 - mae: 10.5511
Epoch 52/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5506 - mae: 10.5506
Epoch 53/100
1/1 [==============================] - 0s 11ms/step - loss: 10.5502 - mae: 10.5502
Epoch 54/100
1/1 [==============================] - 0s 7ms/step - loss: 10.5497 - mae: 10.5497
Epoch 55/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5493 - mae: 10.5493
Epoch 56/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5488 - mae: 10.5488
Epoch 57/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5484 - mae: 10.5484
Epoch 58/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5479 - mae: 10.5479
Epoch 59/100
1/1 [==============================] - 0s 10ms/step - loss: 10.5475 - mae: 10.5475
Epoch 60/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5470 - mae: 10.5470
Epoch 61/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5466 - mae: 10.5466
Epoch 62/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5461 - mae: 10.5461
Epoch 63/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5457 - mae: 10.5457
Epoch 64/100
1/1 [==============================] - 0s 9ms/step - loss: 10.5452 - mae: 10.5452
Epoch 65/100
1/1 [==============================] - 0s 9ms/step - loss: 10.5448 - mae: 10.5448
Epoch 66/100
1/1 [==============================] - 0s 11ms/step - loss: 10.5443 - mae: 10.5443
Epoch 67/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5439 - mae: 10.5439
Epoch 68/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5434 - mae: 10.5434
Epoch 69/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5430 - mae: 10.5430
Epoch 70/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5425 - mae: 10.5425
Epoch 71/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5421 - mae: 10.5421
Epoch 72/100
1/1 [==============================] - 0s 11ms/step - loss: 10.5416 - mae: 10.5416
Epoch 73/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5412 - mae: 10.5412
Epoch 74/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5407 - mae: 10.5407
Epoch 75/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5403 - mae: 10.5403
Epoch 76/100
1/1 [==============================] - 0s 9ms/step - loss: 10.5398 - mae: 10.5398
Epoch 77/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5394 - mae: 10.5394
Epoch 78/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5389 - mae: 10.5389
Epoch 79/100
1/1 [==============================] - 0s 10ms/step - loss: 10.5385 - mae: 10.5385
Epoch 80/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5380 - mae: 10.5380
Epoch 81/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5376 - mae: 10.5376
Epoch 82/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5371 - mae: 10.5371
Epoch 83/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5367 - mae: 10.5367
Epoch 84/100
1/1 [==============================] - 0s 10ms/step - loss: 10.5362 - mae: 10.5362
Epoch 85/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5358 - mae: 10.5358
Epoch 86/100
1/1 [==============================] - 0s 8ms/step - loss: 10.5353 - mae: 10.5353
Epoch 87/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5349 - mae: 10.5349
Epoch 88/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5344 - mae: 10.5344
Epoch 89/100
1/1 [==============================] - 0s 9ms/step - loss: 10.5340 - mae: 10.5340
Epoch 90/100
1/1 [==============================] - 0s 28ms/step - loss: 10.5335 - mae: 10.5335
Epoch 91/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5331 - mae: 10.5331
Epoch 92/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5326 - mae: 10.5326
Epoch 93/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5322 - mae: 10.5322
Epoch 94/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5317 - mae: 10.5317
Epoch 95/100
1/1 [==============================] - 0s 11ms/step - loss: 10.5313 - mae: 10.5313
Epoch 96/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5308 - mae: 10.5308
Epoch 97/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5304 - mae: 10.5304
Epoch 98/100
1/1 [==============================] - 0s 6ms/step - loss: 10.5299 - mae: 10.5299
Epoch 99/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5295 - mae: 10.5295
Epoch 100/100
1/1 [==============================] - 0s 5ms/step - loss: 10.5290 - mae: 10.5290
<keras.callbacks.History at 0x7f07f6530250>
[17]
2s
#experiment change

#create a model
model= tf.keras.Sequential([
    tf.keras.layers.Dense(50,activation=None),
    tf.keras.layers.Dense(1)
])

#compile a model
model.compile(loss=tf.keras.losses.mae,
              optimizer= tf.keras.optimizers.Adam(lr=0.01),
              metrics=['mae']
             )

#fitting a model
model.fit(tf.expand_dims(X,axis=-1),Y ,epochs=100)
Epoch 1/100
1/1 [==============================] - 0s 403ms/step - loss: 14.2261 - mae: 14.2261
Epoch 2/100
1/1 [==============================] - 0s 9ms/step - loss: 13.5328 - mae: 13.5328
Epoch 3/100
1/1 [==============================] - 0s 7ms/step - loss: 12.8450 - mae: 12.8450
Epoch 4/100
1/1 [==============================] - 0s 11ms/step - loss: 12.1611 - mae: 12.1611
Epoch 5/100
1/1 [==============================] - 0s 6ms/step - loss: 11.4786 - mae: 11.4786
Epoch 6/100
1/1 [==============================] - 0s 6ms/step - loss: 10.7953 - mae: 10.7953
Epoch 7/100
1/1 [==============================] - 0s 6ms/step - loss: 10.1084 - mae: 10.1084
Epoch 8/100
1/1 [==============================] - 0s 8ms/step - loss: 9.4153 - mae: 9.4153
Epoch 9/100
1/1 [==============================] - 0s 12ms/step - loss: 8.7133 - mae: 8.7133
Epoch 10/100
1/1 [==============================] - 0s 7ms/step - loss: 7.9998 - mae: 7.9998
Epoch 11/100
1/1 [==============================] - 0s 7ms/step - loss: 7.2724 - mae: 7.2724
Epoch 12/100
1/1 [==============================] - 0s 7ms/step - loss: 6.6391 - mae: 6.6391
Epoch 13/100
1/1 [==============================] - 0s 12ms/step - loss: 6.6431 - mae: 6.6431
Epoch 14/100
1/1 [==============================] - 0s 7ms/step - loss: 6.9208 - mae: 6.9208
Epoch 15/100
1/1 [==============================] - 0s 7ms/step - loss: 7.0980 - mae: 7.0980
Epoch 16/100
1/1 [==============================] - 0s 7ms/step - loss: 7.2570 - mae: 7.2570
Epoch 17/100
1/1 [==============================] - 0s 8ms/step - loss: 7.2611 - mae: 7.2611
Epoch 18/100
1/1 [==============================] - 0s 7ms/step - loss: 7.1037 - mae: 7.1037
Epoch 19/100
1/1 [==============================] - 0s 13ms/step - loss: 6.8542 - mae: 6.8542
Epoch 20/100
1/1 [==============================] - 0s 9ms/step - loss: 6.6416 - mae: 6.6416
Epoch 21/100
1/1 [==============================] - 0s 8ms/step - loss: 6.4073 - mae: 6.4073
Epoch 22/100
1/1 [==============================] - 0s 9ms/step - loss: 6.1556 - mae: 6.1556
Epoch 23/100
1/1 [==============================] - 0s 11ms/step - loss: 5.9512 - mae: 5.9512
Epoch 24/100
1/1 [==============================] - 0s 7ms/step - loss: 5.9037 - mae: 5.9037
Epoch 25/100
1/1 [==============================] - 0s 7ms/step - loss: 5.8553 - mae: 5.8553
Epoch 26/100
1/1 [==============================] - 0s 6ms/step - loss: 5.9016 - mae: 5.9016
Epoch 27/100
1/1 [==============================] - 0s 7ms/step - loss: 5.8472 - mae: 5.8472
Epoch 28/100
1/1 [==============================] - 0s 12ms/step - loss: 5.7049 - mae: 5.7049
Epoch 29/100
1/1 [==============================] - 0s 7ms/step - loss: 5.5259 - mae: 5.5259
Epoch 30/100
1/1 [==============================] - 0s 6ms/step - loss: 5.4255 - mae: 5.4255
Epoch 31/100
1/1 [==============================] - 0s 7ms/step - loss: 5.3213 - mae: 5.3213
Epoch 32/100
1/1 [==============================] - 0s 7ms/step - loss: 5.2131 - mae: 5.2131
Epoch 33/100
1/1 [==============================] - 0s 7ms/step - loss: 5.1472 - mae: 5.1472
Epoch 34/100
1/1 [==============================] - 0s 10ms/step - loss: 5.0909 - mae: 5.0909
Epoch 35/100
1/1 [==============================] - 0s 7ms/step - loss: 4.9926 - mae: 4.9926
Epoch 36/100
1/1 [==============================] - 0s 7ms/step - loss: 4.8552 - mae: 4.8552
Epoch 37/100
1/1 [==============================] - 0s 8ms/step - loss: 4.6821 - mae: 4.6821
Epoch 38/100
1/1 [==============================] - 0s 8ms/step - loss: 4.5740 - mae: 4.5740
Epoch 39/100
1/1 [==============================] - 0s 7ms/step - loss: 4.4584 - mae: 4.4584
Epoch 40/100
1/1 [==============================] - 0s 8ms/step - loss: 4.3351 - mae: 4.3351
Epoch 41/100
1/1 [==============================] - 0s 14ms/step - loss: 4.2039 - mae: 4.2039
Epoch 42/100
1/1 [==============================] - 0s 9ms/step - loss: 4.0647 - mae: 4.0647
Epoch 43/100
1/1 [==============================] - 0s 16ms/step - loss: 3.9171 - mae: 3.9171
Epoch 44/100
1/1 [==============================] - 0s 17ms/step - loss: 3.7610 - mae: 3.7610
Epoch 45/100
1/1 [==============================] - 0s 19ms/step - loss: 3.5961 - mae: 3.5961
Epoch 46/100
1/1 [==============================] - 0s 13ms/step - loss: 3.4221 - mae: 3.4221
Epoch 47/100
1/1 [==============================] - 0s 18ms/step - loss: 3.2387 - mae: 3.2387
Epoch 48/100
1/1 [==============================] - 0s 14ms/step - loss: 3.0457 - mae: 3.0457
Epoch 49/100
1/1 [==============================] - 0s 17ms/step - loss: 2.8428 - mae: 2.8428
Epoch 50/100
1/1 [==============================] - 0s 13ms/step - loss: 2.6857 - mae: 2.6857
Epoch 51/100
1/1 [==============================] - 0s 11ms/step - loss: 2.4548 - mae: 2.4548
Epoch 52/100
1/1 [==============================] - 0s 11ms/step - loss: 2.2068 - mae: 2.2068
Epoch 53/100
1/1 [==============================] - 0s 7ms/step - loss: 1.9829 - mae: 1.9829
Epoch 54/100
1/1 [==============================] - 0s 13ms/step - loss: 1.8204 - mae: 1.8204
Epoch 55/100
1/1 [==============================] - 0s 12ms/step - loss: 1.4857 - mae: 1.4857
Epoch 56/100
1/1 [==============================] - 0s 15ms/step - loss: 1.2132 - mae: 1.2132
Epoch 57/100
1/1 [==============================] - 0s 7ms/step - loss: 0.9993 - mae: 0.9993
Epoch 58/100
1/1 [==============================] - 0s 8ms/step - loss: 0.7101 - mae: 0.7101
Epoch 59/100
1/1 [==============================] - 0s 12ms/step - loss: 0.3594 - mae: 0.3594
Epoch 60/100
1/1 [==============================] - 0s 13ms/step - loss: 0.1118 - mae: 0.1118
Epoch 61/100
1/1 [==============================] - 0s 24ms/step - loss: 0.3994 - mae: 0.3994
Epoch 62/100
1/1 [==============================] - 0s 54ms/step - loss: 0.5249 - mae: 0.5249
Epoch 63/100
1/1 [==============================] - 0s 30ms/step - loss: 0.8002 - mae: 0.8002
Epoch 64/100
1/1 [==============================] - 0s 12ms/step - loss: 0.9110 - mae: 0.9110
Epoch 65/100
1/1 [==============================] - 0s 6ms/step - loss: 0.8892 - mae: 0.8892
Epoch 66/100
1/1 [==============================] - 0s 12ms/step - loss: 0.9457 - mae: 0.9457
Epoch 67/100
1/1 [==============================] - 0s 21ms/step - loss: 0.8913 - mae: 0.8913
Epoch 68/100
1/1 [==============================] - 0s 18ms/step - loss: 0.8768 - mae: 0.8768
Epoch 69/100
1/1 [==============================] - 0s 10ms/step - loss: 0.7506 - mae: 0.7506
Epoch 70/100
1/1 [==============================] - 0s 30ms/step - loss: 0.6897 - mae: 0.6897
Epoch 71/100
1/1 [==============================] - 0s 12ms/step - loss: 0.5674 - mae: 0.5674
Epoch 72/100
1/1 [==============================] - 0s 11ms/step - loss: 0.3382 - mae: 0.3382
Epoch 73/100
1/1 [==============================] - 0s 9ms/step - loss: 0.1770 - mae: 0.1770
Epoch 74/100
1/1 [==============================] - 0s 31ms/step - loss: 0.1846 - mae: 0.1846
Epoch 75/100
1/1 [==============================] - 0s 15ms/step - loss: 0.2042 - mae: 0.2042
Epoch 76/100
1/1 [==============================] - 0s 19ms/step - loss: 0.4884 - mae: 0.4884
Epoch 77/100
1/1 [==============================] - 0s 11ms/step - loss: 0.5454 - mae: 0.5454
Epoch 78/100
1/1 [==============================] - 0s 14ms/step - loss: 0.4443 - mae: 0.4443
Epoch 79/100
1/1 [==============================] - 0s 8ms/step - loss: 0.5707 - mae: 0.5707
Epoch 80/100
1/1 [==============================] - 0s 10ms/step - loss: 0.6051 - mae: 0.6051
Epoch 81/100
1/1 [==============================] - 0s 15ms/step - loss: 0.4780 - mae: 0.4780
Epoch 82/100
1/1 [==============================] - 0s 10ms/step - loss: 0.3399 - mae: 0.3399
Epoch 83/100
1/1 [==============================] - 0s 9ms/step - loss: 0.3067 - mae: 0.3067
Epoch 84/100
1/1 [==============================] - 0s 9ms/step - loss: 0.1018 - mae: 0.1018
Epoch 85/100
1/1 [==============================] - 0s 7ms/step - loss: 0.2650 - mae: 0.2650
Epoch 86/100
1/1 [==============================] - 0s 15ms/step - loss: 0.2374 - mae: 0.2374
Epoch 87/100
1/1 [==============================] - 0s 8ms/step - loss: 0.3402 - mae: 0.3402
Epoch 88/100
1/1 [==============================] - 0s 11ms/step - loss: 0.4551 - mae: 0.4551
Epoch 89/100
1/1 [==============================] - 0s 7ms/step - loss: 0.3949 - mae: 0.3949
Epoch 90/100
1/1 [==============================] - 0s 7ms/step - loss: 0.2212 - mae: 0.2212
Epoch 91/100
1/1 [==============================] - 0s 7ms/step - loss: 0.2634 - mae: 0.2634
Epoch 92/100
1/1 [==============================] - 0s 8ms/step - loss: 0.0835 - mae: 0.0835
Epoch 93/100
1/1 [==============================] - 0s 7ms/step - loss: 0.2379 - mae: 0.2379
Epoch 94/100
1/1 [==============================] - 0s 7ms/step - loss: 0.1928 - mae: 0.1928
Epoch 95/100
1/1 [==============================] - 0s 7ms/step - loss: 0.2699 - mae: 0.2699
Epoch 96/100
1/1 [==============================] - 0s 7ms/step - loss: 0.3601 - mae: 0.3601
Epoch 97/100
1/1 [==============================] - 0s 7ms/step - loss: 0.2943 - mae: 0.2943
Epoch 98/100
1/1 [==============================] - 0s 8ms/step - loss: 0.1065 - mae: 0.1065
Epoch 99/100
1/1 [==============================] - 0s 8ms/step - loss: 0.1595 - mae: 0.1595
Epoch 100/100
1/1 [==============================] - 0s 7ms/step - loss: 0.0556 - mae: 0.0556
<keras.callbacks.History at 0x7f07f5c4af70>
[18]
0s
#model prediction
model.predict([17.0])
1/1 [==============================] - 0s 144ms/step
array([[27.138466]], dtype=float32)
Evaluating a model
In practice we will have a very basic workflow that we need to follow while making neural networks.

build a model-> fit a model -> evaluate the model -> tweak a model 
-> evaluate it -> tweak the model -> fit it .......
When we are evaluating a model we need to remember three words

"visualize visualize visualize"

It is a good idea to visualize :-

the data - what data are we working with? what does it look like?
visaulize the model- what does the model look like?
the training of a model :- how does the model performs while it learns?
predicting of our model :- how does the prediction of our model line up against the ground truth.(The original labels)
[19]
0s
#Make a bigger dataset 
X=tf.range(-100,100,4)
X
<tf.Tensor: shape=(50,), dtype=int32, numpy=
array([-100,  -96,  -92,  -88,  -84,  -80,  -76,  -72,  -68,  -64,  -60,
        -56,  -52,  -48,  -44,  -40,  -36,  -32,  -28,  -24,  -20,  -16,
        -12,   -8,   -4,    0,    4,    8,   12,   16,   20,   24,   28,
         32,   36,   40,   44,   48,   52,   56,   60,   64,   68,   72,
         76,   80,   84,   88,   92,   96], dtype=int32)>
[20]
0s
Y = X +10
Y
<tf.Tensor: shape=(50,), dtype=int32, numpy=
array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42,
       -38, -34, -30, -26, -22, -18, -14, -10,  -6,  -2,   2,   6,  10,
        14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,
        66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106], dtype=int32)>
[21]
2s
#visualize the data
plt.scatter(X,Y)

the three set...
Training set:- depends on the 70-80 percent of the dataset that is avaliable.
validation set:- model gets tuned on this data, which is 10-15 percent of the data available.
test set :- the model gets evaluated on this data to test what is learned, this set is typically 10-15 percent of the data.
[22]
0s
#check the length of samples we have
len(X)
50
[23]
0s
#split the data into train and test_sets
X_train=X[:40] # 80 percent the data
X_test=X[40:] #20 percent of the data

#split the data into train and test_sets
Y_train=Y[:40] # 80 percent the data
Y_test=Y[40:] #20 percent of the data
len(X_train),len(X_test),len(Y_train),len(Y_test)
(40, 10, 40, 10)
Now we get the data in training and test sets
so lets visualize it again.

[24]
0s
plt.figure(figsize=(10,7))
#plot training data
plt.scatter(X_train,Y_train,c="b",label="training data")
#plot test data in green
plt.scatter(X_test,Y_test,c="g",label="testing data")
plt.legend()

[25]
0s
#let's have a look at how to build our neural network for our data

#build our model
model= tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])

#compile the model
model.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=['mae'])

#fit our model
#model.fit(tf.expand_dims(X_train,axis=-1),Y_train,epochs=100)
let us visualize our model before running
[26]
0s
# let us build a model which builds automatically by defining the input_shape argument in first layer
#let's have a look at how to build our neural network for our data

tf.random.set_seed(42)

#build our model
model= tf.keras.Sequential([
    tf.keras.layers.Dense(10,input_shape=[1],name="input_layer"),
    tf.keras.layers.Dense(1,name="output_layer")
],name="model_1")

#compile the model
model.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=['mae'])


[27]
0s
model.summary()
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_layer (Dense)         (None, 10)                20        
                                                                 
 output_layer (Dense)        (None, 1)                 11        
                                                                 
=================================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
_________________________________________________________________
Total params - total no of parameter in the model
trainable parameters - These are the parameters the model can update as the model kept on getting updated
non-trainable params- these are parameters not updated during training (This is typically happens when we bring in already bring in already learned the patterns from other model using Transfer learning)
[28]
2s
#let us train our model
#fit our model
model.fit(tf.expand_dims(X_train,axis=-1),Y_train,epochs=100,verbose=0)
<keras.callbacks.History at 0x7f07f2bf3850>
[29]
0s
#let us get our model summary
model.summary()
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_layer (Dense)         (None, 10)                20        
                                                                 
 output_layer (Dense)        (None, 1)                 11        
                                                                 
=================================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
_________________________________________________________________
[30]
0s
from tensorflow.keras.utils import plot_model
plot_model(model=model,show_shapes=True)

how to vizualize our models' prediction
to vizualize the prediction it is a good idea to plot them against the ground truth labels.

In practices you will see them as in form of y_true or y_test vs y_pred (ground truth vs your model)

[31]
0s
#make some prediction
Y_pred=model.predict(X_test)
Y_pred
1/1 [==============================] - 0s 75ms/step
array([[ 70.552185],
       [ 75.13991 ],
       [ 79.72764 ],
       [ 84.315346],
       [ 88.90308 ],
       [ 93.49081 ],
       [ 98.07852 ],
       [102.666245],
       [107.253975],
       [111.84169 ]], dtype=float32)
[32]
0s
Y_test
<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106], dtype=int32)>
[33]
0s
#let us create a plot function 
def plot_prediction(train_data=X_train,train_labels=Y_train,test_data=X_test,test_labels=Y_test,predictions=Y_pred):
  """plot train data and then compare them to the ground truth"""
  plt.figure(figsize=(10,7))
  #plot training data in blue
  plt.scatter(train_data,train_labels,c="b",label="Training data")
  #Plot testing data
  plt.scatter(test_data,test_labels,c="g",label="Testing data")
  #building a scatter plot for the probability
  plt.scatter(test_data,predictions,c="r",label="predictions")
  #show legend
  plt.legend()
[34]
0s
plot_prediction()

Evaluating the model's prediction with regression evaluation metrics
depending upon the problem you are working on there will different evaluation metrics to evaluate your model's performance.

since we are working on regression , two main metrics :

MAE- mean absolute error,"how wrong is our model predictions are"
MSE- mean square error ,"square the average error"
[35]
1s
#evalute the model on our test set
model.evaluate(X_test,Y_test)
1/1 [==============================] - 0s 278ms/step - loss: 3.1969 - mae: 3.1969
[3.1969382762908936, 3.1969382762908936]
[36]
0s
#mean_absolute_error
mae=tf.keras.losses.MAE(Y_test,Y_pred)
mae
<tf.Tensor: shape=(10,), dtype=float32, numpy=
array([17.558252 , 14.1160555, 11.708944 , 10.336931 , 10.       ,
       10.698161 , 12.447113 , 15.332995 , 19.253975 , 23.84169  ],
      dtype=float32)>
[37]
0s
tf.constant(Y_pred)
<tf.Tensor: shape=(10, 1), dtype=float32, numpy=
array([[ 70.552185],
       [ 75.13991 ],
       [ 79.72764 ],
       [ 84.315346],
       [ 88.90308 ],
       [ 93.49081 ],
       [ 98.07852 ],
       [102.666245],
       [107.253975],
       [111.84169 ]], dtype=float32)>
[38]
0s
Y_test
<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106], dtype=int32)>
[39]
0s
tf.squeeze(Y_pred)
<tf.Tensor: shape=(10,), dtype=float32, numpy=
array([ 70.552185,  75.13991 ,  79.72764 ,  84.315346,  88.90308 ,
        93.49081 ,  98.07852 , 102.666245, 107.253975, 111.84169 ],
      dtype=float32)>
[40]
0s
#caculate the mae
mae=tf.keras.losses.MAE(Y_test,tf.squeeze(Y_pred))
mae
<tf.Tensor: shape=(), dtype=float32, numpy=3.19694>
[41]
0s
#calculate the mean square error
mse=tf.metrics.MSE(Y_test,tf.squeeze(Y_pred))
mse
<tf.Tensor: shape=(), dtype=float32, numpy=13.070127>
[42]
0s
#function created for mae
def mae(Y_test,Y_pred):
  return tf.metrics.MAE(Y_test,Y_pred)

def mse(Y_test,Y_pred):
  return tf.metrics.MSE(Y_test,Y_pred)
Running experiment to improve our models
get more data
make your model more lager and complex moer layers
Train for longer
let us do three modelling experiment

model_1 - same as original model.1 layer trained for the 100 epochs
model_2 - 2 layers trained for 100 for epochs
model_3 - 2 layers trained for 500 for epochs
build model_1

[43]
2s
#set us the random set_seed
tf.random.set_seed(42)

#create the model
model_1=tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])

#compile the model
model_1.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=['mae'])

#fit the model
model_1.fit(tf.expand_dims(X_train,axis=-1),Y_train,epochs=100)
Epoch 1/100
2/2 [==============================] - 1s 9ms/step - loss: 15.9024 - mae: 15.9024
Epoch 2/100
2/2 [==============================] - 0s 7ms/step - loss: 11.2837 - mae: 11.2837
Epoch 3/100
2/2 [==============================] - 0s 4ms/step - loss: 11.1075 - mae: 11.1075
Epoch 4/100
2/2 [==============================] - 0s 9ms/step - loss: 9.2990 - mae: 9.2990
Epoch 5/100
2/2 [==============================] - 0s 5ms/step - loss: 10.1677 - mae: 10.1677
Epoch 6/100
2/2 [==============================] - 0s 19ms/step - loss: 9.4303 - mae: 9.4303
Epoch 7/100
2/2 [==============================] - 0s 9ms/step - loss: 8.5704 - mae: 8.5704
Epoch 8/100
2/2 [==============================] - 0s 13ms/step - loss: 9.0442 - mae: 9.0442
Epoch 9/100
2/2 [==============================] - 0s 5ms/step - loss: 18.7517 - mae: 18.7517
Epoch 10/100
2/2 [==============================] - 0s 4ms/step - loss: 10.1142 - mae: 10.1142
Epoch 11/100
2/2 [==============================] - 0s 6ms/step - loss: 8.3980 - mae: 8.3980
Epoch 12/100
2/2 [==============================] - 0s 7ms/step - loss: 10.6639 - mae: 10.6639
Epoch 13/100
2/2 [==============================] - 0s 7ms/step - loss: 9.7977 - mae: 9.7977
Epoch 14/100
2/2 [==============================] - 0s 12ms/step - loss: 16.0103 - mae: 16.0103
Epoch 15/100
2/2 [==============================] - 0s 7ms/step - loss: 11.4068 - mae: 11.4068
Epoch 16/100
2/2 [==============================] - 0s 11ms/step - loss: 8.5393 - mae: 8.5393
Epoch 17/100
2/2 [==============================] - 0s 17ms/step - loss: 13.6348 - mae: 13.6348
Epoch 18/100
2/2 [==============================] - 0s 9ms/step - loss: 11.4629 - mae: 11.4629
Epoch 19/100
2/2 [==============================] - 0s 19ms/step - loss: 17.9148 - mae: 17.9148
Epoch 20/100
2/2 [==============================] - 0s 10ms/step - loss: 15.0494 - mae: 15.0494
Epoch 21/100
2/2 [==============================] - 0s 9ms/step - loss: 11.0216 - mae: 11.0216
Epoch 22/100
2/2 [==============================] - 0s 26ms/step - loss: 8.1558 - mae: 8.1558
Epoch 23/100
2/2 [==============================] - 0s 11ms/step - loss: 9.5138 - mae: 9.5138
Epoch 24/100
2/2 [==============================] - 0s 22ms/step - loss: 7.6617 - mae: 7.6617
Epoch 25/100
2/2 [==============================] - 0s 5ms/step - loss: 13.1859 - mae: 13.1859
Epoch 26/100
2/2 [==============================] - 0s 8ms/step - loss: 16.4211 - mae: 16.4211
Epoch 27/100
2/2 [==============================] - 0s 29ms/step - loss: 13.1660 - mae: 13.1660
Epoch 28/100
2/2 [==============================] - 0s 7ms/step - loss: 14.2559 - mae: 14.2559
Epoch 29/100
2/2 [==============================] - 0s 10ms/step - loss: 10.0670 - mae: 10.0670
Epoch 30/100
2/2 [==============================] - 0s 14ms/step - loss: 16.3409 - mae: 16.3409
Epoch 31/100
2/2 [==============================] - 0s 12ms/step - loss: 23.6444 - mae: 23.6444
Epoch 32/100
2/2 [==============================] - 0s 10ms/step - loss: 7.6215 - mae: 7.6215
Epoch 33/100
2/2 [==============================] - 0s 9ms/step - loss: 9.3221 - mae: 9.3221
Epoch 34/100
2/2 [==============================] - 0s 7ms/step - loss: 13.7313 - mae: 13.7313
Epoch 35/100
2/2 [==============================] - 0s 7ms/step - loss: 11.1276 - mae: 11.1276
Epoch 36/100
2/2 [==============================] - 0s 10ms/step - loss: 13.3222 - mae: 13.3222
Epoch 37/100
2/2 [==============================] - 0s 11ms/step - loss: 9.4763 - mae: 9.4763
Epoch 38/100
2/2 [==============================] - 0s 6ms/step - loss: 10.1381 - mae: 10.1381
Epoch 39/100
2/2 [==============================] - 0s 10ms/step - loss: 10.1793 - mae: 10.1793
Epoch 40/100
2/2 [==============================] - 0s 5ms/step - loss: 10.9137 - mae: 10.9137
Epoch 41/100
2/2 [==============================] - 0s 14ms/step - loss: 7.9063 - mae: 7.9063
Epoch 42/100
2/2 [==============================] - 0s 5ms/step - loss: 10.0914 - mae: 10.0914
Epoch 43/100
2/2 [==============================] - 0s 10ms/step - loss: 8.7006 - mae: 8.7006
Epoch 44/100
2/2 [==============================] - 0s 17ms/step - loss: 12.2046 - mae: 12.2046
Epoch 45/100
2/2 [==============================] - 0s 5ms/step - loss: 13.7970 - mae: 13.7970
Epoch 46/100
2/2 [==============================] - 0s 4ms/step - loss: 8.4687 - mae: 8.4687
Epoch 47/100
2/2 [==============================] - 0s 14ms/step - loss: 9.1330 - mae: 9.1330
Epoch 48/100
2/2 [==============================] - 0s 7ms/step - loss: 10.6190 - mae: 10.6190
Epoch 49/100
2/2 [==============================] - 0s 18ms/step - loss: 7.7503 - mae: 7.7503
Epoch 50/100
2/2 [==============================] - 0s 19ms/step - loss: 9.5407 - mae: 9.5407
Epoch 51/100
2/2 [==============================] - 0s 7ms/step - loss: 9.1584 - mae: 9.1584
Epoch 52/100
2/2 [==============================] - 0s 20ms/step - loss: 16.3630 - mae: 16.3630
Epoch 53/100
2/2 [==============================] - 0s 10ms/step - loss: 14.1299 - mae: 14.1299
Epoch 54/100
2/2 [==============================] - 0s 6ms/step - loss: 21.1247 - mae: 21.1247
Epoch 55/100
2/2 [==============================] - 0s 7ms/step - loss: 16.3961 - mae: 16.3961
Epoch 56/100
2/2 [==============================] - 0s 6ms/step - loss: 9.9806 - mae: 9.9806
Epoch 57/100
2/2 [==============================] - 0s 23ms/step - loss: 9.9606 - mae: 9.9606
Epoch 58/100
2/2 [==============================] - 0s 21ms/step - loss: 9.2209 - mae: 9.2209
Epoch 59/100
2/2 [==============================] - 0s 4ms/step - loss: 8.4239 - mae: 8.4239
Epoch 60/100
2/2 [==============================] - 0s 7ms/step - loss: 9.4869 - mae: 9.4869
Epoch 61/100
2/2 [==============================] - 0s 13ms/step - loss: 11.4354 - mae: 11.4354
Epoch 62/100
2/2 [==============================] - 0s 12ms/step - loss: 11.6887 - mae: 11.6887
Epoch 63/100
2/2 [==============================] - 0s 8ms/step - loss: 7.0838 - mae: 7.0838
Epoch 64/100
2/2 [==============================] - 0s 7ms/step - loss: 16.9675 - mae: 16.9675
Epoch 65/100
2/2 [==============================] - 0s 5ms/step - loss: 12.4599 - mae: 12.4599
Epoch 66/100
2/2 [==============================] - 0s 5ms/step - loss: 13.0184 - mae: 13.0184
Epoch 67/100
2/2 [==============================] - 0s 5ms/step - loss: 8.0600 - mae: 8.0600
Epoch 68/100
2/2 [==============================] - 0s 34ms/step - loss: 10.1888 - mae: 10.1888
Epoch 69/100
2/2 [==============================] - 0s 19ms/step - loss: 12.3633 - mae: 12.3633
Epoch 70/100
2/2 [==============================] - 0s 21ms/step - loss: 9.0516 - mae: 9.0516
Epoch 71/100
2/2 [==============================] - 0s 5ms/step - loss: 10.0378 - mae: 10.0378
Epoch 72/100
2/2 [==============================] - 0s 6ms/step - loss: 10.0516 - mae: 10.0516
Epoch 73/100
2/2 [==============================] - 0s 6ms/step - loss: 12.6151 - mae: 12.6151
Epoch 74/100
2/2 [==============================] - 0s 7ms/step - loss: 10.3819 - mae: 10.3819
Epoch 75/100
2/2 [==============================] - 0s 4ms/step - loss: 9.7229 - mae: 9.7229
Epoch 76/100
2/2 [==============================] - 0s 16ms/step - loss: 11.2252 - mae: 11.2252
Epoch 77/100
2/2 [==============================] - 0s 6ms/step - loss: 8.3642 - mae: 8.3642
Epoch 78/100
2/2 [==============================] - 0s 12ms/step - loss: 9.1274 - mae: 9.1274
Epoch 79/100
2/2 [==============================] - 0s 4ms/step - loss: 19.5039 - mae: 19.5039
Epoch 80/100
2/2 [==============================] - 0s 8ms/step - loss: 14.8945 - mae: 14.8945
Epoch 81/100
2/2 [==============================] - 0s 10ms/step - loss: 9.0034 - mae: 9.0034
Epoch 82/100
2/2 [==============================] - 0s 28ms/step - loss: 13.0206 - mae: 13.0206
Epoch 83/100
2/2 [==============================] - 0s 15ms/step - loss: 7.9299 - mae: 7.9299
Epoch 84/100
2/2 [==============================] - 0s 7ms/step - loss: 7.6872 - mae: 7.6872
Epoch 85/100
2/2 [==============================] - 0s 21ms/step - loss: 10.0328 - mae: 10.0328
Epoch 86/100
2/2 [==============================] - 0s 14ms/step - loss: 9.2433 - mae: 9.2433
Epoch 87/100
2/2 [==============================] - 0s 5ms/step - loss: 12.0209 - mae: 12.0209
Epoch 88/100
2/2 [==============================] - 0s 5ms/step - loss: 10.6389 - mae: 10.6389
Epoch 89/100
2/2 [==============================] - 0s 16ms/step - loss: 7.2667 - mae: 7.2667
Epoch 90/100
2/2 [==============================] - 0s 8ms/step - loss: 12.7786 - mae: 12.7786
Epoch 91/100
2/2 [==============================] - 0s 17ms/step - loss: 7.3481 - mae: 7.3481
Epoch 92/100
2/2 [==============================] - 0s 8ms/step - loss: 7.7175 - mae: 7.7175
Epoch 93/100
2/2 [==============================] - 0s 27ms/step - loss: 7.1263 - mae: 7.1263
Epoch 94/100
2/2 [==============================] - 0s 8ms/step - loss: 12.6190 - mae: 12.6190
Epoch 95/100
2/2 [==============================] - 0s 7ms/step - loss: 10.0912 - mae: 10.0912
Epoch 96/100
2/2 [==============================] - 0s 4ms/step - loss: 9.3558 - mae: 9.3558
Epoch 97/100
2/2 [==============================] - 0s 16ms/step - loss: 12.6834 - mae: 12.6834
Epoch 98/100
2/2 [==============================] - 0s 6ms/step - loss: 8.6762 - mae: 8.6762
Epoch 99/100
2/2 [==============================] - 0s 10ms/step - loss: 9.4693 - mae: 9.4693
Epoch 100/100
2/2 [==============================] - 0s 5ms/step - loss: 8.7067 - mae: 8.7067
<keras.callbacks.History at 0x7f07fe19fdf0>
[44]
1s
#we will see that how the model will perform 
Y_pred_1=model_1.predict(X_test)
Y_pred_1
plot_prediction(predictions=Y_pred_1)

[45]
0s
#calculate evalution metrics
mae_1=mae(Y_test,Y_pred_1.squeeze())
mse_1=mse(Y_test,Y_pred_1.squeeze())
mae_1,mse_1
(<tf.Tensor: shape=(), dtype=float32, numpy=18.745327>,
 <tf.Tensor: shape=(), dtype=float32, numpy=353.57336>)
[46]
1s
#Build a model_2 epochs
tf.random.set_seed(42)
#build a model
model_2=tf.keras.Sequential([
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

#compile a model
model_2.compile(loss=tf.keras.losses.mae,

Epoch 1/100
2/2 [==============================] - 1s 11ms/step - loss: 27.4058 - mse: 1084.1482
Epoch 2/100
2/2 [==============================] - 0s 9ms/step - loss: 24.6339 - mse: 777.9203
Epoch 3/100
2/2 [==============================] - 0s 8ms/step - loss: 29.8935 - mse: 1334.8953
Epoch 4/100
2/2 [==============================] - 0s 18ms/step - loss: 27.4055 - mse: 1106.8035
Epoch 5/100
2/2 [==============================] - 0s 4ms/step - loss: 14.9463 - mse: 281.1076
Epoch 6/100
2/2 [==============================] - 0s 4ms/step - loss: 11.8819 - mse: 168.6621
Epoch 7/100
2/2 [==============================] - 0s 5ms/step - loss: 11.1988 - mse: 151.3508
Epoch 8/100
2/2 [==============================] - 0s 4ms/step - loss: 11.0910 - mse: 160.3745
Epoch 9/100
2/2 [==============================] - 0s 8ms/step - loss: 40.4763 - mse: 2586.0085
Epoch 10/100
2/2 [==============================] - 0s 5ms/step - loss: 27.8687 - mse: 1094.4380
Epoch 11/100
2/2 [==============================] - 0s 13ms/step - loss: 10.2473 - mse: 147.9359
Epoch 12/100
2/2 [==============================] - 0s 19ms/step - loss: 25.2803 - mse: 890.3867
Epoch 13/100
2/2 [==============================] - 0s 6ms/step - loss: 16.9897 - mse: 399.9677
Epoch 14/100
2/2 [==============================] - 0s 9ms/step - loss: 25.9217 - mse: 1049.5515
Epoch 15/100
2/2 [==============================] - 0s 7ms/step - loss: 17.9948 - mse: 450.2581
Epoch 16/100
2/2 [==============================] - 0s 5ms/step - loss: 7.3510 - mse: 80.6206
Epoch 17/100
2/2 [==============================] - 0s 4ms/step - loss: 10.8636 - mse: 174.7868
Epoch 18/100
2/2 [==============================] - 0s 4ms/step - loss: 19.5304 - mse: 565.8052
Epoch 19/100
2/2 [==============================] - 0s 7ms/step - loss: 10.3469 - mse: 167.7749
Epoch 20/100
2/2 [==============================] - 0s 4ms/step - loss: 17.6985 - mse: 455.7096
Epoch 21/100
2/2 [==============================] - 0s 4ms/step - loss: 15.8985 - mse: 347.1929
Epoch 22/100
2/2 [==============================] - 0s 16ms/step - loss: 14.1991 - mse: 285.1767
Epoch 23/100
2/2 [==============================] - 0s 7ms/step - loss: 8.7720 - mse: 91.7852
Epoch 24/100
2/2 [==============================] - 0s 10ms/step - loss: 11.0570 - mse: 153.7430
Epoch 25/100
2/2 [==============================] - 0s 4ms/step - loss: 12.6838 - mse: 233.2950
Epoch 26/100
2/2 [==============================] - 0s 7ms/step - loss: 26.1877 - mse: 1024.6094
Epoch 27/100
2/2 [==============================] - 0s 17ms/step - loss: 11.7432 - mse: 194.8453
Epoch 28/100
2/2 [==============================] - 0s 8ms/step - loss: 22.8730 - mse: 835.6069
Epoch 29/100
2/2 [==============================] - 0s 7ms/step - loss: 9.2459 - mse: 96.7787
Epoch 30/100
2/2 [==============================] - 0s 8ms/step - loss: 29.2641 - mse: 1535.1335
Epoch 31/100
2/2 [==============================] - 0s 18ms/step - loss: 53.0224 - mse: 5030.2954
Epoch 32/100
2/2 [==============================] - 0s 17ms/step - loss: 11.9951 - mse: 211.7023
Epoch 33/100
2/2 [==============================] - 0s 15ms/step - loss: 15.6357 - mse: 337.3664
Epoch 34/100
2/2 [==============================] - 0s 8ms/step - loss: 12.6925 - mse: 214.4822
Epoch 35/100
2/2 [==============================] - 0s 10ms/step - loss: 9.2398 - mse: 92.9126
Epoch 36/100
2/2 [==============================] - 0s 15ms/step - loss: 16.6497 - mse: 403.6569
Epoch 37/100
2/2 [==============================] - 0s 12ms/step - loss: 11.0382 - mse: 192.3919
Epoch 38/100
2/2 [==============================] - 0s 4ms/step - loss: 18.1634 - mse: 433.6718
Epoch 39/100
2/2 [==============================] - 0s 4ms/step - loss: 19.1013 - mse: 529.6440
Epoch 40/100
2/2 [==============================] - 0s 5ms/step - loss: 20.4324 - mse: 610.1325
Epoch 41/100
2/2 [==============================] - 0s 6ms/step - loss: 14.9102 - mse: 279.6182
Epoch 42/100
2/2 [==============================] - 0s 6ms/step - loss: 12.2809 - mse: 186.6180
Epoch 43/100
2/2 [==============================] - 0s 4ms/step - loss: 10.7333 - mse: 167.0952
Epoch 44/100
2/2 [==============================] - 0s 8ms/step - loss: 23.0260 - mse: 830.4245
Epoch 45/100
2/2 [==============================] - 0s 13ms/step - loss: 10.3897 - mse: 128.9549
Epoch 46/100
2/2 [==============================] - 0s 18ms/step - loss: 11.7904 - mse: 181.9211
Epoch 47/100
2/2 [==============================] - 0s 17ms/step - loss: 9.6438 - mse: 153.8708
Epoch 48/100
2/2 [==============================] - 0s 11ms/step - loss: 17.2335 - mse: 402.8495
Epoch 49/100
2/2 [==============================] - 0s 7ms/step - loss: 9.5729 - mse: 99.8336
Epoch 50/100
2/2 [==============================] - 0s 4ms/step - loss: 13.8185 - mse: 260.3669
Epoch 51/100
2/2 [==============================] - 0s 5ms/step - loss: 11.5958 - mse: 154.7956
Epoch 52/100
2/2 [==============================] - 0s 17ms/step - loss: 30.5538 - mse: 1613.0883
Epoch 53/100
2/2 [==============================] - 0s 7ms/step - loss: 14.3541 - mse: 302.5291
Epoch 54/100
2/2 [==============================] - 0s 13ms/step - loss: 23.9713 - mse: 859.3983
Epoch 55/100
2/2 [==============================] - 0s 7ms/step - loss: 23.1938 - mse: 805.5450
Epoch 56/100
2/2 [==============================] - 0s 17ms/step - loss: 10.8837 - mse: 170.9834
Epoch 57/100
2/2 [==============================] - 0s 12ms/step - loss: 12.7445 - mse: 198.7015
Epoch 58/100
2/2 [==============================] - 0s 10ms/step - loss: 9.5995 - mse: 102.5890
Epoch 59/100
2/2 [==============================] - 0s 8ms/step - loss: 12.5172 - mse: 216.3367
Epoch 60/100
2/2 [==============================] - 0s 9ms/step - loss: 12.3200 - mse: 208.6370
Epoch 61/100
2/2 [==============================] - 0s 9ms/step - loss: 17.4604 - mse: 428.6392
Epoch 62/100
2/2 [==============================] - 0s 5ms/step - loss: 10.6052 - mse: 136.9776
Epoch 63/100
2/2 [==============================] - 0s 5ms/step - loss: 10.4893 - mse: 152.4554
Epoch 64/100
2/2 [==============================] - 0s 7ms/step - loss: 24.8450 - mse: 911.7509
Epoch 65/100
2/2 [==============================] - 0s 4ms/step - loss: 10.6761 - mse: 142.7374
Epoch 66/100
2/2 [==============================] - 0s 6ms/step - loss: 21.7809 - mse: 704.4487
Epoch 67/100
2/2 [==============================] - 0s 6ms/step - loss: 10.7136 - mse: 136.0194
Epoch 68/100
2/2 [==============================] - 0s 15ms/step - loss: 10.6397 - mse: 149.2299
Epoch 69/100
2/2 [==============================] - 0s 7ms/step - loss: 22.6914 - mse: 742.1758
Epoch 70/100
2/2 [==============================] - 0s 4ms/step - loss: 9.3316 - mse: 166.1627
Epoch 71/100
2/2 [==============================] - 0s 6ms/step - loss: 15.4355 - mse: 323.0844
Epoch 72/100
2/2 [==============================] - 0s 9ms/step - loss: 6.7437 - mse: 67.0210
Epoch 73/100
2/2 [==============================] - 0s 4ms/step - loss: 11.6891 - mse: 183.7296
Epoch 74/100
2/2 [==============================] - 0s 6ms/step - loss: 24.0400 - mse: 908.8988
Epoch 75/100
2/2 [==============================] - 0s 5ms/step - loss: 9.5896 - mse: 149.3948
Epoch 76/100
2/2 [==============================] - 0s 4ms/step - loss: 12.4371 - mse: 188.3310
Epoch 77/100
2/2 [==============================] - 0s 5ms/step - loss: 16.6488 - mse: 429.2705
Epoch 78/100
2/2 [==============================] - 0s 5ms/step - loss: 9.0614 - mse: 95.4869
Epoch 79/100
2/2 [==============================] - 0s 5ms/step - loss: 23.9675 - mse: 864.0859
Epoch 80/100
2/2 [==============================] - 0s 5ms/step - loss: 26.7463 - mse: 1104.4030
Epoch 81/100
2/2 [==============================] - 0s 5ms/step - loss: 11.6714 - mse: 170.7055
Epoch 82/100
2/2 [==============================] - 0s 10ms/step - loss: 12.0228 - mse: 211.9191
Epoch 83/100
2/2 [==============================] - 0s 5ms/step - loss: 17.4218 - mse: 395.5590
Epoch 84/100
2/2 [==============================] - 0s 14ms/step - loss: 7.2629 - mse: 73.0935
Epoch 85/100
2/2 [==============================] - 0s 4ms/step - loss: 14.9650 - mse: 312.8361
Epoch 86/100
2/2 [==============================] - 0s 5ms/step - loss: 15.2862 - mse: 315.3606
Epoch 87/100
2/2 [==============================] - 0s 6ms/step - loss: 19.1086 - mse: 521.2535
Epoch 88/100
2/2 [==============================] - 0s 5ms/step - loss: 29.8228 - mse: 1287.1902
Epoch 89/100
2/2 [==============================] - 0s 11ms/step - loss: 10.1742 - mse: 124.1342
Epoch 90/100
2/2 [==============================] - 0s 5ms/step - loss: 21.5240 - mse: 663.8608
Epoch 91/100
2/2 [==============================] - 0s 5ms/step - loss: 10.5716 - mse: 161.7467
Epoch 92/100
2/2 [==============================] - 0s 5ms/step - loss: 18.3977 - mse: 464.1323
Epoch 93/100
2/2 [==============================] - 0s 5ms/step - loss: 7.4138 - mse: 81.9820
Epoch 94/100
2/2 [==============================] - 0s 5ms/step - loss: 17.7380 - mse: 445.7377
Epoch 95/100
2/2 [==============================] - 0s 11ms/step - loss: 11.1144 - mse: 164.0820
Epoch 96/100
2/2 [==============================] - 0s 12ms/step - loss: 19.4346 - mse: 510.5842
Epoch 97/100
2/2 [==============================] - 0s 9ms/step - loss: 12.1593 - mse: 209.9755
Epoch 98/100
2/2 [==============================] - 0s 5ms/step - loss: 11.5653 - mse: 169.4052
Epoch 99/100
2/2 [==============================] - 0s 9ms/step - loss: 13.8827 - mse: 265.4630
Epoch 100/100
2/2 [==============================] - 0s 4ms/step - loss: 20.2277 - mse: 608.8219
<keras.callbacks.History at 0x7f07fe0b13d0>
[47]
0s
#compare our prediction
Y_pred_2=model_2.predict(X_test)
plot_prediction(predictions=Y_pred_2)

[48]
0s
#model_2 evaluation metrics
mae_2=mae(Y_test,Y_pred_2.squeeze())
mse_2=mse(Y_test,Y_pred_2.squeeze())
mae_2,mse_2
(<tf.Tensor: shape=(), dtype=float32, numpy=3.19694>,
 <tf.Tensor: shape=(), dtype=float32, numpy=13.070127>)
[49]
15s
#model_3

#setting up a random seed
tf.random.set_seed(42)

#create the model
model_3=tf.keras.Sequential([
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

#compile the model
model_3.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.SGD(),
                metrics=['mse'])

#fit the model
model_3.fit(tf.expand_dims(X_train,axis=-1),Y_train,epochs=500)
Epoch 1/500
2/2 [==============================] - 1s 8ms/step - loss: 27.4058 - mse: 1084.1482
Epoch 2/500
2/2 [==============================] - 0s 9ms/step - loss: 24.6339 - mse: 777.9203
Epoch 3/500
2/2 [==============================] - 0s 6ms/step - loss: 29.8935 - mse: 1334.8953
Epoch 4/500
2/2 [==============================] - 0s 7ms/step - loss: 27.4055 - mse: 1106.8035
Epoch 5/500
2/2 [==============================] - 0s 11ms/step - loss: 14.9463 - mse: 281.1076
Epoch 6/500
2/2 [==============================] - 0s 7ms/step - loss: 11.8819 - mse: 168.6621
Epoch 7/500
2/2 [==============================] - 0s 16ms/step - loss: 11.1988 - mse: 151.3508
Epoch 8/500
2/2 [==============================] - 0s 8ms/step - loss: 11.0910 - mse: 160.3745
Epoch 9/500
2/2 [==============================] - 0s 6ms/step - loss: 40.4763 - mse: 2586.0085
Epoch 10/500
2/2 [==============================] - 0s 20ms/step - loss: 27.8687 - mse: 1094.4380
Epoch 11/500
2/2 [==============================] - 0s 10ms/step - loss: 10.2473 - mse: 147.9359
Epoch 12/500
2/2 [==============================] - 0s 4ms/step - loss: 25.2803 - mse: 890.3867
Epoch 13/500
2/2 [==============================] - 0s 8ms/step - loss: 16.9897 - mse: 399.9677
Epoch 14/500
2/2 [==============================] - 0s 5ms/step - loss: 25.9217 - mse: 1049.5515
Epoch 15/500
2/2 [==============================] - 0s 7ms/step - loss: 17.9948 - mse: 450.2581
Epoch 16/500
2/2 [==============================] - 0s 13ms/step - loss: 7.3510 - mse: 80.6206
Epoch 17/500
2/2 [==============================] - 0s 5ms/step - loss: 10.8636 - mse: 174.7868
Epoch 18/500
2/2 [==============================] - 0s 20ms/step - loss: 19.5304 - mse: 565.8052
Epoch 19/500
2/2 [==============================] - 0s 7ms/step - loss: 10.3469 - mse: 167.7749
Epoch 20/500
2/2 [==============================] - 0s 4ms/step - loss: 17.6985 - mse: 455.7096
Epoch 21/500
2/2 [==============================] - 0s 6ms/step - loss: 15.8985 - mse: 347.1929
Epoch 22/500
2/2 [==============================] - 0s 11ms/step - loss: 14.1991 - mse: 285.1767
Epoch 23/500
2/2 [==============================] - 0s 4ms/step - loss: 8.7720 - mse: 91.7852
Epoch 24/500
2/2 [==============================] - 0s 4ms/step - loss: 11.0570 - mse: 153.7430
Epoch 25/500
2/2 [==============================] - 0s 4ms/step - loss: 12.6838 - mse: 233.2950
Epoch 26/500
2/2 [==============================] - 0s 5ms/step - loss: 26.1877 - mse: 1024.6094
Epoch 27/500
2/2 [==============================] - 0s 5ms/step - loss: 11.7432 - mse: 194.8453
Epoch 28/500
2/2 [==============================] - 0s 5ms/step - loss: 22.8730 - mse: 835.6069
Epoch 29/500
2/2 [==============================] - 0s 5ms/step - loss: 9.2459 - mse: 96.7787
Epoch 30/500
2/2 [==============================] - 0s 6ms/step - loss: 29.2641 - mse: 1535.1335
Epoch 31/500
2/2 [==============================] - 0s 6ms/step - loss: 53.0224 - mse: 5030.2954
Epoch 32/500
2/2 [==============================] - 0s 8ms/step - loss: 11.9951 - mse: 211.7023
Epoch 33/500
2/2 [==============================] - 0s 5ms/step - loss: 15.6357 - mse: 337.3664
Epoch 34/500
2/2 [==============================] - 0s 7ms/step - loss: 12.6925 - mse: 214.4822
Epoch 35/500
2/2 [==============================] - 0s 6ms/step - loss: 9.2398 - mse: 92.9126
Epoch 36/500
2/2 [==============================] - 0s 9ms/step - loss: 16.6497 - mse: 403.6569
Epoch 37/500
2/2 [==============================] - 0s 5ms/step - loss: 11.0382 - mse: 192.3919
Epoch 38/500
2/2 [==============================] - 0s 6ms/step - loss: 18.1634 - mse: 433.6718
Epoch 39/500
2/2 [==============================] - 0s 4ms/step - loss: 19.1013 - mse: 529.6440
Epoch 40/500
2/2 [==============================] - 0s 5ms/step - loss: 20.4324 - mse: 610.1325
Epoch 41/500
2/2 [==============================] - 0s 7ms/step - loss: 14.9102 - mse: 279.6182
Epoch 42/500
2/2 [==============================] - 0s 8ms/step - loss: 12.2809 - mse: 186.6180
Epoch 43/500
2/2 [==============================] - 0s 12ms/step - loss: 10.7333 - mse: 167.0952
Epoch 44/500
2/2 [==============================] - 0s 7ms/step - loss: 23.0260 - mse: 830.4245
Epoch 45/500
2/2 [==============================] - 0s 10ms/step - loss: 10.3897 - mse: 128.9549
Epoch 46/500
2/2 [==============================] - 0s 10ms/step - loss: 11.7904 - mse: 181.9211
Epoch 47/500
2/2 [==============================] - 0s 4ms/step - loss: 9.6438 - mse: 153.8708
Epoch 48/500
2/2 [==============================] - 0s 6ms/step - loss: 17.2335 - mse: 402.8495
Epoch 49/500
2/2 [==============================] - 0s 7ms/step - loss: 9.5729 - mse: 99.8336
Epoch 50/500
2/2 [==============================] - 0s 17ms/step - loss: 13.8185 - mse: 260.3669
Epoch 51/500
2/2 [==============================] - 0s 27ms/step - loss: 11.5958 - mse: 154.7956
Epoch 52/500
2/2 [==============================] - 0s 9ms/step - loss: 30.5538 - mse: 1613.0883
Epoch 53/500
2/2 [==============================] - 0s 15ms/step - loss: 14.3541 - mse: 302.5291
Epoch 54/500
2/2 [==============================] - 0s 22ms/step - loss: 23.9713 - mse: 859.3983
Epoch 55/500
2/2 [==============================] - 0s 5ms/step - loss: 23.1938 - mse: 805.5450
Epoch 56/500
2/2 [==============================] - 0s 11ms/step - loss: 10.8837 - mse: 170.9834
Epoch 57/500
2/2 [==============================] - 0s 12ms/step - loss: 12.7445 - mse: 198.7015
Epoch 58/500
2/2 [==============================] - 0s 9ms/step - loss: 9.5995 - mse: 102.5890
Epoch 59/500
2/2 [==============================] - 0s 9ms/step - loss: 12.5172 - mse: 216.3367
Epoch 60/500
2/2 [==============================] - 0s 9ms/step - loss: 12.3200 - mse: 208.6370
Epoch 61/500
2/2 [==============================] - 0s 7ms/step - loss: 17.4604 - mse: 428.6392
Epoch 62/500
2/2 [==============================] - 0s 7ms/step - loss: 10.6052 - mse: 136.9776
Epoch 63/500
2/2 [==============================] - 0s 5ms/step - loss: 10.4893 - mse: 152.4554
Epoch 64/500
2/2 [==============================] - 0s 7ms/step - loss: 24.8450 - mse: 911.7509
Epoch 65/500
2/2 [==============================] - 0s 8ms/step - loss: 10.6761 - mse: 142.7374
Epoch 66/500
2/2 [==============================] - 0s 9ms/step - loss: 21.7809 - mse: 704.4487
Epoch 67/500
2/2 [==============================] - 0s 4ms/step - loss: 10.7136 - mse: 136.0194
Epoch 68/500
2/2 [==============================] - 0s 6ms/step - loss: 10.6397 - mse: 149.2299
Epoch 69/500
2/2 [==============================] - 0s 4ms/step - loss: 22.6914 - mse: 742.1758
Epoch 70/500
2/2 [==============================] - 0s 12ms/step - loss: 9.3316 - mse: 166.1627
Epoch 71/500
2/2 [==============================] - 0s 11ms/step - loss: 15.4355 - mse: 323.0844
Epoch 72/500
2/2 [==============================] - 0s 8ms/step - loss: 6.7437 - mse: 67.0210
Epoch 73/500
2/2 [==============================] - 0s 9ms/step - loss: 11.6891 - mse: 183.7296
Epoch 74/500
2/2 [==============================] - 0s 10ms/step - loss: 24.0400 - mse: 908.8988
Epoch 75/500
2/2 [==============================] - 0s 5ms/step - loss: 9.5896 - mse: 149.3948
Epoch 76/500
2/2 [==============================] - 0s 8ms/step - loss: 12.4371 - mse: 188.3310
Epoch 77/500
2/2 [==============================] - 0s 8ms/step - loss: 16.6488 - mse: 429.2705
Epoch 78/500
2/2 [==============================] - 0s 20ms/step - loss: 9.0614 - mse: 95.4869
Epoch 79/500
2/2 [==============================] - 0s 10ms/step - loss: 23.9675 - mse: 864.0859
Epoch 80/500
2/2 [==============================] - 0s 13ms/step - loss: 26.7463 - mse: 1104.4030
Epoch 81/500
2/2 [==============================] - 0s 7ms/step - loss: 11.6714 - mse: 170.7055
Epoch 82/500
2/2 [==============================] - 0s 8ms/step - loss: 12.0228 - mse: 211.9191
Epoch 83/500
2/2 [==============================] - 0s 6ms/step - loss: 17.4218 - mse: 395.5590
Epoch 84/500
2/2 [==============================] - 0s 5ms/step - loss: 7.2629 - mse: 73.0935
Epoch 85/500
2/2 [==============================] - 0s 9ms/step - loss: 14.9650 - mse: 312.8361
Epoch 86/500
2/2 [==============================] - 0s 9ms/step - loss: 15.2862 - mse: 315.3606
Epoch 87/500
2/2 [==============================] - 0s 10ms/step - loss: 19.1086 - mse: 521.2535
Epoch 88/500
2/2 [==============================] - 0s 6ms/step - loss: 29.8228 - mse: 1287.1902
Epoch 89/500
2/2 [==============================] - 0s 5ms/step - loss: 10.1742 - mse: 124.1342
Epoch 90/500
2/2 [==============================] - 0s 14ms/step - loss: 21.5240 - mse: 663.8608
Epoch 91/500
2/2 [==============================] - 0s 5ms/step - loss: 10.5716 - mse: 161.7467
Epoch 92/500
2/2 [==============================] - 0s 9ms/step - loss: 18.3977 - mse: 464.1323
Epoch 93/500
2/2 [==============================] - 0s 7ms/step - loss: 7.4138 - mse: 81.9820
Epoch 94/500
2/2 [==============================] - 0s 4ms/step - loss: 17.7380 - mse: 445.7377
Epoch 95/500
2/2 [==============================] - 0s 5ms/step - loss: 11.1144 - mse: 164.0820
Epoch 96/500
2/2 [==============================] - 0s 7ms/step - loss: 19.4346 - mse: 510.5842
Epoch 97/500
2/2 [==============================] - 0s 7ms/step - loss: 12.1593 - mse: 209.9755
Epoch 98/500
2/2 [==============================] - 0s 6ms/step - loss: 11.5653 - mse: 169.4052
Epoch 99/500
2/2 [==============================] - 0s 25ms/step - loss: 13.8827 - mse: 265.4630
Epoch 100/500
2/2 [==============================] - 0s 4ms/step - loss: 20.2277 - mse: 608.8219
Epoch 101/500
2/2 [==============================] - 0s 5ms/step - loss: 11.4479 - mse: 177.1446
Epoch 102/500
2/2 [==============================] - 0s 6ms/step - loss: 17.4842 - mse: 426.5328
Epoch 103/500
2/2 [==============================] - 0s 11ms/step - loss: 7.0217 - mse: 65.2649
Epoch 104/500
2/2 [==============================] - 0s 15ms/step - loss: 23.5789 - mse: 757.4910
Epoch 105/500
2/2 [==============================] - 0s 21ms/step - loss: 16.8932 - mse: 443.0954
Epoch 106/500
2/2 [==============================] - 0s 20ms/step - loss: 9.2954 - mse: 144.6570
Epoch 107/500
2/2 [==============================] - 0s 5ms/step - loss: 25.3749 - mse: 934.2098
Epoch 108/500
2/2 [==============================] - 0s 15ms/step - loss: 13.4621 - mse: 269.8598
Epoch 109/500
2/2 [==============================] - 0s 14ms/step - loss: 9.5238 - mse: 108.7908
Epoch 110/500
2/2 [==============================] - 0s 11ms/step - loss: 9.6722 - mse: 128.8767
Epoch 111/500
2/2 [==============================] - 0s 17ms/step - loss: 14.5987 - mse: 295.9862
Epoch 112/500
2/2 [==============================] - 0s 10ms/step - loss: 9.5670 - mse: 123.4114
Epoch 113/500
2/2 [==============================] - 0s 9ms/step - loss: 17.8092 - mse: 460.3868
Epoch 114/500
2/2 [==============================] - 0s 9ms/step - loss: 17.1782 - mse: 441.2209
Epoch 115/500
2/2 [==============================] - 0s 9ms/step - loss: 11.1182 - mse: 155.0962
Epoch 116/500
2/2 [==============================] - 0s 13ms/step - loss: 23.3071 - mse: 791.7660
Epoch 117/500
2/2 [==============================] - 0s 8ms/step - loss: 9.6144 - mse: 126.1895
Epoch 118/500
2/2 [==============================] - 0s 10ms/step - loss: 10.6899 - mse: 140.2946
Epoch 119/500
2/2 [==============================] - 0s 5ms/step - loss: 8.0355 - mse: 78.3508
Epoch 120/500
2/2 [==============================] - 0s 4ms/step - loss: 29.6859 - mse: 1315.8650
Epoch 121/500
2/2 [==============================] - 0s 17ms/step - loss: 8.0714 - mse: 70.6648
Epoch 122/500
2/2 [==============================] - 0s 15ms/step - loss: 28.3086 - mse: 1233.3851
Epoch 123/500
2/2 [==============================] - 0s 6ms/step - loss: 32.9014 - mse: 1591.9508
Epoch 124/500
2/2 [==============================] - 0s 16ms/step - loss: 19.6291 - mse: 571.6958
Epoch 125/500
2/2 [==============================] - 0s 19ms/step - loss: 7.0095 - mse: 72.1092
Epoch 126/500
2/2 [==============================] - 0s 4ms/step - loss: 21.8056 - mse: 666.4337
Epoch 127/500
2/2 [==============================] - 0s 9ms/step - loss: 7.9812 - mse: 69.5627
Epoch 128/500
2/2 [==============================] - 0s 24ms/step - loss: 21.0585 - mse: 653.6482
Epoch 129/500
2/2 [==============================] - 0s 11ms/step - loss: 9.0107 - mse: 120.8763
Epoch 130/500
2/2 [==============================] - 0s 7ms/step - loss: 24.0502 - mse: 815.7741
Epoch 131/500
2/2 [==============================] - 0s 20ms/step - loss: 9.7537 - mse: 133.0194
Epoch 132/500
2/2 [==============================] - 0s 6ms/step - loss: 18.3052 - mse: 500.9212
Epoch 133/500
2/2 [==============================] - 0s 23ms/step - loss: 7.5833 - mse: 80.1913
Epoch 134/500
2/2 [==============================] - 0s 8ms/step - loss: 18.5755 - mse: 506.3340
Epoch 135/500
2/2 [==============================] - 0s 28ms/step - loss: 10.5360 - mse: 148.5977
Epoch 136/500
2/2 [==============================] - 0s 10ms/step - loss: 18.2694 - mse: 472.4461
Epoch 137/500
2/2 [==============================] - 0s 19ms/step - loss: 23.1658 - mse: 724.0384
Epoch 138/500
2/2 [==============================] - 0s 13ms/step - loss: 9.1362 - mse: 139.5299
Epoch 139/500
2/2 [==============================] - 0s 17ms/step - loss: 8.9181 - mse: 144.6079
Epoch 140/500
2/2 [==============================] - 0s 25ms/step - loss: 16.4732 - mse: 389.2470
Epoch 141/500
2/2 [==============================] - 0s 5ms/step - loss: 8.4208 - mse: 91.7413
Epoch 142/500
2/2 [==============================] - 0s 7ms/step - loss: 36.9540 - mse: 2404.8645
Epoch 143/500
2/2 [==============================] - 0s 8ms/step - loss: 25.5820 - mse: 927.2340
Epoch 144/500
2/2 [==============================] - 0s 20ms/step - loss: 9.5392 - mse: 144.3266
Epoch 145/500
2/2 [==============================] - 0s 6ms/step - loss: 26.6058 - mse: 957.1339
Epoch 146/500
2/2 [==============================] - 0s 5ms/step - loss: 8.7248 - mse: 109.2954
Epoch 147/500
2/2 [==============================] - 0s 7ms/step - loss: 15.6172 - mse: 311.2310
Epoch 148/500
2/2 [==============================] - 0s 6ms/step - loss: 18.3065 - mse: 464.7855
Epoch 149/500
2/2 [==============================] - 0s 5ms/step - loss: 8.1994 - mse: 106.4584
Epoch 150/500
2/2 [==============================] - 0s 5ms/step - loss: 7.4964 - mse: 66.4456
Epoch 151/500
2/2 [==============================] - 0s 8ms/step - loss: 18.3374 - mse: 485.9980
Epoch 152/500
2/2 [==============================] - 0s 5ms/step - loss: 10.2895 - mse: 130.5616
Epoch 153/500
2/2 [==============================] - 0s 20ms/step - loss: 29.6425 - mse: 1268.4640
Epoch 154/500
2/2 [==============================] - 0s 11ms/step - loss: 10.5555 - mse: 201.2336
Epoch 155/500
2/2 [==============================] - 0s 6ms/step - loss: 15.4537 - mse: 347.2168
Epoch 156/500
2/2 [==============================] - 0s 8ms/step - loss: 17.0174 - mse: 438.8493
Epoch 157/500
2/2 [==============================] - 0s 5ms/step - loss: 32.8218 - mse: 1744.8157
Epoch 158/500
2/2 [==============================] - 0s 12ms/step - loss: 10.7038 - mse: 151.0814
Epoch 159/500
2/2 [==============================] - 0s 5ms/step - loss: 8.9054 - mse: 97.7705
Epoch 160/500
2/2 [==============================] - 0s 5ms/step - loss: 22.1321 - mse: 709.4400
Epoch 161/500
2/2 [==============================] - 0s 8ms/step - loss: 11.7113 - mse: 202.9613
Epoch 162/500
2/2 [==============================] - 0s 10ms/step - loss: 21.5734 - mse: 670.2867
Epoch 163/500
2/2 [==============================] - 0s 10ms/step - loss: 19.2485 - mse: 533.4080
Epoch 164/500
2/2 [==============================] - 0s 6ms/step - loss: 11.0156 - mse: 177.6828
Epoch 165/500
2/2 [==============================] - 0s 6ms/step - loss: 9.6187 - mse: 179.8634
Epoch 166/500
2/2 [==============================] - 0s 23ms/step - loss: 21.5908 - mse: 681.9664
Epoch 167/500
2/2 [==============================] - 0s 14ms/step - loss: 26.2851 - mse: 1048.8456
Epoch 168/500
2/2 [==============================] - 0s 10ms/step - loss: 9.8525 - mse: 118.2891
Epoch 169/500
2/2 [==============================] - 0s 6ms/step - loss: 22.5631 - mse: 787.0594
Epoch 170/500
2/2 [==============================] - 0s 5ms/step - loss: 10.1499 - mse: 195.9051
Epoch 171/500
2/2 [==============================] - 0s 5ms/step - loss: 18.0464 - mse: 503.4531
Epoch 172/500
2/2 [==============================] - 0s 10ms/step - loss: 28.8377 - mse: 1305.7993
Epoch 173/500
2/2 [==============================] - 0s 19ms/step - loss: 16.5279 - mse: 432.1444
Epoch 174/500
2/2 [==============================] - 0s 10ms/step - loss: 11.2115 - mse: 199.9692
Epoch 175/500
2/2 [==============================] - 0s 8ms/step - loss: 27.5839 - mse: 1097.8187
Epoch 176/500
2/2 [==============================] - 0s 5ms/step - loss: 8.2680 - mse: 77.5372
Epoch 177/500
2/2 [==============================] - 0s 10ms/step - loss: 9.2580 - mse: 108.9075
Epoch 178/500
2/2 [==============================] - 0s 4ms/step - loss: 18.1440 - mse: 466.0513
Epoch 179/500
2/2 [==============================] - 0s 8ms/step - loss: 10.5995 - mse: 147.8543
Epoch 180/500
2/2 [==============================] - 0s 7ms/step - loss: 7.8992 - mse: 100.6932
Epoch 181/500
2/2 [==============================] - 0s 5ms/step - loss: 17.4015 - mse: 438.6036
Epoch 182/500
2/2 [==============================] - 0s 6ms/step - loss: 11.0089 - mse: 157.1596
Epoch 183/500
2/2 [==============================] - 0s 5ms/step - loss: 11.7027 - mse: 203.9956
Epoch 184/500
2/2 [==============================] - 0s 5ms/step - loss: 30.4062 - mse: 1387.3850
Epoch 185/500
2/2 [==============================] - 0s 4ms/step - loss: 7.5557 - mse: 98.6847
Epoch 186/500
2/2 [==============================] - 0s 5ms/step - loss: 15.9905 - mse: 369.3700
Epoch 187/500
2/2 [==============================] - 0s 8ms/step - loss: 8.5579 - mse: 85.4846
Epoch 188/500
2/2 [==============================] - 0s 6ms/step - loss: 28.7339 - mse: 1175.7063
Epoch 189/500
2/2 [==============================] - 0s 10ms/step - loss: 13.1689 - mse: 271.3447
Epoch 190/500
2/2 [==============================] - 0s 7ms/step - loss: 18.3101 - mse: 512.5546
Epoch 191/500
2/2 [==============================] - 0s 8ms/step - loss: 13.7376 - mse: 264.9675
Epoch 192/500
2/2 [==============================] - 0s 7ms/step - loss: 13.7104 - mse: 261.0630
Epoch 193/500
2/2 [==============================] - 0s 10ms/step - loss: 28.5842 - mse: 1135.1305
Epoch 194/500
2/2 [==============================] - 0s 16ms/step - loss: 7.0707 - mse: 78.6824
Epoch 195/500
2/2 [==============================] - 0s 14ms/step - loss: 7.0550 - mse: 74.9454
Epoch 196/500
2/2 [==============================] - 0s 5ms/step - loss: 22.0067 - mse: 709.4540
Epoch 197/500
2/2 [==============================] - 0s 20ms/step - loss: 20.8443 - mse: 643.6118
Epoch 198/500
2/2 [==============================] - 0s 8ms/step - loss: 12.4713 - mse: 236.5509
Epoch 199/500
2/2 [==============================] - 0s 12ms/step - loss: 17.9099 - mse: 460.3528
Epoch 200/500
2/2 [==============================] - 0s 8ms/step - loss: 13.7493 - mse: 282.2666
Epoch 201/500
2/2 [==============================] - 0s 12ms/step - loss: 5.4687 - mse: 41.0882
Epoch 202/500
2/2 [==============================] - 0s 8ms/step - loss: 13.7005 - mse: 293.8444
Epoch 203/500
2/2 [==============================] - 0s 8ms/step - loss: 9.4142 - mse: 139.9066
Epoch 204/500
2/2 [==============================] - 0s 13ms/step - loss: 20.9796 - mse: 656.2624
Epoch 205/500
2/2 [==============================] - 0s 5ms/step - loss: 9.5470 - mse: 127.9145
Epoch 206/500
2/2 [==============================] - 0s 8ms/step - loss: 11.7256 - mse: 195.6239
Epoch 207/500
2/2 [==============================] - 0s 6ms/step - loss: 14.3772 - mse: 317.1676
Epoch 208/500
2/2 [==============================] - 0s 5ms/step - loss: 14.8579 - mse: 323.1452
Epoch 209/500
2/2 [==============================] - 0s 6ms/step - loss: 14.9706 - mse: 345.7375
Epoch 210/500
2/2 [==============================] - 0s 4ms/step - loss: 17.8998 - mse: 467.4644
Epoch 211/500
2/2 [==============================] - 0s 11ms/step - loss: 9.8327 - mse: 145.8637
Epoch 212/500
2/2 [==============================] - 0s 20ms/step - loss: 18.3352 - mse: 507.7762
Epoch 213/500
2/2 [==============================] - 0s 29ms/step - loss: 15.0383 - mse: 307.6407
Epoch 214/500
2/2 [==============================] - 0s 5ms/step - loss: 14.5874 - mse: 293.5643
Epoch 215/500
2/2 [==============================] - 0s 5ms/step - loss: 23.3015 - mse: 799.4149
Epoch 216/500
2/2 [==============================] - 0s 14ms/step - loss: 13.3613 - mse: 278.0948
Epoch 217/500
2/2 [==============================] - 0s 6ms/step - loss: 9.8517 - mse: 136.3091
Epoch 218/500
2/2 [==============================] - 0s 9ms/step - loss: 12.5451 - mse: 206.5810
Epoch 219/500
2/2 [==============================] - 0s 8ms/step - loss: 4.9472 - mse: 37.4063
Epoch 220/500
2/2 [==============================] - 0s 4ms/step - loss: 7.1130 - mse: 57.5580
Epoch 221/500
2/2 [==============================] - 0s 5ms/step - loss: 35.4567 - mse: 2108.1008
Epoch 222/500
2/2 [==============================] - 0s 5ms/step - loss: 34.8634 - mse: 1966.9600
Epoch 223/500
2/2 [==============================] - 0s 6ms/step - loss: 7.9846 - mse: 119.7548
Epoch 224/500
2/2 [==============================] - 0s 14ms/step - loss: 14.7004 - mse: 318.5026
Epoch 225/500
2/2 [==============================] - 0s 8ms/step - loss: 16.7196 - mse: 360.6417
Epoch 226/500
2/2 [==============================] - 0s 9ms/step - loss: 15.9329 - mse: 369.8892
Epoch 227/500
2/2 [==============================] - 0s 9ms/step - loss: 16.1644 - mse: 369.5818
Epoch 228/500
2/2 [==============================] - 0s 5ms/step - loss: 13.9324 - mse: 282.1422
Epoch 229/500
2/2 [==============================] - 0s 9ms/step - loss: 18.0504 - mse: 457.4228
Epoch 230/500
2/2 [==============================] - 0s 5ms/step - loss: 15.6120 - mse: 314.6218
Epoch 231/500
2/2 [==============================] - 0s 8ms/step - loss: 21.2041 - mse: 670.7589
Epoch 232/500
2/2 [==============================] - 0s 11ms/step - loss: 25.2732 - mse: 913.5802
Epoch 233/500
2/2 [==============================] - 0s 12ms/step - loss: 16.3176 - mse: 387.7157
Epoch 234/500
2/2 [==============================] - 0s 6ms/step - loss: 7.2729 - mse: 66.5098
Epoch 235/500
2/2 [==============================] - 0s 6ms/step - loss: 16.9688 - mse: 403.1293
Epoch 236/500
2/2 [==============================] - 0s 16ms/step - loss: 7.1225 - mse: 69.7516
Epoch 237/500
2/2 [==============================] - 0s 12ms/step - loss: 9.2058 - mse: 118.9100
Epoch 238/500
2/2 [==============================] - 0s 10ms/step - loss: 8.0961 - mse: 86.7114
Epoch 239/500
2/2 [==============================] - 0s 7ms/step - loss: 17.0538 - mse: 438.0265
Epoch 240/500
2/2 [==============================] - 0s 15ms/step - loss: 8.8627 - mse: 107.8191
Epoch 241/500
2/2 [==============================] - 0s 10ms/step - loss: 13.1711 - mse: 273.2420
Epoch 242/500
2/2 [==============================] - 0s 26ms/step - loss: 8.7886 - mse: 104.8365
Epoch 243/500
2/2 [==============================] - 0s 7ms/step - loss: 18.8161 - mse: 541.4709
Epoch 244/500
2/2 [==============================] - 0s 9ms/step - loss: 14.0531 - mse: 275.3817
Epoch 245/500
2/2 [==============================] - 0s 4ms/step - loss: 14.6831 - mse: 290.1829
Epoch 246/500
2/2 [==============================] - 0s 5ms/step - loss: 15.8045 - mse: 370.7156
Epoch 247/500
2/2 [==============================] - 0s 6ms/step - loss: 17.6810 - mse: 421.4948
Epoch 248/500
2/2 [==============================] - 0s 6ms/step - loss: 13.2367 - mse: 251.7198
Epoch 249/500
2/2 [==============================] - 0s 5ms/step - loss: 14.5070 - mse: 288.7694
Epoch 250/500
2/2 [==============================] - 0s 5ms/step - loss: 23.2322 - mse: 793.2504
Epoch 251/500
2/2 [==============================] - 0s 22ms/step - loss: 9.3009 - mse: 119.0355
Epoch 252/500
2/2 [==============================] - 0s 12ms/step - loss: 36.6568 - mse: 2195.6157
Epoch 253/500
2/2 [==============================] - 0s 11ms/step - loss: 21.8205 - mse: 667.9622
Epoch 254/500
2/2 [==============================] - 0s 9ms/step - loss: 7.2792 - mse: 77.8094
Epoch 255/500
2/2 [==============================] - 0s 6ms/step - loss: 24.7126 - mse: 882.9651
Epoch 256/500
2/2 [==============================] - 0s 5ms/step - loss: 12.4220 - mse: 220.3513
Epoch 257/500
2/2 [==============================] - 0s 13ms/step - loss: 10.5823 - mse: 164.0438
Epoch 258/500
2/2 [==============================] - 0s 8ms/step - loss: 14.4883 - mse: 322.3509
Epoch 259/500
2/2 [==============================] - 0s 4ms/step - loss: 8.6132 - mse: 98.6931
Epoch 260/500
2/2 [==============================] - 0s 13ms/step - loss: 43.0580 - mse: 2975.2288
Epoch 261/500
2/2 [==============================] - 0s 4ms/step - loss: 18.4611 - mse: 493.6656
Epoch 262/500
2/2 [==============================] - 0s 9ms/step - loss: 6.8820 - mse: 94.3407
Epoch 263/500
2/2 [==============================] - 0s 4ms/step - loss: 13.7211 - mse: 268.5680
Epoch 264/500
2/2 [==============================] - 0s 6ms/step - loss: 21.0154 - mse: 653.9621
Epoch 265/500
2/2 [==============================] - 0s 5ms/step - loss: 19.3730 - mse: 545.6237
Epoch 266/500
2/2 [==============================] - 0s 5ms/step - loss: 11.4735 - mse: 234.7203
Epoch 267/500
2/2 [==============================] - 0s 14ms/step - loss: 7.5302 - mse: 112.0906
Epoch 268/500
2/2 [==============================] - 0s 8ms/step - loss: 21.6453 - mse: 670.2257
Epoch 269/500
2/2 [==============================] - 0s 8ms/step - loss: 33.1784 - mse: 1653.7346
Epoch 270/500
2/2 [==============================] - 0s 17ms/step - loss: 10.0833 - mse: 148.7349
Epoch 271/500
2/2 [==============================] - 0s 6ms/step - loss: 12.1012 - mse: 303.1721
Epoch 272/500
2/2 [==============================] - 0s 4ms/step - loss: 26.1372 - mse: 944.6783
Epoch 273/500
2/2 [==============================] - 0s 18ms/step - loss: 12.1751 - mse: 228.8029
Epoch 274/500
2/2 [==============================] - 0s 4ms/step - loss: 13.3272 - mse: 282.3031
Epoch 275/500
2/2 [==============================] - 0s 25ms/step - loss: 29.3775 - mse: 1229.1169
Epoch 276/500
2/2 [==============================] - 0s 8ms/step - loss: 7.3329 - mse: 104.3414
Epoch 277/500
2/2 [==============================] - 0s 4ms/step - loss: 31.1362 - mse: 1360.7684
Epoch 278/500
2/2 [==============================] - 0s 7ms/step - loss: 12.3016 - mse: 234.6619
Epoch 279/500
2/2 [==============================] - 0s 5ms/step - loss: 16.4103 - mse: 416.9118
Epoch 280/500
2/2 [==============================] - 0s 13ms/step - loss: 21.9118 - mse: 716.7849
Epoch 281/500
2/2 [==============================] - 0s 14ms/step - loss: 22.1500 - mse: 745.2045
Epoch 282/500
2/2 [==============================] - 0s 13ms/step - loss: 7.7429 - mse: 90.5224
Epoch 283/500
2/2 [==============================] - 0s 12ms/step - loss: 8.1429 - mse: 87.9049
Epoch 284/500
2/2 [==============================] - 0s 14ms/step - loss: 24.9434 - mse: 936.1584
Epoch 285/500
2/2 [==============================] - 0s 17ms/step - loss: 13.6958 - mse: 299.3605
Epoch 286/500
2/2 [==============================] - 0s 18ms/step - loss: 6.8926 - mse: 81.2997
Epoch 287/500
2/2 [==============================] - 0s 30ms/step - loss: 24.5352 - mse: 862.2884
Epoch 288/500
2/2 [==============================] - 0s 9ms/step - loss: 20.1721 - mse: 605.2863
Epoch 289/500
2/2 [==============================] - 0s 19ms/step - loss: 11.9658 - mse: 244.8306
Epoch 290/500
2/2 [==============================] - 0s 5ms/step - loss: 16.5391 - mse: 366.5722
Epoch 291/500
2/2 [==============================] - 0s 8ms/step - loss: 16.8017 - mse: 414.8180
Epoch 292/500
2/2 [==============================] - 0s 4ms/step - loss: 9.4642 - mse: 213.4542
Epoch 293/500
2/2 [==============================] - 0s 13ms/step - loss: 15.2711 - mse: 318.1686
Epoch 294/500
2/2 [==============================] - 0s 7ms/step - loss: 22.7179 - mse: 780.8715
Epoch 295/500
2/2 [==============================] - 0s 5ms/step - loss: 17.9234 - mse: 458.5727
Epoch 296/500
2/2 [==============================] - 0s 4ms/step - loss: 6.1742 - mse: 59.0634
Epoch 297/500
2/2 [==============================] - 0s 17ms/step - loss: 10.9440 - mse: 254.2858
Epoch 298/500
2/2 [==============================] - 0s 11ms/step - loss: 23.1530 - mse: 785.9871
Epoch 299/500
2/2 [==============================] - 0s 8ms/step - loss: 17.7331 - mse: 469.9260
Epoch 300/500
2/2 [==============================] - 0s 7ms/step - loss: 6.9824 - mse: 65.0501
Epoch 301/500
2/2 [==============================] - 0s 17ms/step - loss: 25.1857 - mse: 898.2772
Epoch 302/500
2/2 [==============================] - 0s 16ms/step - loss: 8.9025 - mse: 116.0075
Epoch 303/500
2/2 [==============================] - 0s 16ms/step - loss: 17.7668 - mse: 461.9366
Epoch 304/500
2/2 [==============================] - 0s 8ms/step - loss: 11.0002 - mse: 173.9247
Epoch 305/500
2/2 [==============================] - 0s 4ms/step - loss: 12.9191 - mse: 271.8379
Epoch 306/500
2/2 [==============================] - 0s 13ms/step - loss: 8.4033 - mse: 94.4443
Epoch 307/500
2/2 [==============================] - 0s 10ms/step - loss: 13.6094 - mse: 279.1168
Epoch 308/500
2/2 [==============================] - 0s 11ms/step - loss: 7.4404 - mse: 78.3838
Epoch 309/500
2/2 [==============================] - 0s 13ms/step - loss: 9.4642 - mse: 138.8856
Epoch 310/500
2/2 [==============================] - 0s 8ms/step - loss: 10.7099 - mse: 198.2184
Epoch 311/500
2/2 [==============================] - 0s 8ms/step - loss: 13.2814 - mse: 254.2446
Epoch 312/500
2/2 [==============================] - 0s 14ms/step - loss: 29.9763 - mse: 1241.6277
Epoch 313/500
2/2 [==============================] - 0s 8ms/step - loss: 7.6304 - mse: 104.4100
Epoch 314/500
2/2 [==============================] - 0s 17ms/step - loss: 9.9106 - mse: 239.5552
Epoch 315/500
2/2 [==============================] - 0s 9ms/step - loss: 23.7669 - mse: 820.7440
Epoch 316/500
2/2 [==============================] - 0s 17ms/step - loss: 16.3936 - mse: 404.4791
Epoch 317/500
2/2 [==============================] - 0s 11ms/step - loss: 21.0758 - mse: 606.4597
Epoch 318/500
2/2 [==============================] - 0s 5ms/step - loss: 7.9367 - mse: 81.3764
Epoch 319/500
2/2 [==============================] - 0s 4ms/step - loss: 17.9731 - mse: 479.6087
Epoch 320/500
2/2 [==============================] - 0s 9ms/step - loss: 10.2375 - mse: 174.0453
Epoch 321/500
2/2 [==============================] - 0s 5ms/step - loss: 8.3338 - mse: 110.8493
Epoch 322/500
2/2 [==============================] - 0s 7ms/step - loss: 5.0621 - mse: 39.6195
Epoch 323/500
2/2 [==============================] - 0s 15ms/step - loss: 23.5109 - mse: 802.4886
Epoch 324/500
2/2 [==============================] - 0s 4ms/step - loss: 6.8309 - mse: 57.6746
Epoch 325/500
2/2 [==============================] - 0s 12ms/step - loss: 16.3863 - mse: 384.4402
Epoch 326/500
2/2 [==============================] - 0s 5ms/step - loss: 7.5019 - mse: 78.7650
Epoch 327/500
2/2 [==============================] - 0s 17ms/step - loss: 20.0573 - mse: 568.8238
Epoch 328/500
2/2 [==============================] - 0s 9ms/step - loss: 13.7661 - mse: 265.8646
Epoch 329/500
2/2 [==============================] - 0s 4ms/step - loss: 16.8282 - mse: 430.9921
Epoch 330/500
2/2 [==============================] - 0s 20ms/step - loss: 7.0514 - mse: 81.9922
Epoch 331/500
2/2 [==============================] - 0s 5ms/step - loss: 21.4846 - mse: 704.3916
Epoch 332/500
2/2 [==============================] - 0s 8ms/step - loss: 12.2880 - mse: 236.5700
Epoch 333/500
2/2 [==============================] - 0s 4ms/step - loss: 11.8117 - mse: 222.0919
Epoch 334/500
2/2 [==============================] - 0s 13ms/step - loss: 8.3600 - mse: 169.8893
Epoch 335/500
2/2 [==============================] - 0s 5ms/step - loss: 12.4833 - mse: 276.8588
Epoch 336/500
2/2 [==============================] - 0s 4ms/step - loss: 32.2171 - mse: 1416.1726
Epoch 337/500
2/2 [==============================] - 0s 5ms/step - loss: 10.4477 - mse: 180.3260
Epoch 338/500
2/2 [==============================] - 0s 7ms/step - loss: 19.6832 - mse: 573.4708
Epoch 339/500
2/2 [==============================] - 0s 14ms/step - loss: 35.0762 - mse: 1849.1559
Epoch 340/500
2/2 [==============================] - 0s 5ms/step - loss: 10.4192 - mse: 181.4145
Epoch 341/500
2/2 [==============================] - 0s 6ms/step - loss: 9.7625 - mse: 156.3231
Epoch 342/500
2/2 [==============================] - 0s 4ms/step - loss: 11.9500 - mse: 188.5578
Epoch 343/500
2/2 [==============================] - 0s 14ms/step - loss: 9.3943 - mse: 145.3949
Epoch 344/500
2/2 [==============================] - 0s 4ms/step - loss: 5.6071 - mse: 44.6760
Epoch 345/500
2/2 [==============================] - 0s 8ms/step - loss: 37.4876 - mse: 2291.7800
Epoch 346/500
2/2 [==============================] - 0s 4ms/step - loss: 16.8830 - mse: 411.1354
Epoch 347/500
2/2 [==============================] - 0s 4ms/step - loss: 12.8748 - mse: 281.5984
Epoch 348/500
2/2 [==============================] - 0s 13ms/step - loss: 8.1960 - mse: 162.6964
Epoch 349/500
2/2 [==============================] - 0s 12ms/step - loss: 13.5568 - mse: 261.1367
Epoch 350/500
2/2 [==============================] - 0s 5ms/step - loss: 15.4354 - mse: 333.2079
Epoch 351/500
2/2 [==============================] - 0s 5ms/step - loss: 32.9626 - mse: 1524.7930
Epoch 352/500
2/2 [==============================] - 0s 18ms/step - loss: 14.2040 - mse: 282.7505
Epoch 353/500
2/2 [==============================] - 0s 5ms/step - loss: 15.9196 - mse: 374.1913
Epoch 354/500
2/2 [==============================] - 0s 9ms/step - loss: 19.0878 - mse: 534.9767
Epoch 355/500
2/2 [==============================] - 0s 5ms/step - loss: 34.1178 - mse: 1782.7045
Epoch 356/500
2/2 [==============================] - 0s 5ms/step - loss: 7.6798 - mse: 90.9119
Epoch 357/500
2/2 [==============================] - 0s 4ms/step - loss: 25.2287 - mse: 974.0955
Epoch 358/500
2/2 [==============================] - 0s 5ms/step - loss: 22.6759 - mse: 742.0946
Epoch 359/500
2/2 [==============================] - 0s 5ms/step - loss: 8.8765 - mse: 198.2594
Epoch 360/500
2/2 [==============================] - 0s 5ms/step - loss: 21.4709 - mse: 687.3273
Epoch 361/500
2/2 [==============================] - 0s 18ms/step - loss: 20.6073 - mse: 616.7955
Epoch 362/500
2/2 [==============================] - 0s 4ms/step - loss: 7.0611 - mse: 69.3817
Epoch 363/500
2/2 [==============================] - 0s 13ms/step - loss: 25.8117 - mse: 994.8207
Epoch 364/500
2/2 [==============================] - 0s 4ms/step - loss: 32.2247 - mse: 1527.6091
Epoch 365/500
2/2 [==============================] - 0s 4ms/step - loss: 10.0205 - mse: 161.6851
Epoch 366/500
2/2 [==============================] - 0s 4ms/step - loss: 9.6722 - mse: 227.5728
Epoch 367/500
2/2 [==============================] - 0s 5ms/step - loss: 30.4171 - mse: 1294.6042
Epoch 368/500
2/2 [==============================] - 0s 23ms/step - loss: 10.5020 - mse: 236.3137
Epoch 369/500
2/2 [==============================] - 0s 7ms/step - loss: 14.9909 - mse: 330.8540
Epoch 370/500
2/2 [==============================] - 0s 22ms/step - loss: 14.6580 - mse: 308.0733
Epoch 371/500
2/2 [==============================] - 0s 6ms/step - loss: 23.3672 - mse: 789.0350
Epoch 372/500
2/2 [==============================] - 0s 7ms/step - loss: 13.1025 - mse: 290.2273
Epoch 373/500
2/2 [==============================] - 0s 8ms/step - loss: 9.2586 - mse: 140.7055
Epoch 374/500
2/2 [==============================] - 0s 4ms/step - loss: 9.6648 - mse: 194.1148
Epoch 375/500
2/2 [==============================] - 0s 5ms/step - loss: 13.0041 - mse: 237.9866
Epoch 376/500
2/2 [==============================] - 0s 15ms/step - loss: 14.8863 - mse: 324.7534
Epoch 377/500
2/2 [==============================] - 0s 8ms/step - loss: 14.7932 - mse: 297.2056
Epoch 378/500
2/2 [==============================] - 0s 10ms/step - loss: 16.2751 - mse: 418.5629
Epoch 379/500
2/2 [==============================] - 0s 10ms/step - loss: 20.8307 - mse: 596.8128
Epoch 380/500
2/2 [==============================] - 0s 7ms/step - loss: 33.5318 - mse: 1684.8971
Epoch 381/500
2/2 [==============================] - 0s 7ms/step - loss: 8.2166 - mse: 100.6387
Epoch 382/500
2/2 [==============================] - 0s 8ms/step - loss: 13.0960 - mse: 282.0389
Epoch 383/500
2/2 [==============================] - 0s 5ms/step - loss: 8.3999 - mse: 126.0927
Epoch 384/500
2/2 [==============================] - 0s 8ms/step - loss: 7.1283 - mse: 72.4400
Epoch 385/500
2/2 [==============================] - 0s 13ms/step - loss: 10.9390 - mse: 249.4379
Epoch 386/500
2/2 [==============================] - 0s 8ms/step - loss: 19.7654 - mse: 601.5789
Epoch 387/500
2/2 [==============================] - 0s 14ms/step - loss: 24.8625 - mse: 896.3713
Epoch 388/500
2/2 [==============================] - 0s 4ms/step - loss: 8.7422 - mse: 128.0919
Epoch 389/500
2/2 [==============================] - 0s 4ms/step - loss: 5.9488 - mse: 48.1570
Epoch 390/500
2/2 [==============================] - 0s 12ms/step - loss: 24.4401 - mse: 881.1058
Epoch 391/500
2/2 [==============================] - 0s 13ms/step - loss: 5.9771 - mse: 73.2798
Epoch 392/500
2/2 [==============================] - 0s 13ms/step - loss: 16.3250 - mse: 379.2599
Epoch 393/500
2/2 [==============================] - 0s 4ms/step - loss: 6.0917 - mse: 76.7832
Epoch 394/500
2/2 [==============================] - 0s 4ms/step - loss: 11.0963 - mse: 204.7652
Epoch 395/500
2/2 [==============================] - 0s 11ms/step - loss: 14.9601 - mse: 336.1571
Epoch 396/500
2/2 [==============================] - 0s 6ms/step - loss: 7.6462 - mse: 103.3248
Epoch 397/500
2/2 [==============================] - 0s 89ms/step - loss: 8.7654 - mse: 136.5435
Epoch 398/500
2/2 [==============================] - 0s 4ms/step - loss: 14.5992 - mse: 320.4543
Epoch 399/500
2/2 [==============================] - 0s 4ms/step - loss: 11.3166 - mse: 280.6894
Epoch 400/500
2/2 [==============================] - 0s 17ms/step - loss: 21.9080 - mse: 741.3455
Epoch 401/500
2/2 [==============================] - 0s 21ms/step - loss: 14.8654 - mse: 348.6730
Epoch 402/500
2/2 [==============================] - 0s 6ms/step - loss: 8.4970 - mse: 115.5855
Epoch 403/500
2/2 [==============================] - 0s 6ms/step - loss: 10.3957 - mse: 191.0405
Epoch 404/500
2/2 [==============================] - 0s 6ms/step - loss: 10.2556 - mse: 207.5720
Epoch 405/500
2/2 [==============================] - 0s 13ms/step - loss: 6.3392 - mse: 68.2857
Epoch 406/500
2/2 [==============================] - 0s 10ms/step - loss: 17.4602 - mse: 460.2061
Epoch 407/500
2/2 [==============================] - 0s 30ms/step - loss: 11.4627 - mse: 284.7281
Epoch 408/500
2/2 [==============================] - 0s 5ms/step - loss: 20.7294 - mse: 662.1494
Epoch 409/500
2/2 [==============================] - 0s 10ms/step - loss: 31.3339 - mse: 1522.7719
Epoch 410/500
2/2 [==============================] - 0s 9ms/step - loss: 9.2542 - mse: 221.9490
Epoch 411/500
2/2 [==============================] - 0s 5ms/step - loss: 14.8621 - mse: 285.8412
Epoch 412/500
2/2 [==============================] - 0s 4ms/step - loss: 21.7182 - mse: 721.0385
Epoch 413/500
2/2 [==============================] - 0s 4ms/step - loss: 12.6615 - mse: 248.9698
Epoch 414/500
2/2 [==============================] - 0s 8ms/step - loss: 6.0687 - mse: 88.3709
Epoch 415/500
2/2 [==============================] - 0s 4ms/step - loss: 13.2201 - mse: 239.8953
Epoch 416/500
2/2 [==============================] - 0s 9ms/step - loss: 27.4244 - mse: 1037.9575
Epoch 417/500
2/2 [==============================] - 0s 4ms/step - loss: 10.6407 - mse: 207.4824
Epoch 418/500
2/2 [==============================] - 0s 36ms/step - loss: 12.8230 - mse: 236.7906
Epoch 419/500
2/2 [==============================] - 0s 5ms/step - loss: 15.8836 - mse: 374.5479
Epoch 420/500
2/2 [==============================] - 0s 4ms/step - loss: 24.7510 - mse: 849.5324
Epoch 421/500
2/2 [==============================] - 0s 7ms/step - loss: 17.3753 - mse: 444.1839
Epoch 422/500
2/2 [==============================] - 0s 8ms/step - loss: 7.8241 - mse: 157.5332
Epoch 423/500
2/2 [==============================] - 0s 7ms/step - loss: 25.3789 - mse: 907.6133
Epoch 424/500
2/2 [==============================] - 0s 6ms/step - loss: 15.1031 - mse: 369.3296
Epoch 425/500
2/2 [==============================] - 0s 5ms/step - loss: 7.1643 - mse: 75.3718
Epoch 426/500
2/2 [==============================] - 0s 5ms/step - loss: 20.3318 - mse: 572.0365
Epoch 427/500
2/2 [==============================] - 0s 9ms/step - loss: 6.3283 - mse: 84.9744
Epoch 428/500
2/2 [==============================] - 0s 10ms/step - loss: 12.9962 - mse: 288.4222
Epoch 429/500
2/2 [==============================] - 0s 5ms/step - loss: 10.7869 - mse: 192.7745
Epoch 430/500
2/2 [==============================] - 0s 6ms/step - loss: 11.4007 - mse: 248.2727
Epoch 431/500
2/2 [==============================] - 0s 7ms/step - loss: 10.6153 - mse: 218.3400
Epoch 432/500
2/2 [==============================] - 0s 8ms/step - loss: 11.4582 - mse: 239.2202
Epoch 433/500
2/2 [==============================] - 0s 10ms/step - loss: 11.3851 - mse: 296.6789
Epoch 434/500
2/2 [==============================] - 0s 7ms/step - loss: 30.3986 - mse: 1296.5627
Epoch 435/500
2/2 [==============================] - 0s 7ms/step - loss: 10.5052 - mse: 283.6820
Epoch 436/500
2/2 [==============================] - 0s 14ms/step - loss: 28.8810 - mse: 1216.2332
Epoch 437/500
2/2 [==============================] - 0s 12ms/step - loss: 8.5916 - mse: 209.5437
Epoch 438/500
2/2 [==============================] - 0s 10ms/step - loss: 12.7378 - mse: 264.4515
Epoch 439/500
2/2 [==============================] - 0s 8ms/step - loss: 33.6754 - mse: 1570.2357
Epoch 440/500
2/2 [==============================] - 0s 11ms/step - loss: 15.0962 - mse: 294.7130
Epoch 441/500
2/2 [==============================] - 0s 13ms/step - loss: 17.4813 - mse: 487.0568
Epoch 442/500
2/2 [==============================] - 0s 11ms/step - loss: 22.3049 - mse: 750.9299
Epoch 443/500
2/2 [==============================] - 0s 14ms/step - loss: 23.5841 - mse: 784.1398
Epoch 444/500
2/2 [==============================] - 0s 5ms/step - loss: 11.0008 - mse: 203.7588
Epoch 445/500
2/2 [==============================] - 0s 7ms/step - loss: 14.9175 - mse: 316.4787
Epoch 446/500
2/2 [==============================] - 0s 6ms/step - loss: 17.9979 - mse: 515.1707
Epoch 447/500
2/2 [==============================] - 0s 10ms/step - loss: 5.4482 - mse: 51.4485
Epoch 448/500
2/2 [==============================] - 0s 8ms/step - loss: 10.0527 - mse: 253.4831
Epoch 449/500
2/2 [==============================] - 0s 8ms/step - loss: 14.0052 - mse: 277.3351
Epoch 450/500
2/2 [==============================] - 0s 5ms/step - loss: 16.7782 - mse: 424.4223
Epoch 451/500
2/2 [==============================] - 0s 8ms/step - loss: 14.2937 - mse: 303.4475
Epoch 452/500
2/2 [==============================] - 0s 4ms/step - loss: 30.6192 - mse: 1329.8262
Epoch 453/500
2/2 [==============================] - 0s 8ms/step - loss: 7.6541 - mse: 159.0381
Epoch 454/500
2/2 [==============================] - 0s 7ms/step - loss: 28.1428 - mse: 1104.4135
Epoch 455/500
2/2 [==============================] - 0s 4ms/step - loss: 8.0017 - mse: 119.3277
Epoch 456/500
2/2 [==============================] - 0s 5ms/step - loss: 10.3933 - mse: 289.6396
Epoch 457/500
2/2 [==============================] - 0s 10ms/step - loss: 15.0242 - mse: 318.3532
Epoch 458/500
2/2 [==============================] - 0s 5ms/step - loss: 16.5653 - mse: 439.8002
Epoch 459/500
2/2 [==============================] - 0s 7ms/step - loss: 26.8566 - mse: 1033.5481
Epoch 460/500
2/2 [==============================] - 0s 4ms/step - loss: 12.4852 - mse: 251.1166
Epoch 461/500
2/2 [==============================] - 0s 8ms/step - loss: 12.4784 - mse: 272.9072
Epoch 462/500
2/2 [==============================] - 0s 7ms/step - loss: 13.3186 - mse: 245.2334
Epoch 463/500
2/2 [==============================] - 0s 4ms/step - loss: 29.5524 - mse: 1219.8655
Epoch 464/500
2/2 [==============================] - 0s 4ms/step - loss: 3.4664 - mse: 22.0146
Epoch 465/500
2/2 [==============================] - 0s 6ms/step - loss: 15.2136 - mse: 352.9364
Epoch 466/500
2/2 [==============================] - 0s 7ms/step - loss: 20.8327 - mse: 651.1586
Epoch 467/500
2/2 [==============================] - 0s 8ms/step - loss: 30.5108 - mse: 1421.0045
Epoch 468/500
2/2 [==============================] - 0s 4ms/step - loss: 11.0598 - mse: 236.2268
Epoch 469/500
2/2 [==============================] - 0s 5ms/step - loss: 12.8372 - mse: 278.2050
Epoch 470/500
2/2 [==============================] - 0s 5ms/step - loss: 3.2398 - mse: 14.0654
Epoch 471/500
2/2 [==============================] - 0s 6ms/step - loss: 16.6964 - mse: 372.9562
Epoch 472/500
2/2 [==============================] - 0s 8ms/step - loss: 13.3883 - mse: 253.4230
Epoch 473/500
2/2 [==============================] - 0s 17ms/step - loss: 15.2771 - mse: 396.6104
Epoch 474/500
2/2 [==============================] - 0s 6ms/step - loss: 11.7448 - mse: 305.3253
Epoch 475/500
2/2 [==============================] - 0s 5ms/step - loss: 16.4113 - mse: 397.1608
Epoch 476/500
2/2 [==============================] - 0s 10ms/step - loss: 13.8785 - mse: 273.7974
Epoch 477/500
2/2 [==============================] - 0s 7ms/step - loss: 30.6702 - mse: 1309.4620
Epoch 478/500
2/2 [==============================] - 0s 5ms/step - loss: 8.5880 - mse: 188.9075
Epoch 479/500
2/2 [==============================] - 0s 7ms/step - loss: 10.7384 - mse: 261.5207
Epoch 480/500
2/2 [==============================] - 0s 9ms/step - loss: 17.9051 - mse: 484.4149
Epoch 481/500
2/2 [==============================] - 0s 5ms/step - loss: 15.8094 - mse: 368.1040
Epoch 482/500
2/2 [==============================] - 0s 13ms/step - loss: 21.3054 - mse: 698.1629
Epoch 483/500
2/2 [==============================] - 0s 5ms/step - loss: 25.3845 - mse: 955.1268
Epoch 484/500
2/2 [==============================] - 0s 5ms/step - loss: 23.9816 - mse: 811.2802
Epoch 485/500
2/2 [==============================] - 0s 4ms/step - loss: 5.7734 - mse: 53.7612
Epoch 486/500
2/2 [==============================] - 0s 5ms/step - loss: 20.0011 - mse: 565.2890
Epoch 487/500
2/2 [==============================] - 0s 4ms/step - loss: 14.0419 - mse: 286.2682
Epoch 488/500
2/2 [==============================] - 0s 4ms/step - loss: 30.6088 - mse: 1334.5725
Epoch 489/500
2/2 [==============================] - 0s 48ms/step - loss: 11.9409 - mse: 237.4729
Epoch 490/500
2/2 [==============================] - 0s 24ms/step - loss: 12.7352 - mse: 252.5675
Epoch 491/500
2/2 [==============================] - 0s 22ms/step - loss: 23.6139 - mse: 837.7731
Epoch 492/500
2/2 [==============================] - 0s 11ms/step - loss: 20.5365 - mse: 592.2192
Epoch 493/500
2/2 [==============================] - 0s 109ms/step - loss: 4.9942 - mse: 48.0281
Epoch 494/500
2/2 [==============================] - 0s 24ms/step - loss: 12.7987 - mse: 247.7650
Epoch 495/500
2/2 [==============================] - 0s 32ms/step - loss: 13.3772 - mse: 239.0899
Epoch 496/500
2/2 [==============================] - 0s 32ms/step - loss: 12.6727 - mse: 241.5636
Epoch 497/500
2/2 [==============================] - 0s 18ms/step - loss: 17.6192 - mse: 515.3856
Epoch 498/500
2/2 [==============================] - 0s 27ms/step - loss: 23.5629 - mse: 816.5514
Epoch 499/500
2/2 [==============================] - 0s 12ms/step - loss: 9.3755 - mse: 136.1010
Epoch 500/500
2/2 [==============================] - 0s 8ms/step - loss: 14.6316 - mse: 295.2642
<keras.callbacks.History at 0x7f07fdfa8700>
[50]
0s
#comparing our model
Y_pred_3=model_3.predict(X_test)
plot_prediction(predictions=Y_pred_3) 

[51]
0s
#evaluation
mae_3=mae(Y_test,Y_pred_3.squeeze())
mse_3=mse(Y_test,Y_pred_3.squeeze())
mae_3,mse_3
(<tf.Tensor: shape=(), dtype=float32, numpy=68.713615>,
 <tf.Tensor: shape=(), dtype=float32, numpy=4808.0273>)
we have run a few expriments now
let us compare the results

[52]
0s
#let us compare our model's result using a pandas dataframe
import pandas as pd

model_result=[["model_1",mae_1.numpy(),mse_1.numpy()],["model_2",mae_2.numpy(),mse_2.numpy()],["model_3",mae_3.numpy(),mse_3.numpy()]]

all_result=pd.DataFrame(model_result,columns=["model","mae","mse"])
all_result

looks like model_2 performed the best
[53]
0s
model_2.summary()
Model: "sequential_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_7 (Dense)             (None, 10)                20        
                                                                 
 dense_8 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
_________________________________________________________________
Tracking your expriments
one really good habit in machin learning modelling is to track the results of your experiments.

In doing so it can be tedious when you are running a lot of experiments. luckily there are tools to help us:

tensorboard- a components build staight into tensorflow library to help tarck modelling experiments.

Weight and baises- A tool used for tacking all kinds of expriments (plugs staright into tensorboard)

saving our models
saving our models helps us to make it use outside our google colab. maybe in an app or web application.

There are basically two ways to save our model:

the savemodel format
HDF5 format
[54]
0s
#save model using savemodel format
model_2.save("best_model_SavedModel_format")
[55]
0s
#save model using HDF5 format
model_2.save("best_model_HDF5_format.h5")
loading the saved model
[56]
0s
loaded_savedmodel_model=tf.keras.models.load_model("/content/best_model_SavedModel_format")
loaded_savedmodel_model.summary()
Model: "sequential_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_7 (Dense)             (None, 10)                20        
                                                                 
 dense_8 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
_________________________________________________________________
[57]
0s
model_2.summary()
Model: "sequential_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_7 (Dense)             (None, 10)                20        
                                                                 
 dense_8 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
_________________________________________________________________
[58]
0s
#compare model_2 prediction with saved format model predictions
model_2_pred=model_2.predict(X_test)
loaded_savedmodel_model_pred=loaded_savedmodel_model.predict(X_test)
model_2_pred==loaded_savedmodel_model_pred
1/1 [==============================] - 0s 95ms/step
1/1 [==============================] - 0s 98ms/step
array([[ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True]])
[59]
0s
mae(Y_test,model_2_pred.squeeze()) == mae(Y_test,loaded_savedmodel_model_pred.squeeze())
<tf.Tensor: shape=(), dtype=bool, numpy=True>
[60]
0s
#load a model using H5 model
loaded_h5_model=tf.keras.models.load_model("/content/best_model_HDF5_format.h5")
loaded_h5_model.summary()
Model: "sequential_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_7 (Dense)             (None, 10)                20        
                                                                 
 dense_8 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
_________________________________________________________________
[61]
0s
#checks the h5 loaded prediction are same
model_2_pred=model_2.predict(X_test)
loaded_h5_model_pred=loaded_h5_model.predict(X_test)
model_2_pred==loaded_h5_model_pred
1/1 [==============================] - 0s 24ms/step
1/1 [==============================] - 0s 93ms/step
array([[ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True],
       [ True]])
how to download a file from google colab
if you want to download your files form google colab you can :

to the files section . right click on the file you wanna download and select download

we can use code to download a file

we will save it to google drive by connecting with google drive and copying it there.

[62]
0s
#Download a google file from google colab
from google.colab import files
files.download("/content/best_model_HDF5_format.h5")

[63]
0s
#save a file from google colab to google drive(requires mounting a drive)
!cp /content/best_model_HDF5_format.h5 /content/drive/MyDrive/Classroom
larger example
[64]
0s
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
[65]
0s
#read in the insurance dataset
insurance= pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")
insurance

[66]
0s
#let us one hot encode our dataframe so it all numbers
insurance_one_hot=pd.get_dummies(insurance)
insurance_one_hot.head()

[67]
0s
#create X and Y values (features and labels)
X=insurance_one_hot.drop("charges",axis=1)
Y=insurance_one_hot["charges"]
[68]
0s
#let us view X
X.head()

[69]
0s
#ley us view Y
Y.head()
0    16884.92400
1     1725.55230
2     4449.46200
3    21984.47061
4     3866.85520
Name: charges, dtype: float64
[70]
1s
#create a training and test sets
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)
len(X),len(X_train),len(X_test)
(1338, 1070, 268)
[71]
11s
#Build a neural network 
tf.random.set_seed(42)

#1. create model
insurance_model=tf.keras.Sequential([
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

#2.compile the model
insurance_model.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["mae"])

#3. fit the model
insurance_model.fit(X_train,Y_train,epochs=100)
Epoch 1/100
34/34 [==============================] - 0s 2ms/step - loss: 8637.0996 - mae: 8637.0996
Epoch 2/100
34/34 [==============================] - 0s 1ms/step - loss: 7886.7769 - mae: 7886.7769
Epoch 3/100
34/34 [==============================] - 0s 1ms/step - loss: 7558.1475 - mae: 7558.1475
Epoch 4/100
34/34 [==============================] - 0s 1ms/step - loss: 7792.0220 - mae: 7792.0220
Epoch 5/100
34/34 [==============================] - 0s 1ms/step - loss: 7748.3892 - mae: 7748.3892
Epoch 6/100
34/34 [==============================] - 0s 2ms/step - loss: 7595.3940 - mae: 7595.3940
Epoch 7/100
34/34 [==============================] - 0s 1ms/step - loss: 7589.9849 - mae: 7589.9849
Epoch 8/100
34/34 [==============================] - 0s 2ms/step - loss: 7698.5591 - mae: 7698.5591
Epoch 9/100
34/34 [==============================] - 0s 2ms/step - loss: 7496.7788 - mae: 7496.7788
Epoch 10/100
34/34 [==============================] - 0s 2ms/step - loss: 7493.1743 - mae: 7493.1743
Epoch 11/100
34/34 [==============================] - 0s 2ms/step - loss: 7769.7314 - mae: 7769.7314
Epoch 12/100
34/34 [==============================] - 0s 2ms/step - loss: 7706.9033 - mae: 7706.9033
Epoch 13/100
34/34 [==============================] - 0s 1ms/step - loss: 7687.7227 - mae: 7687.7227
Epoch 14/100
34/34 [==============================] - 0s 2ms/step - loss: 7689.8999 - mae: 7689.8999
Epoch 15/100
34/34 [==============================] - 0s 2ms/step - loss: 7393.5322 - mae: 7393.5322
Epoch 16/100
34/34 [==============================] - 0s 1ms/step - loss: 7780.6982 - mae: 7780.6982
Epoch 17/100
34/34 [==============================] - 0s 2ms/step - loss: 7578.5093 - mae: 7578.5093
Epoch 18/100
34/34 [==============================] - 0s 1ms/step - loss: 7750.8350 - mae: 7750.8350
Epoch 19/100
34/34 [==============================] - 0s 1ms/step - loss: 7739.2134 - mae: 7739.2134
Epoch 20/100
34/34 [==============================] - 0s 1ms/step - loss: 7875.0635 - mae: 7875.0635
Epoch 21/100
34/34 [==============================] - 0s 1ms/step - loss: 7466.6768 - mae: 7466.6768
Epoch 22/100
34/34 [==============================] - 0s 1ms/step - loss: 7941.2310 - mae: 7941.2310
Epoch 23/100
34/34 [==============================] - 0s 2ms/step - loss: 7640.2725 - mae: 7640.2725
Epoch 24/100
34/34 [==============================] - 0s 2ms/step - loss: 7539.2656 - mae: 7539.2656
Epoch 25/100
34/34 [==============================] - 0s 2ms/step - loss: 7619.9658 - mae: 7619.9658
Epoch 26/100
34/34 [==============================] - 0s 2ms/step - loss: 7644.1709 - mae: 7644.1709
Epoch 27/100
34/34 [==============================] - 0s 2ms/step - loss: 7709.0361 - mae: 7709.0361
Epoch 28/100
34/34 [==============================] - 0s 1ms/step - loss: 7366.8662 - mae: 7366.8662
Epoch 29/100
34/34 [==============================] - 0s 1ms/step - loss: 7444.3135 - mae: 7444.3135
Epoch 30/100
34/34 [==============================] - 0s 1ms/step - loss: 7616.4087 - mae: 7616.4087
Epoch 31/100
34/34 [==============================] - 0s 2ms/step - loss: 7686.3853 - mae: 7686.3853
Epoch 32/100
34/34 [==============================] - 0s 1ms/step - loss: 7548.0981 - mae: 7548.0981
Epoch 33/100
34/34 [==============================] - 0s 2ms/step - loss: 7501.5532 - mae: 7501.5532
Epoch 34/100
34/34 [==============================] - 0s 1ms/step - loss: 7363.4155 - mae: 7363.4155
Epoch 35/100
34/34 [==============================] - 0s 1ms/step - loss: 7295.4468 - mae: 7295.4468
Epoch 36/100
34/34 [==============================] - 0s 2ms/step - loss: 7569.8813 - mae: 7569.8813
Epoch 37/100
34/34 [==============================] - 0s 1ms/step - loss: 7548.2002 - mae: 7548.2002
Epoch 38/100
34/34 [==============================] - 0s 2ms/step - loss: 7424.3979 - mae: 7424.3979
Epoch 39/100
34/34 [==============================] - 0s 1ms/step - loss: 7529.7739 - mae: 7529.7739
Epoch 40/100
34/34 [==============================] - 0s 2ms/step - loss: 7467.3232 - mae: 7467.3232
Epoch 41/100
34/34 [==============================] - 0s 2ms/step - loss: 7635.9282 - mae: 7635.9282
Epoch 42/100
34/34 [==============================] - 0s 2ms/step - loss: 7536.8398 - mae: 7536.8398
Epoch 43/100
34/34 [==============================] - 0s 1ms/step - loss: 7616.5845 - mae: 7616.5845
Epoch 44/100
34/34 [==============================] - 0s 1ms/step - loss: 7439.4932 - mae: 7439.4932
Epoch 45/100
34/34 [==============================] - 0s 2ms/step - loss: 7538.0156 - mae: 7538.0156
Epoch 46/100
34/34 [==============================] - 0s 2ms/step - loss: 7415.1460 - mae: 7415.1460
Epoch 47/100
34/34 [==============================] - 0s 2ms/step - loss: 7420.6938 - mae: 7420.6938
Epoch 48/100
34/34 [==============================] - 0s 2ms/step - loss: 7509.9829 - mae: 7509.9829
Epoch 49/100
34/34 [==============================] - 0s 1ms/step - loss: 7541.1123 - mae: 7541.1123
Epoch 50/100
34/34 [==============================] - 0s 1ms/step - loss: 7467.8633 - mae: 7467.8633
Epoch 51/100
34/34 [==============================] - 0s 1ms/step - loss: 7389.3545 - mae: 7389.3545
Epoch 52/100
34/34 [==============================] - 0s 2ms/step - loss: 7499.7749 - mae: 7499.7749
Epoch 53/100
34/34 [==============================] - 0s 2ms/step - loss: 7523.9282 - mae: 7523.9282
Epoch 54/100
34/34 [==============================] - 0s 1ms/step - loss: 7243.3120 - mae: 7243.3120
Epoch 55/100
34/34 [==============================] - 0s 1ms/step - loss: 7429.5854 - mae: 7429.5854
Epoch 56/100
34/34 [==============================] - 0s 2ms/step - loss: 7313.4004 - mae: 7313.4004
Epoch 57/100
34/34 [==============================] - 0s 2ms/step - loss: 7526.3887 - mae: 7526.3887
Epoch 58/100
34/34 [==============================] - 0s 2ms/step - loss: 7542.2661 - mae: 7542.2661
Epoch 59/100
34/34 [==============================] - 0s 2ms/step - loss: 7576.9277 - mae: 7576.9277
Epoch 60/100
34/34 [==============================] - 0s 1ms/step - loss: 7546.4058 - mae: 7546.4058
Epoch 61/100
34/34 [==============================] - 0s 1ms/step - loss: 7351.2261 - mae: 7351.2261
Epoch 62/100
34/34 [==============================] - 0s 1ms/step - loss: 7302.1436 - mae: 7302.1436
Epoch 63/100
34/34 [==============================] - 0s 1ms/step - loss: 7393.0874 - mae: 7393.0874
Epoch 64/100
34/34 [==============================] - 0s 2ms/step - loss: 7442.2886 - mae: 7442.2886
Epoch 65/100
34/34 [==============================] - 0s 2ms/step - loss: 7492.6782 - mae: 7492.6782
Epoch 66/100
34/34 [==============================] - 0s 2ms/step - loss: 7561.9165 - mae: 7561.9165
Epoch 67/100
34/34 [==============================] - 0s 2ms/step - loss: 7340.5137 - mae: 7340.5137
Epoch 68/100
34/34 [==============================] - 0s 2ms/step - loss: 7496.0850 - mae: 7496.0850
Epoch 69/100
34/34 [==============================] - 0s 1ms/step - loss: 7617.0298 - mae: 7617.0298
Epoch 70/100
34/34 [==============================] - 0s 2ms/step - loss: 7641.1958 - mae: 7641.1958
Epoch 71/100
34/34 [==============================] - 0s 2ms/step - loss: 7084.2744 - mae: 7084.2744
Epoch 72/100
34/34 [==============================] - 0s 2ms/step - loss: 7240.4907 - mae: 7240.4907
Epoch 73/100
34/34 [==============================] - 0s 2ms/step - loss: 7283.4883 - mae: 7283.4883
Epoch 74/100
34/34 [==============================] - 0s 2ms/step - loss: 7335.5063 - mae: 7335.5063
Epoch 75/100
34/34 [==============================] - 0s 1ms/step - loss: 7275.6392 - mae: 7275.6392
Epoch 76/100
34/34 [==============================] - 0s 1ms/step - loss: 7313.1855 - mae: 7313.1855
Epoch 77/100
34/34 [==============================] - 0s 2ms/step - loss: 7485.7578 - mae: 7485.7578
Epoch 78/100
34/34 [==============================] - 0s 1ms/step - loss: 7352.2798 - mae: 7352.2798
Epoch 79/100
34/34 [==============================] - 0s 2ms/step - loss: 7520.5703 - mae: 7520.5703
Epoch 80/100
34/34 [==============================] - 0s 1ms/step - loss: 7279.3784 - mae: 7279.3784
Epoch 81/100
34/34 [==============================] - 0s 2ms/step - loss: 7273.8477 - mae: 7273.8477
Epoch 82/100
34/34 [==============================] - 0s 2ms/step - loss: 7176.5205 - mae: 7176.5205
Epoch 83/100
34/34 [==============================] - 0s 1ms/step - loss: 7425.6289 - mae: 7425.6289
Epoch 84/100
34/34 [==============================] - 0s 2ms/step - loss: 7403.1289 - mae: 7403.1289
Epoch 85/100
34/34 [==============================] - 0s 1ms/step - loss: 7356.0093 - mae: 7356.0093
Epoch 86/100
34/34 [==============================] - 0s 1ms/step - loss: 7484.7271 - mae: 7484.7271
Epoch 87/100
34/34 [==============================] - 0s 2ms/step - loss: 7217.6079 - mae: 7217.6079
Epoch 88/100
34/34 [==============================] - 0s 2ms/step - loss: 7261.0000 - mae: 7261.0000
Epoch 89/100
34/34 [==============================] - 0s 2ms/step - loss: 7134.1553 - mae: 7134.1553
Epoch 90/100
34/34 [==============================] - 0s 1ms/step - loss: 7083.4351 - mae: 7083.4351
Epoch 91/100
34/34 [==============================] - 0s 1ms/step - loss: 7254.1782 - mae: 7254.1782
Epoch 92/100
34/34 [==============================] - 0s 1ms/step - loss: 7268.7456 - mae: 7268.7456
Epoch 93/100
34/34 [==============================] - 0s 1ms/step - loss: 7470.5225 - mae: 7470.5225
Epoch 94/100
34/34 [==============================] - 0s 2ms/step - loss: 7210.9541 - mae: 7210.9541
Epoch 95/100
34/34 [==============================] - 0s 2ms/step - loss: 7395.6807 - mae: 7395.6807
Epoch 96/100
34/34 [==============================] - 0s 2ms/step - loss: 7328.0884 - mae: 7328.0884
Epoch 97/100
34/34 [==============================] - 0s 2ms/step - loss: 7230.4375 - mae: 7230.4375
Epoch 98/100
34/34 [==============================] - 0s 2ms/step - loss: 7261.3936 - mae: 7261.3936
Epoch 99/100
34/34 [==============================] - 0s 2ms/step - loss: 7342.5684 - mae: 7342.5684
Epoch 100/100
34/34 [==============================] - 0s 1ms/step - loss: 7106.1709 - mae: 7106.1709
<keras.callbacks.History at 0x7f07fde65eb0>
[72]
0s
#evaluate the model on the test data set
insurance_model.evaluate(X_test,Y_test)
9/9 [==============================] - 0s 3ms/step - loss: 7023.3286 - mae: 7023.3286
[7023.32861328125, 7023.32861328125]
[73]
0s
Y_train.median(),Y_train.mean()
(9575.4421, 13346.089736364485)
same olf stuff let us improve our model
we will try to improve using two expriments:

Add an extra layers with more hidden layers and use adam optimizer
Train for longer
absolute random here
[74]
10s
#let's us start we experiment one
tf.random.set_seed(42)

#1. build the model
insurance_model_2=tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

#2. compile the model
insurance_model_2.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=['mae'])

#fit our model
insurance_model_2.fit(X_train,Y_train,epochs=100,verbose=1)
Epoch 1/100
34/34 [==============================] - 1s 2ms/step - loss: 13273.1602 - mae: 13273.1602
Epoch 2/100
34/34 [==============================] - 0s 2ms/step - loss: 13104.4297 - mae: 13104.4297
Epoch 3/100
34/34 [==============================] - 0s 2ms/step - loss: 12749.5420 - mae: 12749.5420
Epoch 4/100
34/34 [==============================] - 0s 2ms/step - loss: 12055.7510 - mae: 12055.7510
Epoch 5/100
34/34 [==============================] - 0s 2ms/step - loss: 10905.8154 - mae: 10905.8154
Epoch 6/100
34/34 [==============================] - 0s 2ms/step - loss: 9457.7217 - mae: 9457.7217
Epoch 7/100
34/34 [==============================] - 0s 2ms/step - loss: 8147.6543 - mae: 8147.6543
Epoch 8/100
34/34 [==============================] - 0s 2ms/step - loss: 7528.8408 - mae: 7528.8408
Epoch 9/100
34/34 [==============================] - 0s 3ms/step - loss: 7429.1528 - mae: 7429.1528
Epoch 10/100
34/34 [==============================] - 0s 2ms/step - loss: 7409.0811 - mae: 7409.0811
Epoch 11/100
34/34 [==============================] - 0s 2ms/step - loss: 7390.8042 - mae: 7390.8042
Epoch 12/100
34/34 [==============================] - 0s 2ms/step - loss: 7368.9180 - mae: 7368.9180
Epoch 13/100
34/34 [==============================] - 0s 2ms/step - loss: 7348.5190 - mae: 7348.5190
Epoch 14/100
34/34 [==============================] - 0s 2ms/step - loss: 7326.4893 - mae: 7326.4893
Epoch 15/100
34/34 [==============================] - 0s 2ms/step - loss: 7307.5815 - mae: 7307.5815
Epoch 16/100
34/34 [==============================] - 0s 2ms/step - loss: 7285.7734 - mae: 7285.7734
Epoch 17/100
34/34 [==============================] - 0s 2ms/step - loss: 7265.7104 - mae: 7265.7104
Epoch 18/100
34/34 [==============================] - 0s 2ms/step - loss: 7242.5488 - mae: 7242.5488
Epoch 19/100
34/34 [==============================] - 0s 2ms/step - loss: 7220.5068 - mae: 7220.5068
Epoch 20/100
34/34 [==============================] - 0s 2ms/step - loss: 7197.1978 - mae: 7197.1978
Epoch 21/100
34/34 [==============================] - 0s 3ms/step - loss: 7179.0195 - mae: 7179.0195
Epoch 22/100
34/34 [==============================] - 0s 2ms/step - loss: 7151.2104 - mae: 7151.2104
Epoch 23/100
34/34 [==============================] - 0s 2ms/step - loss: 7126.4639 - mae: 7126.4639
Epoch 24/100
34/34 [==============================] - 0s 2ms/step - loss: 7101.9199 - mae: 7101.9199
Epoch 25/100
34/34 [==============================] - 0s 2ms/step - loss: 7084.3379 - mae: 7084.3379
Epoch 26/100
34/34 [==============================] - 0s 2ms/step - loss: 7052.3296 - mae: 7052.3296
Epoch 27/100
34/34 [==============================] - 0s 2ms/step - loss: 7024.3506 - mae: 7024.3506
Epoch 28/100
34/34 [==============================] - 0s 2ms/step - loss: 6996.6963 - mae: 6996.6963
Epoch 29/100
34/34 [==============================] - 0s 2ms/step - loss: 6969.0112 - mae: 6969.0112
Epoch 30/100
34/34 [==============================] - 0s 2ms/step - loss: 6942.1899 - mae: 6942.1899
Epoch 31/100
34/34 [==============================] - 0s 2ms/step - loss: 6911.7275 - mae: 6911.7275
Epoch 32/100
34/34 [==============================] - 0s 2ms/step - loss: 6884.0205 - mae: 6884.0205
Epoch 33/100
34/34 [==============================] - 0s 3ms/step - loss: 6853.4648 - mae: 6853.4648
Epoch 34/100
34/34 [==============================] - 0s 2ms/step - loss: 6823.0674 - mae: 6823.0674
Epoch 35/100
34/34 [==============================] - 0s 2ms/step - loss: 6789.6855 - mae: 6789.6855
Epoch 36/100
34/34 [==============================] - 0s 2ms/step - loss: 6755.7646 - mae: 6755.7646
Epoch 37/100
34/34 [==============================] - 0s 2ms/step - loss: 6720.2026 - mae: 6720.2026
Epoch 38/100
34/34 [==============================] - 0s 2ms/step - loss: 6689.7158 - mae: 6689.7158
Epoch 39/100
34/34 [==============================] - 0s 2ms/step - loss: 6652.4614 - mae: 6652.4614
Epoch 40/100
34/34 [==============================] - 0s 2ms/step - loss: 6618.1016 - mae: 6618.1016
Epoch 41/100
34/34 [==============================] - 0s 2ms/step - loss: 6585.8643 - mae: 6585.8643
Epoch 42/100
34/34 [==============================] - 0s 2ms/step - loss: 6559.4956 - mae: 6559.4956
Epoch 43/100
34/34 [==============================] - 0s 2ms/step - loss: 6530.0439 - mae: 6530.0439
Epoch 44/100
34/34 [==============================] - 0s 2ms/step - loss: 6506.8071 - mae: 6506.8071
Epoch 45/100
34/34 [==============================] - 0s 2ms/step - loss: 6493.5718 - mae: 6493.5718
Epoch 46/100
34/34 [==============================] - 0s 2ms/step - loss: 6475.9258 - mae: 6475.9258
Epoch 47/100
34/34 [==============================] - 0s 2ms/step - loss: 6458.8984 - mae: 6458.8984
Epoch 48/100
34/34 [==============================] - 0s 3ms/step - loss: 6445.1494 - mae: 6445.1494
Epoch 49/100
34/34 [==============================] - 0s 2ms/step - loss: 6430.9639 - mae: 6430.9639
Epoch 50/100
34/34 [==============================] - 0s 2ms/step - loss: 6417.7510 - mae: 6417.7510
Epoch 51/100
34/34 [==============================] - 0s 2ms/step - loss: 6403.2754 - mae: 6403.2754
Epoch 52/100
34/34 [==============================] - 0s 2ms/step - loss: 6392.4141 - mae: 6392.4141
Epoch 53/100
34/34 [==============================] - 0s 2ms/step - loss: 6378.7451 - mae: 6378.7451
Epoch 54/100
34/34 [==============================] - 0s 2ms/step - loss: 6364.9126 - mae: 6364.9126
Epoch 55/100
34/34 [==============================] - 0s 2ms/step - loss: 6351.5269 - mae: 6351.5269
Epoch 56/100
34/34 [==============================] - 0s 2ms/step - loss: 6337.6606 - mae: 6337.6606
Epoch 57/100
34/34 [==============================] - 0s 2ms/step - loss: 6324.8369 - mae: 6324.8369
Epoch 58/100
34/34 [==============================] - 0s 2ms/step - loss: 6310.1948 - mae: 6310.1948
Epoch 59/100
34/34 [==============================] - 0s 2ms/step - loss: 6295.6035 - mae: 6295.6035
Epoch 60/100
34/34 [==============================] - 0s 2ms/step - loss: 6284.8696 - mae: 6284.8696
Epoch 61/100
34/34 [==============================] - 0s 2ms/step - loss: 6265.6411 - mae: 6265.6411
Epoch 62/100
34/34 [==============================] - 0s 2ms/step - loss: 6253.0107 - mae: 6253.0107
Epoch 63/100
34/34 [==============================] - 0s 2ms/step - loss: 6234.9292 - mae: 6234.9292
Epoch 64/100
34/34 [==============================] - 0s 2ms/step - loss: 6218.0430 - mae: 6218.0430
Epoch 65/100
34/34 [==============================] - 0s 2ms/step - loss: 6201.1899 - mae: 6201.1899
Epoch 66/100
34/34 [==============================] - 0s 2ms/step - loss: 6183.9590 - mae: 6183.9590
Epoch 67/100
34/34 [==============================] - 0s 2ms/step - loss: 6171.2993 - mae: 6171.2993
Epoch 68/100
34/34 [==============================] - 0s 2ms/step - loss: 6148.8398 - mae: 6148.8398
Epoch 69/100
34/34 [==============================] - 0s 2ms/step - loss: 6132.5981 - mae: 6132.5981
Epoch 70/100
34/34 [==============================] - 0s 3ms/step - loss: 6112.3848 - mae: 6112.3848
Epoch 71/100
34/34 [==============================] - 0s 2ms/step - loss: 6092.7202 - mae: 6092.7202
Epoch 72/100
34/34 [==============================] - 0s 2ms/step - loss: 6073.7422 - mae: 6073.7422
Epoch 73/100
34/34 [==============================] - 0s 2ms/step - loss: 6059.4883 - mae: 6059.4883
Epoch 74/100
34/34 [==============================] - 0s 2ms/step - loss: 6031.3843 - mae: 6031.3843
Epoch 75/100
34/34 [==============================] - 0s 2ms/step - loss: 6010.3350 - mae: 6010.3350
Epoch 76/100
34/34 [==============================] - 0s 2ms/step - loss: 5995.2168 - mae: 5995.2168
Epoch 77/100
34/34 [==============================] - 0s 2ms/step - loss: 5963.0723 - mae: 5963.0723
Epoch 78/100
34/34 [==============================] - 0s 2ms/step - loss: 5940.0610 - mae: 5940.0610
Epoch 79/100
34/34 [==============================] - 0s 3ms/step - loss: 5915.1064 - mae: 5915.1064
Epoch 80/100
34/34 [==============================] - 0s 2ms/step - loss: 5887.9990 - mae: 5887.9990
Epoch 81/100
34/34 [==============================] - 0s 2ms/step - loss: 5861.6992 - mae: 5861.6992
Epoch 82/100
34/34 [==============================] - 0s 2ms/step - loss: 5834.3071 - mae: 5834.3071
Epoch 83/100
34/34 [==============================] - 0s 2ms/step - loss: 5805.8242 - mae: 5805.8242
Epoch 84/100
34/34 [==============================] - 0s 2ms/step - loss: 5772.3232 - mae: 5772.3232
Epoch 85/100
34/34 [==============================] - 0s 2ms/step - loss: 5745.1514 - mae: 5745.1514
Epoch 86/100
34/34 [==============================] - 0s 2ms/step - loss: 5711.3477 - mae: 5711.3477
Epoch 87/100
34/34 [==============================] - 0s 2ms/step - loss: 5674.5215 - mae: 5674.5215
Epoch 88/100
34/34 [==============================] - 0s 2ms/step - loss: 5639.4927 - mae: 5639.4927
Epoch 89/100
34/34 [==============================] - 0s 2ms/step - loss: 5600.6655 - mae: 5600.6655
Epoch 90/100
34/34 [==============================] - 0s 2ms/step - loss: 5559.4326 - mae: 5559.4326
Epoch 91/100
34/34 [==============================] - 0s 2ms/step - loss: 5523.6187 - mae: 5523.6187
Epoch 92/100
34/34 [==============================] - 0s 2ms/step - loss: 5474.1250 - mae: 5474.1250
Epoch 93/100
34/34 [==============================] - 0s 2ms/step - loss: 5432.2656 - mae: 5432.2656
Epoch 94/100
34/34 [==============================] - 0s 2ms/step - loss: 5386.0527 - mae: 5386.0527
Epoch 95/100
34/34 [==============================] - 0s 2ms/step - loss: 5333.1812 - mae: 5333.1812
Epoch 96/100
34/34 [==============================] - 0s 2ms/step - loss: 5288.8159 - mae: 5288.8159
Epoch 97/100
34/34 [==============================] - 0s 2ms/step - loss: 5234.6787 - mae: 5234.6787
Epoch 98/100
34/34 [==============================] - 0s 2ms/step - loss: 5170.9360 - mae: 5170.9360
Epoch 99/100
34/34 [==============================] - 0s 2ms/step - loss: 5112.9443 - mae: 5112.9443
Epoch 100/100
34/34 [==============================] - 0s 2ms/step - loss: 5059.8447 - mae: 5059.8447
<keras.callbacks.History at 0x7f07f6389e80>
[75]
0s
#evaluate our model
insurance_model_2.evaluate(X_test,Y_test)
9/9 [==============================] - 0s 2ms/step - loss: 4924.4956 - mae: 4924.4956
[4924.49560546875, 4924.49560546875]
[76]
20s
#let us do our third expriment 
tf.random.set_seed(42)

#create our model
insurance_model_3=tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

#compile our model
insurance_model_3.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=['mae'])

#fit our model
history=insurance_model_3.fit(X,Y,epochs=200)
Epoch 1/200
42/42 [==============================] - 0s 2ms/step - loss: 13180.3643 - mae: 13180.3643
Epoch 2/200
42/42 [==============================] - 0s 2ms/step - loss: 12925.4170 - mae: 12925.4170
Epoch 3/200
42/42 [==============================] - 0s 2ms/step - loss: 12303.5654 - mae: 12303.5654
Epoch 4/200
42/42 [==============================] - 0s 2ms/step - loss: 11051.4922 - mae: 11051.4922
Epoch 5/200
42/42 [==============================] - 0s 2ms/step - loss: 9317.2578 - mae: 9317.2578
Epoch 6/200
42/42 [==============================] - 0s 2ms/step - loss: 7928.2144 - mae: 7928.2144
Epoch 7/200
42/42 [==============================] - 0s 3ms/step - loss: 7496.8086 - mae: 7496.8086
Epoch 8/200
42/42 [==============================] - 0s 2ms/step - loss: 7438.1426 - mae: 7438.1426
Epoch 9/200
42/42 [==============================] - 0s 2ms/step - loss: 7410.6606 - mae: 7410.6606
Epoch 10/200
42/42 [==============================] - 0s 2ms/step - loss: 7382.4917 - mae: 7382.4917
Epoch 11/200
42/42 [==============================] - 0s 2ms/step - loss: 7354.2378 - mae: 7354.2378
Epoch 12/200
42/42 [==============================] - 0s 3ms/step - loss: 7325.6606 - mae: 7325.6606
Epoch 13/200
42/42 [==============================] - 0s 2ms/step - loss: 7295.9453 - mae: 7295.9453
Epoch 14/200
42/42 [==============================] - 0s 2ms/step - loss: 7263.8872 - mae: 7263.8872
Epoch 15/200
42/42 [==============================] - 0s 2ms/step - loss: 7233.5337 - mae: 7233.5337
Epoch 16/200
42/42 [==============================] - 0s 2ms/step - loss: 7201.8594 - mae: 7201.8594
Epoch 17/200
42/42 [==============================] - 0s 2ms/step - loss: 7169.2764 - mae: 7169.2764
Epoch 18/200
42/42 [==============================] - 0s 2ms/step - loss: 7135.8242 - mae: 7135.8242
Epoch 19/200
42/42 [==============================] - 0s 2ms/step - loss: 7101.6885 - mae: 7101.6885
Epoch 20/200
42/42 [==============================] - 0s 2ms/step - loss: 7067.6440 - mae: 7067.6440
Epoch 21/200
42/42 [==============================] - 0s 2ms/step - loss: 7027.5845 - mae: 7027.5845
Epoch 22/200
42/42 [==============================] - 0s 2ms/step - loss: 6989.1001 - mae: 6989.1001
Epoch 23/200
42/42 [==============================] - 0s 2ms/step - loss: 6948.4751 - mae: 6948.4751
Epoch 24/200
42/42 [==============================] - 0s 2ms/step - loss: 6910.8071 - mae: 6910.8071
Epoch 25/200
42/42 [==============================] - 0s 2ms/step - loss: 6869.9351 - mae: 6869.9351
Epoch 26/200
42/42 [==============================] - 0s 2ms/step - loss: 6823.1748 - mae: 6823.1748
Epoch 27/200
42/42 [==============================] - 0s 2ms/step - loss: 6776.2617 - mae: 6776.2617
Epoch 28/200
42/42 [==============================] - 0s 2ms/step - loss: 6728.5073 - mae: 6728.5073
Epoch 29/200
42/42 [==============================] - 0s 2ms/step - loss: 6681.6973 - mae: 6681.6973
Epoch 30/200
42/42 [==============================] - 0s 2ms/step - loss: 6635.8901 - mae: 6635.8901
Epoch 31/200
42/42 [==============================] - 0s 2ms/step - loss: 6588.8716 - mae: 6588.8716
Epoch 32/200
42/42 [==============================] - 0s 2ms/step - loss: 6548.8032 - mae: 6548.8032
Epoch 33/200
42/42 [==============================] - 0s 3ms/step - loss: 6513.4277 - mae: 6513.4277
Epoch 34/200
42/42 [==============================] - 0s 2ms/step - loss: 6490.2520 - mae: 6490.2520
Epoch 35/200
42/42 [==============================] - 0s 2ms/step - loss: 6468.8594 - mae: 6468.8594
Epoch 36/200
42/42 [==============================] - 0s 2ms/step - loss: 6447.5464 - mae: 6447.5464
Epoch 37/200
42/42 [==============================] - 0s 2ms/step - loss: 6432.1646 - mae: 6432.1646
Epoch 38/200
42/42 [==============================] - 0s 2ms/step - loss: 6414.8945 - mae: 6414.8945
Epoch 39/200
42/42 [==============================] - 0s 2ms/step - loss: 6400.2856 - mae: 6400.2856
Epoch 40/200
42/42 [==============================] - 0s 2ms/step - loss: 6380.9243 - mae: 6380.9243
Epoch 41/200
42/42 [==============================] - 0s 2ms/step - loss: 6368.9609 - mae: 6368.9609
Epoch 42/200
42/42 [==============================] - 0s 2ms/step - loss: 6349.7705 - mae: 6349.7705
Epoch 43/200
42/42 [==============================] - 0s 2ms/step - loss: 6333.6299 - mae: 6333.6299
Epoch 44/200
42/42 [==============================] - 0s 2ms/step - loss: 6312.6821 - mae: 6312.6821
Epoch 45/200
42/42 [==============================] - 0s 2ms/step - loss: 6317.8877 - mae: 6317.8877
Epoch 46/200
42/42 [==============================] - 0s 2ms/step - loss: 6286.5732 - mae: 6286.5732
Epoch 47/200
42/42 [==============================] - 0s 2ms/step - loss: 6268.6108 - mae: 6268.6108
Epoch 48/200
42/42 [==============================] - 0s 2ms/step - loss: 6245.0552 - mae: 6245.0552
Epoch 49/200
42/42 [==============================] - 0s 2ms/step - loss: 6228.5742 - mae: 6228.5742
Epoch 50/200
42/42 [==============================] - 0s 2ms/step - loss: 6210.4644 - mae: 6210.4644
Epoch 51/200
42/42 [==============================] - 0s 2ms/step - loss: 6190.7412 - mae: 6190.7412
Epoch 52/200
42/42 [==============================] - 0s 2ms/step - loss: 6173.9165 - mae: 6173.9165
Epoch 53/200
42/42 [==============================] - 0s 2ms/step - loss: 6151.5312 - mae: 6151.5312
Epoch 54/200
42/42 [==============================] - 0s 2ms/step - loss: 6129.1885 - mae: 6129.1885
Epoch 55/200
42/42 [==============================] - 0s 2ms/step - loss: 6109.8638 - mae: 6109.8638
Epoch 56/200
42/42 [==============================] - 0s 2ms/step - loss: 6086.5264 - mae: 6086.5264
Epoch 57/200
42/42 [==============================] - 0s 2ms/step - loss: 6063.3433 - mae: 6063.3433
Epoch 58/200
42/42 [==============================] - 0s 2ms/step - loss: 6041.5244 - mae: 6041.5244
Epoch 59/200
42/42 [==============================] - 0s 2ms/step - loss: 6014.2495 - mae: 6014.2495
Epoch 60/200
42/42 [==============================] - 0s 2ms/step - loss: 5996.0981 - mae: 5996.0981
Epoch 61/200
42/42 [==============================] - 0s 2ms/step - loss: 5963.0107 - mae: 5963.0107
Epoch 62/200
42/42 [==============================] - 0s 2ms/step - loss: 5937.3623 - mae: 5937.3623
Epoch 63/200
42/42 [==============================] - 0s 2ms/step - loss: 5906.0273 - mae: 5906.0273
Epoch 64/200
42/42 [==============================] - 0s 3ms/step - loss: 5878.1362 - mae: 5878.1362
Epoch 65/200
42/42 [==============================] - 0s 2ms/step - loss: 5844.7588 - mae: 5844.7588
Epoch 66/200
42/42 [==============================] - 0s 2ms/step - loss: 5811.8848 - mae: 5811.8848
Epoch 67/200
42/42 [==============================] - 0s 2ms/step - loss: 5775.6460 - mae: 5775.6460
Epoch 68/200
42/42 [==============================] - 0s 2ms/step - loss: 5739.3223 - mae: 5739.3223
Epoch 69/200
42/42 [==============================] - 0s 2ms/step - loss: 5704.1582 - mae: 5704.1582
Epoch 70/200
42/42 [==============================] - 0s 2ms/step - loss: 5660.9473 - mae: 5660.9473
Epoch 71/200
42/42 [==============================] - 0s 2ms/step - loss: 5618.5161 - mae: 5618.5161
Epoch 72/200
42/42 [==============================] - 0s 2ms/step - loss: 5577.0342 - mae: 5577.0342
Epoch 73/200
42/42 [==============================] - 0s 2ms/step - loss: 5532.0566 - mae: 5532.0566
Epoch 74/200
42/42 [==============================] - 0s 2ms/step - loss: 5478.0981 - mae: 5478.0981
Epoch 75/200
42/42 [==============================] - 0s 2ms/step - loss: 5421.3525 - mae: 5421.3525
Epoch 76/200
42/42 [==============================] - 0s 2ms/step - loss: 5368.3711 - mae: 5368.3711
Epoch 77/200
42/42 [==============================] - 0s 2ms/step - loss: 5308.4380 - mae: 5308.4380
Epoch 78/200
42/42 [==============================] - 0s 2ms/step - loss: 5260.5190 - mae: 5260.5190
Epoch 79/200
42/42 [==============================] - 0s 2ms/step - loss: 5172.9062 - mae: 5172.9062
Epoch 80/200
42/42 [==============================] - 0s 2ms/step - loss: 5104.4053 - mae: 5104.4053
Epoch 81/200
42/42 [==============================] - 0s 2ms/step - loss: 5034.5054 - mae: 5034.5054
Epoch 82/200
42/42 [==============================] - 0s 2ms/step - loss: 4950.7046 - mae: 4950.7046
Epoch 83/200
42/42 [==============================] - 0s 2ms/step - loss: 4863.7217 - mae: 4863.7217
Epoch 84/200
42/42 [==============================] - 0s 2ms/step - loss: 4771.0479 - mae: 4771.0483
Epoch 85/200
42/42 [==============================] - 0s 2ms/step - loss: 4680.4263 - mae: 4680.4263
Epoch 86/200
42/42 [==============================] - 0s 2ms/step - loss: 4574.4370 - mae: 4574.4370
Epoch 87/200
42/42 [==============================] - 0s 2ms/step - loss: 4480.1313 - mae: 4480.1313
Epoch 88/200
42/42 [==============================] - 0s 2ms/step - loss: 4372.9824 - mae: 4372.9824
Epoch 89/200
42/42 [==============================] - 0s 2ms/step - loss: 4256.1157 - mae: 4256.1157
Epoch 90/200
42/42 [==============================] - 0s 2ms/step - loss: 4152.1479 - mae: 4152.1479
Epoch 91/200
42/42 [==============================] - 0s 2ms/step - loss: 4053.1477 - mae: 4053.1477
Epoch 92/200
42/42 [==============================] - 0s 2ms/step - loss: 3979.3787 - mae: 3979.3787
Epoch 93/200
42/42 [==============================] - 0s 2ms/step - loss: 3926.3599 - mae: 3926.3599
Epoch 94/200
42/42 [==============================] - 0s 2ms/step - loss: 3877.6128 - mae: 3877.6128
Epoch 95/200
42/42 [==============================] - 0s 2ms/step - loss: 3848.6831 - mae: 3848.6831
Epoch 96/200
42/42 [==============================] - 0s 2ms/step - loss: 3821.4866 - mae: 3821.4866
Epoch 97/200
42/42 [==============================] - 0s 2ms/step - loss: 3806.8044 - mae: 3806.8044
Epoch 98/200
42/42 [==============================] - 0s 2ms/step - loss: 3790.1189 - mae: 3790.1189
Epoch 99/200
42/42 [==============================] - 0s 2ms/step - loss: 3780.4458 - mae: 3780.4458
Epoch 100/200
42/42 [==============================] - 0s 2ms/step - loss: 3769.1670 - mae: 3769.1670
Epoch 101/200
42/42 [==============================] - 0s 2ms/step - loss: 3762.4167 - mae: 3762.4167
Epoch 102/200
42/42 [==============================] - 0s 2ms/step - loss: 3763.2781 - mae: 3763.2781
Epoch 103/200
42/42 [==============================] - 0s 2ms/step - loss: 3755.9783 - mae: 3755.9783
Epoch 104/200
42/42 [==============================] - 0s 2ms/step - loss: 3754.0449 - mae: 3754.0449
Epoch 105/200
42/42 [==============================] - 0s 2ms/step - loss: 3747.0254 - mae: 3747.0254
Epoch 106/200
42/42 [==============================] - 0s 2ms/step - loss: 3742.6719 - mae: 3742.6719
Epoch 107/200
42/42 [==============================] - 0s 2ms/step - loss: 3736.1716 - mae: 3736.1716
Epoch 108/200
42/42 [==============================] - 0s 2ms/step - loss: 3737.0408 - mae: 3737.0408
Epoch 109/200
42/42 [==============================] - 0s 2ms/step - loss: 3740.7998 - mae: 3740.7998
Epoch 110/200
42/42 [==============================] - 0s 2ms/step - loss: 3729.1831 - mae: 3729.1831
Epoch 111/200
42/42 [==============================] - 0s 2ms/step - loss: 3736.3113 - mae: 3736.3113
Epoch 112/200
42/42 [==============================] - 0s 2ms/step - loss: 3731.2803 - mae: 3731.2803
Epoch 113/200
42/42 [==============================] - 0s 2ms/step - loss: 3724.1174 - mae: 3724.1174
Epoch 114/200
42/42 [==============================] - 0s 2ms/step - loss: 3726.1643 - mae: 3726.1643
Epoch 115/200
42/42 [==============================] - 0s 2ms/step - loss: 3718.6660 - mae: 3718.6660
Epoch 116/200
42/42 [==============================] - 0s 3ms/step - loss: 3716.3823 - mae: 3716.3823
Epoch 117/200
42/42 [==============================] - 0s 2ms/step - loss: 3715.5593 - mae: 3715.5593
Epoch 118/200
42/42 [==============================] - 0s 2ms/step - loss: 3717.7556 - mae: 3717.7556
Epoch 119/200
42/42 [==============================] - 0s 2ms/step - loss: 3723.4050 - mae: 3723.4050
Epoch 120/200
42/42 [==============================] - 0s 2ms/step - loss: 3713.6794 - mae: 3713.6794
Epoch 121/200
42/42 [==============================] - 0s 2ms/step - loss: 3706.2029 - mae: 3706.2029
Epoch 122/200
42/42 [==============================] - 0s 2ms/step - loss: 3702.5371 - mae: 3702.5371
Epoch 123/200
42/42 [==============================] - 0s 2ms/step - loss: 3699.3625 - mae: 3699.3625
Epoch 124/200
42/42 [==============================] - 0s 2ms/step - loss: 3695.6436 - mae: 3695.6436
Epoch 125/200
42/42 [==============================] - 0s 2ms/step - loss: 3699.0283 - mae: 3699.0283
Epoch 126/200
42/42 [==============================] - 0s 2ms/step - loss: 3701.2852 - mae: 3701.2852
Epoch 127/200
42/42 [==============================] - 0s 2ms/step - loss: 3690.8289 - mae: 3690.8289
Epoch 128/200
42/42 [==============================] - 0s 2ms/step - loss: 3690.9233 - mae: 3690.9233
Epoch 129/200
42/42 [==============================] - 0s 2ms/step - loss: 3686.4141 - mae: 3686.4141
Epoch 130/200
42/42 [==============================] - 0s 2ms/step - loss: 3687.7227 - mae: 3687.7227
Epoch 131/200
42/42 [==============================] - 0s 2ms/step - loss: 3678.2175 - mae: 3678.2175
Epoch 132/200
42/42 [==============================] - 0s 2ms/step - loss: 3680.3628 - mae: 3680.3628
Epoch 133/200
42/42 [==============================] - 0s 2ms/step - loss: 3675.9211 - mae: 3675.9211
Epoch 134/200
42/42 [==============================] - 0s 2ms/step - loss: 3674.8108 - mae: 3674.8108
Epoch 135/200
42/42 [==============================] - 0s 2ms/step - loss: 3678.6277 - mae: 3678.6277
Epoch 136/200
42/42 [==============================] - 0s 2ms/step - loss: 3675.7368 - mae: 3675.7368
Epoch 137/200
42/42 [==============================] - 0s 2ms/step - loss: 3669.1267 - mae: 3669.1267
Epoch 138/200
42/42 [==============================] - 0s 2ms/step - loss: 3664.4832 - mae: 3664.4832
Epoch 139/200
42/42 [==============================] - 0s 2ms/step - loss: 3665.1985 - mae: 3665.1985
Epoch 140/200
42/42 [==============================] - 0s 2ms/step - loss: 3661.1169 - mae: 3661.1169
Epoch 141/200
42/42 [==============================] - 0s 2ms/step - loss: 3658.1345 - mae: 3658.1345
Epoch 142/200
42/42 [==============================] - 0s 2ms/step - loss: 3660.9189 - mae: 3660.9189
Epoch 143/200
42/42 [==============================] - 0s 2ms/step - loss: 3659.6414 - mae: 3659.6414
Epoch 144/200
42/42 [==============================] - 0s 2ms/step - loss: 3655.4253 - mae: 3655.4253
Epoch 145/200
42/42 [==============================] - 0s 2ms/step - loss: 3649.3127 - mae: 3649.3127
Epoch 146/200
42/42 [==============================] - 0s 2ms/step - loss: 3648.5818 - mae: 3648.5818
Epoch 147/200
42/42 [==============================] - 0s 2ms/step - loss: 3649.4846 - mae: 3649.4846
Epoch 148/200
42/42 [==============================] - 0s 2ms/step - loss: 3640.1711 - mae: 3640.1711
Epoch 149/200
42/42 [==============================] - 0s 2ms/step - loss: 3638.8315 - mae: 3638.8315
Epoch 150/200
42/42 [==============================] - 0s 2ms/step - loss: 3642.0029 - mae: 3642.0029
Epoch 151/200
42/42 [==============================] - 0s 2ms/step - loss: 3633.6838 - mae: 3633.6838
Epoch 152/200
42/42 [==============================] - 0s 2ms/step - loss: 3629.8562 - mae: 3629.8562
Epoch 153/200
42/42 [==============================] - 0s 2ms/step - loss: 3640.5205 - mae: 3640.5205
Epoch 154/200
42/42 [==============================] - 0s 2ms/step - loss: 3632.1421 - mae: 3632.1421
Epoch 155/200
42/42 [==============================] - 0s 2ms/step - loss: 3629.3960 - mae: 3629.3960
Epoch 156/200
42/42 [==============================] - 0s 2ms/step - loss: 3627.4570 - mae: 3627.4570
Epoch 157/200
42/42 [==============================] - 0s 2ms/step - loss: 3619.3906 - mae: 3619.3906
Epoch 158/200
42/42 [==============================] - 0s 2ms/step - loss: 3615.5142 - mae: 3615.5142
Epoch 159/200
42/42 [==============================] - 0s 2ms/step - loss: 3637.8142 - mae: 3637.8142
Epoch 160/200
42/42 [==============================] - 0s 2ms/step - loss: 3611.0051 - mae: 3611.0051
Epoch 161/200
42/42 [==============================] - 0s 2ms/step - loss: 3617.5146 - mae: 3617.5146
Epoch 162/200
42/42 [==============================] - 0s 2ms/step - loss: 3607.5330 - mae: 3607.5330
Epoch 163/200
42/42 [==============================] - 0s 2ms/step - loss: 3603.2119 - mae: 3603.2119
Epoch 164/200
42/42 [==============================] - 0s 2ms/step - loss: 3601.3704 - mae: 3601.3704
Epoch 165/200
42/42 [==============================] - 0s 2ms/step - loss: 3595.2097 - mae: 3595.2097
Epoch 166/200
42/42 [==============================] - 0s 2ms/step - loss: 3599.6125 - mae: 3599.6125
Epoch 167/200
42/42 [==============================] - 0s 2ms/step - loss: 3605.4526 - mae: 3605.4526
Epoch 168/200
42/42 [==============================] - 0s 2ms/step - loss: 3591.1599 - mae: 3591.1599
Epoch 169/200
42/42 [==============================] - 0s 2ms/step - loss: 3589.5564 - mae: 3589.5564
Epoch 170/200
42/42 [==============================] - 0s 3ms/step - loss: 3589.0515 - mae: 3589.0515
Epoch 171/200
42/42 [==============================] - 0s 2ms/step - loss: 3582.0242 - mae: 3582.0242
Epoch 172/200
42/42 [==============================] - 0s 2ms/step - loss: 3583.9199 - mae: 3583.9199
Epoch 173/200
42/42 [==============================] - 0s 2ms/step - loss: 3578.9656 - mae: 3578.9656
Epoch 174/200
42/42 [==============================] - 0s 2ms/step - loss: 3573.6763 - mae: 3573.6763
Epoch 175/200
42/42 [==============================] - 0s 2ms/step - loss: 3572.6719 - mae: 3572.6719
Epoch 176/200
42/42 [==============================] - 0s 2ms/step - loss: 3569.7473 - mae: 3569.7473
Epoch 177/200
42/42 [==============================] - 0s 2ms/step - loss: 3570.8950 - mae: 3570.8950
Epoch 178/200
42/42 [==============================] - 0s 2ms/step - loss: 3565.0330 - mae: 3565.0330
Epoch 179/200
42/42 [==============================] - 0s 2ms/step - loss: 3567.4114 - mae: 3567.4114
Epoch 180/200
42/42 [==============================] - 0s 2ms/step - loss: 3561.4080 - mae: 3561.4080
Epoch 181/200
42/42 [==============================] - 0s 2ms/step - loss: 3556.6365 - mae: 3556.6365
Epoch 182/200
42/42 [==============================] - 0s 2ms/step - loss: 3556.0127 - mae: 3556.0127
Epoch 183/200
42/42 [==============================] - 0s 2ms/step - loss: 3546.9702 - mae: 3546.9702
Epoch 184/200
42/42 [==============================] - 0s 2ms/step - loss: 3546.5732 - mae: 3546.5732
Epoch 185/200
42/42 [==============================] - 0s 2ms/step - loss: 3540.6494 - mae: 3540.6494
Epoch 186/200
42/42 [==============================] - 0s 2ms/step - loss: 3547.3550 - mae: 3547.3550
Epoch 187/200
42/42 [==============================] - 0s 2ms/step - loss: 3539.3254 - mae: 3539.3254
Epoch 188/200
42/42 [==============================] - 0s 2ms/step - loss: 3529.5374 - mae: 3529.5374
Epoch 189/200
42/42 [==============================] - 0s 2ms/step - loss: 3526.7727 - mae: 3526.7727
Epoch 190/200
42/42 [==============================] - 0s 2ms/step - loss: 3524.6526 - mae: 3524.6526
Epoch 191/200
42/42 [==============================] - 0s 2ms/step - loss: 3530.1191 - mae: 3530.1191
Epoch 192/200
42/42 [==============================] - 0s 2ms/step - loss: 3518.1401 - mae: 3518.1401
Epoch 193/200
42/42 [==============================] - 0s 2ms/step - loss: 3520.2202 - mae: 3520.2202
Epoch 194/200
42/42 [==============================] - 0s 2ms/step - loss: 3514.5881 - mae: 3514.5881
Epoch 195/200
42/42 [==============================] - 0s 2ms/step - loss: 3510.1741 - mae: 3510.1741
Epoch 196/200
42/42 [==============================] - 0s 2ms/step - loss: 3514.7266 - mae: 3514.7266
Epoch 197/200
42/42 [==============================] - 0s 2ms/step - loss: 3509.1660 - mae: 3509.1660
Epoch 198/200
42/42 [==============================] - 0s 2ms/step - loss: 3506.7156 - mae: 3506.7156
Epoch 199/200
42/42 [==============================] - 0s 2ms/step - loss: 3504.1311 - mae: 3504.1311
Epoch 200/200
42/42 [==============================] - 0s 2ms/step - loss: 3497.2690 - mae: 3497.2690
[77]
0s
#evaluate our third model
insurance_model_3.evaluate(X_test,Y_test)
9/9 [==============================] - 0s 2ms/step - loss: 3278.3650 - mae: 3278.3650
[3278.364990234375, 3278.364990234375]
[78]
0s
#plot history (also know as a loss curve or a training curve)
pd.DataFrame(history.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs")

question "how long should I train my model for?"

answer:- it really depends. It really depends on the problem you are working. many people have asked this question a lot. guess what Tensorflow has a solution! it is called EarlyStoppingCallback. which is a tensorflow metrics which stops training when the metrics stops improving after a certain time.

preprocessing data (normalization and standardization)
in terms of scaling values,neural network tend to prefer normalization. If you are not sure which to use try both and compare

[79]
0s
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

#read the insurance dataframe
insurance=pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")
insurance

To prepare our data we can borrow a few classes from scikit-learn.

[99]
0s
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler,OneHotEncoder
from sklearn.model_selection import train_test_split

#create a column transformer
ct=make_column_transformer(
    (MinMaxScaler(),['age','bmi','children']), #turn all values in these column in between 0 and 1
    (OneHotEncoder(handle_unknown="ignore"),["sex","smoker","region"])
)

#create X and Y
X=insurance.drop("charges",axis=1)
Y=insurance["charges"]

#build our train and test states
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)

#Fit the column transformer to our training dataset
ct.fit(X_train)

#transform training and test data with normalization (MinMaxScaler) and OneHotEncoding
X_train_normal=ct.transform(X_train)
X_test_normal=ct.transform(X_test)
[100]
0s
#what does our data look like
X_train.loc[0]
age                19
sex            female
bmi              27.9
children            0
smoker            yes
region      southwest
Name: 0, dtype: object
[101]
0s
X_train_normal[0]
array([0.60869565, 0.10734463, 0.4       , 1.        , 0.        ,
       1.        , 0.        , 0.        , 1.        , 0.        ,
       0.        ])
[102]
8s
#let us finaly build a model for our neural network
tf.random.set_seed(42)

#1. create the model
insurance_model_4=tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])


Epoch 1/100
34/34 [==============================] - 1s 2ms/step - loss: 13342.6494 - mae: 13342.6494
Epoch 2/100
34/34 [==============================] - 0s 2ms/step - loss: 13333.4785 - mae: 13333.4785
Epoch 3/100
34/34 [==============================] - 0s 2ms/step - loss: 13312.0234 - mae: 13312.0234
Epoch 4/100
34/34 [==============================] - 0s 2ms/step - loss: 13267.7930 - mae: 13267.7930
Epoch 5/100
34/34 [==============================] - 0s 2ms/step - loss: 13189.5830 - mae: 13189.5830
Epoch 6/100
34/34 [==============================] - 0s 2ms/step - loss: 13066.4502 - mae: 13066.4502
Epoch 7/100
34/34 [==============================] - 0s 2ms/step - loss: 12888.1953 - mae: 12888.1953
Epoch 8/100
34/34 [==============================] - 0s 2ms/step - loss: 12644.6523 - mae: 12644.6523
Epoch 9/100
34/34 [==============================] - 0s 3ms/step - loss: 12325.5469 - mae: 12325.5469
Epoch 10/100
34/34 [==============================] - 0s 2ms/step - loss: 11925.9658 - mae: 11925.9658
Epoch 11/100
34/34 [==============================] - 0s 2ms/step - loss: 11454.3350 - mae: 11454.3350
Epoch 12/100
34/34 [==============================] - 0s 2ms/step - loss: 10949.8076 - mae: 10949.8076
Epoch 13/100
34/34 [==============================] - 0s 2ms/step - loss: 10448.9404 - mae: 10448.9404
Epoch 14/100
34/34 [==============================] - 0s 2ms/step - loss: 9951.6250 - mae: 9951.6250
Epoch 15/100
34/34 [==============================] - 0s 2ms/step - loss: 9482.7422 - mae: 9482.7422
Epoch 16/100
34/34 [==============================] - 0s 2ms/step - loss: 9066.7461 - mae: 9066.7461
Epoch 17/100
34/34 [==============================] - 0s 2ms/step - loss: 8721.9854 - mae: 8721.9854
Epoch 18/100
34/34 [==============================] - 0s 2ms/step - loss: 8441.2002 - mae: 8441.2002
Epoch 19/100
34/34 [==============================] - 0s 2ms/step - loss: 8227.5117 - mae: 8227.5117
Epoch 20/100
34/34 [==============================] - 0s 3ms/step - loss: 8081.9775 - mae: 8081.9775
Epoch 21/100
34/34 [==============================] - 0s 2ms/step - loss: 7973.8945 - mae: 7973.8945
Epoch 22/100
34/34 [==============================] - 0s 2ms/step - loss: 7899.1597 - mae: 7899.1597
Epoch 23/100
34/34 [==============================] - 0s 2ms/step - loss: 7840.3906 - mae: 7840.3906
Epoch 24/100
34/34 [==============================] - 0s 2ms/step - loss: 7787.9619 - mae: 7787.9619
Epoch 25/100
34/34 [==============================] - 0s 2ms/step - loss: 7749.2622 - mae: 7749.2622
Epoch 26/100
34/34 [==============================] - 0s 2ms/step - loss: 7697.9600 - mae: 7697.9600
Epoch 27/100
34/34 [==============================] - 0s 2ms/step - loss: 7656.0269 - mae: 7656.0269
Epoch 28/100
34/34 [==============================] - 0s 2ms/step - loss: 7613.4780 - mae: 7613.4780
Epoch 29/100
34/34 [==============================] - 0s 2ms/step - loss: 7570.9482 - mae: 7570.9482
Epoch 30/100
34/34 [==============================] - 0s 3ms/step - loss: 7527.4175 - mae: 7527.4175
Epoch 31/100
34/34 [==============================] - 0s 2ms/step - loss: 7483.5947 - mae: 7483.5947
Epoch 32/100
34/34 [==============================] - 0s 2ms/step - loss: 7439.4424 - mae: 7439.4424
Epoch 33/100
34/34 [==============================] - 0s 2ms/step - loss: 7395.0547 - mae: 7395.0547
Epoch 34/100
34/34 [==============================] - 0s 2ms/step - loss: 7346.8120 - mae: 7346.8120
Epoch 35/100
34/34 [==============================] - 0s 2ms/step - loss: 7300.0488 - mae: 7300.0488
Epoch 36/100
34/34 [==============================] - 0s 2ms/step - loss: 7249.8452 - mae: 7249.8452
Epoch 37/100
34/34 [==============================] - 0s 3ms/step - loss: 7199.5303 - mae: 7199.5303
Epoch 38/100
34/34 [==============================] - 0s 2ms/step - loss: 7148.4805 - mae: 7148.4805
Epoch 39/100
34/34 [==============================] - 0s 2ms/step - loss: 7093.6650 - mae: 7093.6650
Epoch 40/100
34/34 [==============================] - 0s 2ms/step - loss: 7038.1797 - mae: 7038.1797
Epoch 41/100
34/34 [==============================] - 0s 2ms/step - loss: 6981.7393 - mae: 6981.7393
Epoch 42/100
34/34 [==============================] - 0s 3ms/step - loss: 6922.7847 - mae: 6922.7847
Epoch 43/100
34/34 [==============================] - 0s 2ms/step - loss: 6860.1724 - mae: 6860.1724
Epoch 44/100
34/34 [==============================] - 0s 3ms/step - loss: 6793.7969 - mae: 6793.7969
Epoch 45/100
34/34 [==============================] - 0s 2ms/step - loss: 6726.6201 - mae: 6726.6201
Epoch 46/100
34/34 [==============================] - 0s 2ms/step - loss: 6657.4683 - mae: 6657.4683
Epoch 47/100
34/34 [==============================] - 0s 2ms/step - loss: 6586.3086 - mae: 6586.3086
Epoch 48/100
34/34 [==============================] - 0s 2ms/step - loss: 6507.5063 - mae: 6507.5063
Epoch 49/100
34/34 [==============================] - 0s 2ms/step - loss: 6428.6021 - mae: 6428.6021
Epoch 50/100
34/34 [==============================] - 0s 2ms/step - loss: 6342.7100 - mae: 6342.7100
Epoch 51/100
34/34 [==============================] - 0s 2ms/step - loss: 6258.0718 - mae: 6258.0718
Epoch 52/100
34/34 [==============================] - 0s 2ms/step - loss: 6164.7046 - mae: 6164.7046
Epoch 53/100
34/34 [==============================] - 0s 2ms/step - loss: 6068.6748 - mae: 6068.6748
Epoch 54/100
34/34 [==============================] - 0s 3ms/step - loss: 5970.0981 - mae: 5970.0981
Epoch 55/100
34/34 [==============================] - 0s 2ms/step - loss: 5862.5625 - mae: 5862.5625
Epoch 56/100
34/34 [==============================] - 0s 3ms/step - loss: 5753.9526 - mae: 5753.9526
Epoch 57/100
34/34 [==============================] - 0s 2ms/step - loss: 5638.0942 - mae: 5638.0942
Epoch 58/100
34/34 [==============================] - 0s 3ms/step - loss: 5519.8687 - mae: 5519.8687
Epoch 59/100
34/34 [==============================] - 0s 2ms/step - loss: 5401.3198 - mae: 5401.3198
Epoch 60/100
34/34 [==============================] - 0s 2ms/step - loss: 5277.3506 - mae: 5277.3506
Epoch 61/100
34/34 [==============================] - 0s 2ms/step - loss: 5149.7637 - mae: 5149.7637
Epoch 62/100
34/34 [==============================] - 0s 2ms/step - loss: 5019.3535 - mae: 5019.3535
Epoch 63/100
34/34 [==============================] - 0s 2ms/step - loss: 4889.6865 - mae: 4889.6865
Epoch 64/100
34/34 [==============================] - 0s 2ms/step - loss: 4756.8560 - mae: 4756.8560
Epoch 65/100
34/34 [==============================] - 0s 2ms/step - loss: 4629.4370 - mae: 4629.4370
Epoch 66/100
34/34 [==============================] - 0s 2ms/step - loss: 4503.5991 - mae: 4503.5991
Epoch 67/100
34/34 [==============================] - 0s 2ms/step - loss: 4392.9922 - mae: 4392.9922
Epoch 68/100
34/34 [==============================] - 0s 3ms/step - loss: 4284.3862 - mae: 4284.3862
Epoch 69/100
34/34 [==============================] - 0s 2ms/step - loss: 4182.6182 - mae: 4182.6182
Epoch 70/100
34/34 [==============================] - 0s 2ms/step - loss: 4089.5725 - mae: 4089.5725
Epoch 71/100
34/34 [==============================] - 0s 2ms/step - loss: 4003.3896 - mae: 4003.3896
Epoch 72/100
34/34 [==============================] - 0s 2ms/step - loss: 3929.0093 - mae: 3929.0093
Epoch 73/100
34/34 [==============================] - 0s 2ms/step - loss: 3866.3110 - mae: 3866.3110
Epoch 74/100
34/34 [==============================] - 0s 2ms/step - loss: 3813.7144 - mae: 3813.7144
Epoch 75/100
34/34 [==============================] - 0s 2ms/step - loss: 3773.0317 - mae: 3773.0317
Epoch 76/100
34/34 [==============================] - 0s 2ms/step - loss: 3744.1995 - mae: 3744.1995
Epoch 77/100
34/34 [==============================] - 0s 2ms/step - loss: 3719.6870 - mae: 3719.6870
Epoch 78/100
34/34 [==============================] - 0s 2ms/step - loss: 3702.9109 - mae: 3702.9109
Epoch 79/100
34/34 [==============================] - 0s 2ms/step - loss: 3691.8787 - mae: 3691.8787
Epoch 80/100
34/34 [==============================] - 0s 2ms/step - loss: 3682.8350 - mae: 3682.8350
Epoch 81/100
34/34 [==============================] - 0s 2ms/step - loss: 3676.9763 - mae: 3676.9763
Epoch 82/100
34/34 [==============================] - 0s 2ms/step - loss: 3673.9492 - mae: 3673.9492
Epoch 83/100
34/34 [==============================] - 0s 2ms/step - loss: 3667.8452 - mae: 3667.8452
Epoch 84/100
34/34 [==============================] - 0s 2ms/step - loss: 3664.5757 - mae: 3664.5757
Epoch 85/100
34/34 [==============================] - 0s 2ms/step - loss: 3661.8562 - mae: 3661.8562
Epoch 86/100
34/34 [==============================] - 0s 2ms/step - loss: 3660.3044 - mae: 3660.3044
Epoch 87/100
34/34 [==============================] - 0s 2ms/step - loss: 3657.5137 - mae: 3657.5137
Epoch 88/100
34/34 [==============================] - 0s 2ms/step - loss: 3655.2200 - mae: 3655.2200
Epoch 89/100
34/34 [==============================] - 0s 2ms/step - loss: 3653.8831 - mae: 3653.8831
Epoch 90/100
34/34 [==============================] - 0s 2ms/step - loss: 3652.0198 - mae: 3652.0198
Epoch 91/100
34/34 [==============================] - 0s 2ms/step - loss: 3648.9990 - mae: 3648.9990
Epoch 92/100
34/34 [==============================] - 0s 3ms/step - loss: 3648.4460 - mae: 3648.4460
Epoch 93/100
34/34 [==============================] - 0s 2ms/step - loss: 3646.2300 - mae: 3646.2300
Epoch 94/100
34/34 [==============================] - 0s 2ms/step - loss: 3644.4377 - mae: 3644.4377
Epoch 95/100
34/34 [==============================] - 0s 2ms/step - loss: 3645.8772 - mae: 3645.8772
Epoch 96/100
34/34 [==============================] - 0s 2ms/step - loss: 3642.2576 - mae: 3642.2576
Epoch 97/100
34/34 [==============================] - 0s 2ms/step - loss: 3640.1189 - mae: 3640.1189
Epoch 98/100
34/34 [==============================] - 0s 2ms/step - loss: 3638.0647 - mae: 3638.0647
Epoch 99/100
34/34 [==============================] - 0s 2ms/step - loss: 3637.2051 - mae: 3637.2051
Epoch 100/100
34/34 [==============================] - 0s 3ms/step - loss: 3636.1707 - mae: 3636.1707
<keras.callbacks.History at 0x7f07ee440a30>
[103]
0s
#evaluate our data in normal form
insurance_model_4.evaluate(X_test_normal,Y_test)
9/9 [==============================] - 0s 2ms/step - loss: 3438.7844 - mae: 3438.7844
[3438.784423828125, 3438.784423828125]
[97]
0s
X_train.shape,X_train_normal.shape
((1070, 6), (1070, 11))
[81]
0s
X["age"].plot(kind="hist")

[82]
0s
X["bmi"].plot(kind="hist")

[83]
0s
X["children"].value_counts()
0    574
1    324
2    240
3    157
4     25
5     18
Name: children, dtype: int64
Colab paid products - Cancel contracts here

check
0s
completed at 12:51 AM
x, hint
Saved successfully!
